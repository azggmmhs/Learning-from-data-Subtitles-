1
00:00:00,000 --> 00:00:00,285


2
00:00:00,285 --> 00:00:03,270
ANNOUNCER: The following program
is brought to you by Caltech.

3
00:00:03,270 --> 00:00:15,870


4
00:00:15,870 --> 00:00:19,460
YASER ABU-MOSTAFA: Welcome back.

5
00:00:19,460 --> 00:00:23,380
Last time, we introduced support
vector machines.

6
00:00:23,380 --> 00:00:29,240
And if you think of linear models as
economy cars, which is what we said

7
00:00:29,240 --> 00:00:33,040
when we introduced them, you can think
of support vector machines as the

8
00:00:33,040 --> 00:00:35,720
luxury line of those cars.

9
00:00:35,720 --> 00:00:39,400
And indeed, they are nothing but
a linear model in the simplest form

10
00:00:39,400 --> 00:00:44,690
except that they actually are a little
bit more keen on the performance.

11
00:00:44,690 --> 00:00:49,810
And the key to the performance was the
idea of the margin-- is that if the

12
00:00:49,810 --> 00:00:54,400
data is linearly separable, there
is more than one line that can

13
00:00:54,400 --> 00:00:56,600
separate the data.

14
00:00:56,600 --> 00:01:01,110
And if you take the line that has the
biggest margin, furthest away from the

15
00:01:01,110 --> 00:01:03,980
closest point, then you
have an advantage.

16
00:01:03,980 --> 00:01:08,560
It's both an intuitive advantage and
an advantage that can be theoretically

17
00:01:08,560 --> 00:01:11,600
established, which we did through
the idea of the growth

18
00:01:11,600 --> 00:01:13,510
function in this case.

19
00:01:13,510 --> 00:01:17,520
And after we determined that it's a good
idea to maximize the margin, we

20
00:01:17,520 --> 00:01:19,100
set out to do that.

21
00:01:19,100 --> 00:01:23,450
And after a chain of mathematics, we
ended up with a Lagrangian that

22
00:01:23,450 --> 00:01:25,090
we're going to maximize.

23
00:01:25,090 --> 00:01:27,420
And the Lagrangian has very
interesting properties.

24
00:01:27,420 --> 00:01:30,510
It's quadratic, so it's
a simple function.

25
00:01:30,510 --> 00:01:33,880
And the constraints are inequality
constraints, very simple inequality

26
00:01:33,880 --> 00:01:36,720
constraints in this case, and
one equality constraint.

27
00:01:36,720 --> 00:01:40,110
And we're not going to actually
do the solving ourselves.

28
00:01:40,110 --> 00:01:44,690
We're going to pass the problem on to
a package of quadratic programming.

29
00:01:44,690 --> 00:01:47,760
And then, we will wait for quadratic
programming to give us back

30
00:01:47,760 --> 00:01:50,170
the values of alphas.

31
00:01:50,170 --> 00:01:54,460
Now, quadratic programming will have
problems with solving this if the

32
00:01:54,460 --> 00:01:56,580
number of examples is bigger.

33
00:01:56,580 --> 00:02:00,750
So once you get to thousands,
its becomes an issue.

34
00:02:00,750 --> 00:02:04,050
And then, there are all kinds of
heuristics to deal with that case.

35
00:02:04,050 --> 00:02:08,389
And in general, quadratic programming
sometimes needs babysitting, tweaking,

36
00:02:08,389 --> 00:02:09,910
limiting range, and whatnot.

37
00:02:09,910 --> 00:02:13,110
But at least someone else wrote it, and
we only have to do these things in

38
00:02:13,110 --> 00:02:15,740
order to get the solution, rather
than to write this from scratch.

39
00:02:15,740 --> 00:02:17,820
So it's not a bad deal for us.

40
00:02:17,820 --> 00:02:22,370
And once we get the alphas back, there
is a very interesting interpretation

41
00:02:22,370 --> 00:02:23,500
that happens.

42
00:02:23,500 --> 00:02:26,780
You look at the alphas, the
Lagrange multipliers.

43
00:02:26,780 --> 00:02:29,120
And some of them will
be greater than 0.

44
00:02:29,120 --> 00:02:33,910
And most of them will be 0,
should be identically 0.

45
00:02:33,910 --> 00:02:35,790
In reality, because of the rounding
error, we might get

46
00:02:35,790 --> 00:02:36,610
them very, very small.

47
00:02:36,610 --> 00:02:39,110
And you set them manually to 0.

48
00:02:39,110 --> 00:02:43,010
But the guys that happen to be bigger
than 0 are special, and they are

49
00:02:43,010 --> 00:02:45,120
called the support vectors.

50
00:02:45,120 --> 00:02:49,690
And whether you're working in the X space,
or took the X space and moved to

51
00:02:49,690 --> 00:02:55,380
a Z space and moved back here, the
support vectors are the ones that

52
00:02:55,380 --> 00:02:56,510
achieve the margin.

53
00:02:56,510 --> 00:02:59,510
They are sitting exactly at
the critical point here.

54
00:02:59,510 --> 00:03:02,210
And they're used to define the plane.

55
00:03:02,210 --> 00:03:08,510
And the most important aspect about them
is the fact that you can predict

56
00:03:08,510 --> 00:03:13,460
a bound on the out-of-sample error, based
on the number of support vectors

57
00:03:13,460 --> 00:03:14,390
that you get.

58
00:03:14,390 --> 00:03:18,910
And it is the normal form of dividing
the complexity, in terms of the number

59
00:03:18,910 --> 00:03:21,460
of parameters, in this case the non-zero
alphas or the number of

60
00:03:21,460 --> 00:03:24,810
support vectors that corresponds to it,
divided by more or less the number

61
00:03:24,810 --> 00:03:25,350
of examples.

62
00:03:25,350 --> 00:03:27,030
We have seen that before.

63
00:03:27,030 --> 00:03:32,780
But the key issue here is something
really worth noting.

64
00:03:32,780 --> 00:03:36,090
The right-hand side without the expected
value-- the expected value just

65
00:03:36,090 --> 00:03:39,870
tells us that we average this over
a number of cases for this to be true.

66
00:03:39,870 --> 00:03:44,210
We are dividing the number of support
vectors by N. The number of support

67
00:03:44,210 --> 00:03:47,960
vectors is an in-sample quantity.

68
00:03:47,960 --> 00:03:49,610
You do all of this.

69
00:03:49,610 --> 00:03:50,980
You get the alphas back.

70
00:03:50,980 --> 00:03:55,570
And you can tell what the number of
support vectors are, in sample.

71
00:03:55,570 --> 00:03:59,620
So we are able to check on the
out-of-sample error, using

72
00:03:59,620 --> 00:04:01,330
an in-sample quantity.

73
00:04:01,330 --> 00:04:04,870
And we know, by the previous experience,
that this is a biggie.

74
00:04:04,870 --> 00:04:08,320
Because now, not only are we going to
check on the in-sample error, we're

75
00:04:08,320 --> 00:04:11,060
also going to check on the
out-of-sample error using

76
00:04:11,060 --> 00:04:13,000
a quantity we can measure.

77
00:04:13,000 --> 00:04:19,130
Now, we applied support vectors only
to linearly separable data, at

78
00:04:19,130 --> 00:04:20,390
least in the previous lecture.

79
00:04:20,390 --> 00:04:23,200
In this lecture, we will
generalize that.

80
00:04:23,200 --> 00:04:26,800
And in order to deal with cases where
the data is not linearly separable in

81
00:04:26,800 --> 00:04:30,800
the X space, what we did is we used
nonlinear transform, as we did before

82
00:04:30,800 --> 00:04:32,220
with linear models.

83
00:04:32,220 --> 00:04:34,980
And a curious thing happened
when we did that.

84
00:04:34,980 --> 00:04:38,390
Because we went to a fairly
high-dimensional Z space.

85
00:04:38,390 --> 00:04:43,610
And we got a surface that is wiggly,
and so on, which in our mind raises

86
00:04:43,610 --> 00:04:46,510
alarm bells as far as generalization
is concerned.

87
00:04:46,510 --> 00:04:51,470
But we ended up with something that can
be stated in a simplistic form as:

88
00:04:51,470 --> 00:04:57,090
we get a complex hypothesis,
which is the snake,

89
00:04:57,090 --> 00:05:00,540
but we don't pay the price for it in
terms of the complexity of the

90
00:05:00,540 --> 00:05:01,420
hypothesis set.

91
00:05:01,420 --> 00:05:03,700
Remember, the complexity of the
hypothesis set is what we pay the

92
00:05:03,700 --> 00:05:06,570
price for in terms of the
VC analysis, right?

93
00:05:06,570 --> 00:05:12,800
And it's typically the case that when
H is more complex,

94
00:05:12,800 --> 00:05:15,190
each individual is also complex.

95
00:05:15,190 --> 00:05:17,290
But here, we sort of did some cheating.

96
00:05:17,290 --> 00:05:20,720
And we managed to use a high-dimensional
Z space,

97
00:05:20,720 --> 00:05:24,570
so it's a complex H,
complex hypothesis set.

98
00:05:24,570 --> 00:05:29,510
But the hypothesis we get is really--

99
00:05:29,510 --> 00:05:33,490
although it looks very complex, it
really belongs to a simple set because

100
00:05:33,490 --> 00:05:35,020
it maximizes the margin.

101
00:05:35,020 --> 00:05:39,190
So we get the benefit of a fairly low
out-of-sample error, in spite of the

102
00:05:39,190 --> 00:05:43,270
fact that we captured the fitting very
well by getting the 0 in-sample error.

103
00:05:43,270 --> 00:05:45,910
Now, this is exaggerated.

104
00:05:45,910 --> 00:05:46,910
I grant you that.

105
00:05:46,910 --> 00:05:49,110
But it has an element of truth in it.

106
00:05:49,110 --> 00:05:52,460
And it captures what support
vector machines do.

107
00:05:52,460 --> 00:05:55,530
They allow you to go very sophisticated,
without fully paying the

108
00:05:55,530 --> 00:05:57,440
price for it.

109
00:05:57,440 --> 00:06:02,680
Today, we're going to continue
this by extending the support vector

110
00:06:02,680 --> 00:06:06,820
machines in the basic case, and we're
going to cover the main method, which

111
00:06:06,820 --> 00:06:09,260
is the kernel methods, in
the bulk of the lecture.

112
00:06:09,260 --> 00:06:15,140
And the two topics are the kernels,
referred to as the kernel trick

113
00:06:15,140 --> 00:06:19,900
actually, formally, and that takes care
of the nonlinear transformation

114
00:06:19,900 --> 00:06:23,820
when the Z space can be very
sophisticated, so sophisticated that

115
00:06:23,820 --> 00:06:24,970
you can't even write it down.

116
00:06:24,970 --> 00:06:27,200
It's an infinite-dimensional
space.

117
00:06:27,200 --> 00:06:30,000
Which would be completely unheard
of if you're using plain

118
00:06:30,000 --> 00:06:32,260
vanilla linear models.

119
00:06:32,260 --> 00:06:37,010
The other topic is to extend support
vector machines from the linearly

120
00:06:37,010 --> 00:06:41,010
separable case to the non-linearly-separable
case, allowing yourself to

121
00:06:41,010 --> 00:06:42,620
make errors.

122
00:06:42,620 --> 00:06:46,070
So this is pretty much that, if you were
using perceptrons and went to

123
00:06:46,070 --> 00:06:50,670
pocket, this would be if you went from
the support vector machines that we

124
00:06:50,670 --> 00:06:55,260
introduced-- that we're going to label now
hard-margin, because they strictly

125
00:06:55,260 --> 00:06:59,530
obey the margin-- to a soft margin
that allows some errors.

126
00:06:59,530 --> 00:07:04,780
And both of these extensions will expand
your horizons in terms of the

127
00:07:04,780 --> 00:07:06,330
problems you're able to deal with.

128
00:07:06,330 --> 00:07:10,280
And chances are, in a practical problem,
you're going to use both.

129
00:07:10,280 --> 00:07:13,310
You're going to go to
a high-dimensional space, sometimes

130
00:07:13,310 --> 00:07:16,150
an infinite-dimensional space, without
paying the price for it, as we'll see

131
00:07:16,150 --> 00:07:17,020
in a moment.

132
00:07:17,020 --> 00:07:20,400
And in addition to that, you're going
to allow some errors in order not to

133
00:07:20,400 --> 00:07:26,880
make outliers dictate an unduly complex
nonlinear transformation.

134
00:07:26,880 --> 00:07:29,320
So both of them will come in handy.

135
00:07:29,320 --> 00:07:32,270
Let's start with the kernels.

136
00:07:32,270 --> 00:07:36,520
So the idea of the kernels is that I
want to go to the Z space without

137
00:07:36,520 --> 00:07:37,810
paying the price for it.

138
00:07:37,810 --> 00:07:39,820
And we are already halfway there.

139
00:07:39,820 --> 00:07:43,860
If you remember from last lecture,
the way z manifests itself in the

140
00:07:43,860 --> 00:07:45,950
computation is very simple.

141
00:07:45,950 --> 00:07:49,450
You do an inner product in the Z
space, and from then on, it's

142
00:07:49,450 --> 00:07:51,530
a regular quadratic programming problem.

143
00:07:51,530 --> 00:07:55,810
And the dimensionality of the problem
depends on the number of examples, not

144
00:07:55,810 --> 00:07:59,100
on the dimensionality of the Z space,
once you get the inner product.

145
00:07:59,100 --> 00:08:04,960
And when you get the result back, you
count the number of support vectors,

146
00:08:04,960 --> 00:08:07,690
which really depends again on the
number of examples, not the

147
00:08:07,690 --> 00:08:10,880
dimensionality. Obviously, the
dimensionality will come in because

148
00:08:10,880 --> 00:08:16,410
you may end up with such a wiggly
surface that every other vector

149
00:08:16,410 --> 00:08:21,160
becomes a support vector in order to
support this type of boundary.

150
00:08:21,160 --> 00:08:25,230
But basically, the dimensionality
of Z explicitly doesn't appear.

151
00:08:25,230 --> 00:08:28,990
Nonetheless, we still have to take
an inner product in the Z space.

152
00:08:28,990 --> 00:08:33,460
So in this viewgraph, I'm going to zoom
in to the very simple question.

153
00:08:33,460 --> 00:08:37,549
What do I need from the Z space in
order to be able to carry out the

154
00:08:37,549 --> 00:08:41,179
machinery that I have seen so far?

155
00:08:41,179 --> 00:08:43,409
So what do we do?

156
00:08:43,409 --> 00:08:45,330
We have a Lagrangian to solve.

157
00:08:45,330 --> 00:08:47,680
So the Lagrangian looks like this.

158
00:08:47,680 --> 00:08:52,080
And since we are interested in what we
do in the Z space, I'm going to make

159
00:08:52,080 --> 00:08:54,450
these purple.

160
00:08:54,450 --> 00:08:58,330
So in order to be able to carry out
the Lagrangian, I need to get the

161
00:08:58,330 --> 00:09:00,870
inner product in the Z space.

162
00:09:00,870 --> 00:09:05,630
But getting an inner product in the Z
space is less demand than getting the

163
00:09:05,630 --> 00:09:07,990
actual vector in the Z space.

164
00:09:07,990 --> 00:09:10,060
Think of it this way.

165
00:09:10,060 --> 00:09:11,980
I am a guardian of the Z space.

166
00:09:11,980 --> 00:09:12,830
I'm closing the door.

167
00:09:12,830 --> 00:09:15,200
Nobody has access to the Z space.

168
00:09:15,200 --> 00:09:17,850
You come to me with requests.

169
00:09:17,850 --> 00:09:20,480
If you give me an x and ask me,
what is the transformation,

170
00:09:20,480 --> 00:09:21,470
that's a big demand.

171
00:09:21,470 --> 00:09:24,370
I have to hand you a big z.

172
00:09:24,370 --> 00:09:26,340
And I may not allow that.

173
00:09:26,340 --> 00:09:31,910
But let's say that all I'm willing
to give you are inner products.

174
00:09:31,910 --> 00:09:37,100
You give me x and x dash, I close the
door, do my thing, and come back with

175
00:09:37,100 --> 00:09:41,410
a number, which is the inner product
between z and z dash, without actually

176
00:09:41,410 --> 00:09:44,660
telling you what z and z dash were.

177
00:09:44,660 --> 00:09:46,570
That would be a simple operation.

178
00:09:46,570 --> 00:09:49,780
And if you can get away with it, then
that's a pretty good thing.

179
00:09:49,780 --> 00:09:53,530
Because now, we can completely focus
on inner products in the Z space

180
00:09:53,530 --> 00:09:56,200
and see if that can lead
to a simplification.

181
00:09:56,200 --> 00:10:00,790
So in this slide, we are going through
step by step in the entire process to

182
00:10:00,790 --> 00:10:05,142
see if we ever need anything
out of the Z space, other

183
00:10:05,142 --> 00:10:07,020
than the inner product.

184
00:10:07,020 --> 00:10:12,020
So in forming the Lagrangian,
we need the inner product.

185
00:10:12,020 --> 00:10:13,130
Let's look at the constraints.

186
00:10:13,130 --> 00:10:15,630
We have to pass the constraints
to quadratic programming.

187
00:10:15,630 --> 00:10:17,300
This is the first constraint.

188
00:10:17,300 --> 00:10:18,590
I don't see any z.

189
00:10:18,590 --> 00:10:20,410
So we're cool.

190
00:10:20,410 --> 00:10:23,780
The other one, the equality,
I don't see any z either.

191
00:10:23,780 --> 00:10:28,750
So if you have an inner product in the
Z space, you are ready with the

192
00:10:28,750 --> 00:10:32,110
problem that you're going to pass
on to the quadratic programming.

193
00:10:32,110 --> 00:10:35,590
You give it to quadratic programming.
Back comes the vector alpha: alpha_1,

194
00:10:35,590 --> 00:10:39,460
alpha_2, up to alpha_N. Now, you need
to implement your function.

195
00:10:39,460 --> 00:10:40,330
You're not just solving this.

196
00:10:40,330 --> 00:10:43,680
You actually are going to hand a hypothesis
to your customer, right?

197
00:10:43,680 --> 00:10:46,680
And the hypothesis looks like this.

198
00:10:46,680 --> 00:10:47,290
Now, I look at this.

199
00:10:47,290 --> 00:10:52,800
And now, I'm a little bit worried
because here's w and z.

200
00:10:52,800 --> 00:10:55,290
Although this is an inner product, it's
not an inner product between points

201
00:10:55,290 --> 00:10:59,380
in X-- between w, and I don't know what
w is. w lives in the Z space.

202
00:10:59,380 --> 00:11:02,900
So I want to make sure: can I get away
with just inner products in order to

203
00:11:02,900 --> 00:11:04,270
solve this?

204
00:11:04,270 --> 00:11:05,640
Well, w is no mystery to us.

205
00:11:05,640 --> 00:11:07,580
We have solved for it explicitly.

206
00:11:07,580 --> 00:11:12,480
And we found that you can find w by
adding up, over all the vectors but in

207
00:11:12,480 --> 00:11:15,340
particular over the support vectors
that happen to have nonzero alpha,

208
00:11:15,340 --> 00:11:17,030
this quantity.

209
00:11:17,030 --> 00:11:21,710
If you take this quantity back and plug
it in for w, what do you get in

210
00:11:21,710 --> 00:11:23,830
terms of what you need to compute?

211
00:11:23,830 --> 00:11:28,460
You need to compute inner products.

212
00:11:28,460 --> 00:11:31,750
That's encouraging.

213
00:11:31,750 --> 00:11:37,020
One more item, this innocent-looking b
is loaded.

214
00:11:37,020 --> 00:11:39,870
This is one of the parameters
that we solved for.

215
00:11:39,870 --> 00:11:41,730
Maybe that's what will kill us.

216
00:11:41,730 --> 00:11:42,460
Let's see.

217
00:11:42,460 --> 00:11:45,430
How do I solve for b?

218
00:11:45,430 --> 00:11:50,560
I solve for b by taking any support
vector, and solving for this equation.

219
00:11:50,560 --> 00:11:55,020
So I take a support vector,
m, and plug it in.

220
00:11:55,020 --> 00:11:57,000
Am I in trouble because I have the w?

221
00:11:57,000 --> 00:11:58,860
No, we already saw that w is here.

222
00:11:58,860 --> 00:11:59,740
It has this form.

223
00:11:59,740 --> 00:12:01,380
So I can plug it in here.

224
00:12:01,380 --> 00:12:07,590
And all I need, in order to solve
for b here, is this fellow.

225
00:12:07,590 --> 00:12:09,130
Done.

226
00:12:09,130 --> 00:12:12,710
We only deal with z as far as the
inner product is concerned.

227
00:12:12,710 --> 00:12:16,660
Now, that raises a very interesting
possibility.

228
00:12:16,660 --> 00:12:21,190
If I am able to compute the inner
product in the Z space, without

229
00:12:21,190 --> 00:12:27,600
visiting the Z space, I still
can carry this machinery.

230
00:12:27,600 --> 00:12:30,290
We can even move further.

231
00:12:30,290 --> 00:12:35,420
If I can carry the inner product in the
Z space, without knowing what the Z

232
00:12:35,420 --> 00:12:39,350
space is, I still will be OK.

233
00:12:39,350 --> 00:12:40,750
You may wonder, how am
I going to do that?

234
00:12:40,750 --> 00:12:42,290
That's a different question.

235
00:12:42,290 --> 00:12:46,730
But all we need to do now is something:
I give you x and x dash,

236
00:12:46,730 --> 00:12:48,120
two points in the X space.

237
00:12:48,120 --> 00:12:51,680
You do your thing, come back with
a number, and promise me that this is

238
00:12:51,680 --> 00:12:55,990
the inner product in the Z space,
the mysterious Z space.

239
00:12:55,990 --> 00:13:01,240
And then, I will do all the support
vector machinery in your space that I

240
00:13:01,240 --> 00:13:05,030
never visited, and come up with the
support vectors which live in your

241
00:13:05,030 --> 00:13:08,800
space, and get the performance based on
the number of support vectors, and

242
00:13:08,800 --> 00:13:11,210
deliver to the customer, and tell
them I used really a very

243
00:13:11,210 --> 00:13:12,340
sophisticated space.

244
00:13:12,340 --> 00:13:15,200
And then, they would ask, what is it?

245
00:13:15,200 --> 00:13:19,080
And usually, we have our stunned-silence
moments in machine learning

246
00:13:19,080 --> 00:13:24,050
where you do something, and you know
that the existence is sufficient.

247
00:13:24,050 --> 00:13:27,960
So let's look at this idea as
being a generalized inner

248
00:13:27,960 --> 00:13:29,530
product. x and x dash,

249
00:13:29,530 --> 00:13:31,950
we transform them and take an inner
product in the Z space.

250
00:13:31,950 --> 00:13:33,920
We're going to treat it as if
it was a generalized inner

251
00:13:33,920 --> 00:13:35,020
product in the X space.

252
00:13:35,020 --> 00:13:37,170
So what are the components?

253
00:13:37,170 --> 00:13:38,080
You take two points.

254
00:13:38,080 --> 00:13:41,770
We're going to label them x and
x dash in the input space.

255
00:13:41,770 --> 00:13:45,440
And we need this quantity.

256
00:13:45,440 --> 00:13:51,210
So this quantity is a function
of x and x dash.

257
00:13:51,210 --> 00:13:52,250
That much we know.

258
00:13:52,250 --> 00:13:54,560
We don't know which function,
but we know it's a function.

259
00:13:54,560 --> 00:13:55,480
Why's that?

260
00:13:55,480 --> 00:14:00,630
Because z is exclusively
a function of x.

261
00:14:00,630 --> 00:14:03,670
z dash is exclusively a function
of x dash, being

262
00:14:03,670 --> 00:14:05,780
transformed versions of them.

263
00:14:05,780 --> 00:14:08,700
And therefore, their inner product will
be a function that is determined

264
00:14:08,700 --> 00:14:10,350
by x and x dash.

265
00:14:10,350 --> 00:14:14,770
So this is the function that
I'm looking for.

266
00:14:14,770 --> 00:14:17,500
Now, we're going to call this
the kernel, hence the name.

267
00:14:17,500 --> 00:14:19,480
So this is the kernel
we're going to use.

268
00:14:19,480 --> 00:14:24,770
A kernel will correspond
to some Z space.

269
00:14:24,770 --> 00:14:28,390
And as I mentioned, this will be
labeled as an inner product--

270
00:14:28,390 --> 00:14:31,170
I put it between quotations because it's
general-- between x and x dash.

271
00:14:31,170 --> 00:14:33,960
It's not a straight inner product,
but an inner product after

272
00:14:33,960 --> 00:14:35,310
a transformation.

273
00:14:35,310 --> 00:14:37,890


274
00:14:37,890 --> 00:14:39,500
Now let me give you an example.

275
00:14:39,500 --> 00:14:44,810
It's a bit of a simplistic example,
but just to illustrate the idea.

276
00:14:44,810 --> 00:14:48,530
Let's say that I have x
being two-dimensional.

277
00:14:48,530 --> 00:14:52,010
Two-dimensional Euclidean space, so
I have two coordinates, x_1 and x_2.

278
00:14:52,010 --> 00:14:55,070
And I'm using a nonlinear
transformation, which happens to be

279
00:14:55,070 --> 00:14:56,070
2nd-order polynomial.

280
00:14:56,070 --> 00:14:57,970
We have seen that a number of times.

281
00:14:57,970 --> 00:15:01,000
So what do we have?

282
00:15:01,000 --> 00:15:06,370
We have a transformation that takes the
vector x, produces the vector z.

283
00:15:06,370 --> 00:15:11,230
And that would be the full 2nd-order
guy, so we have 6 coordinates

284
00:15:11,230 --> 00:15:15,030
corresponding to all terms of the second
order involving x_1 and x_2, and

285
00:15:15,030 --> 00:15:15,600
this is the guy.

286
00:15:15,600 --> 00:15:17,970
We used that before.

287
00:15:17,970 --> 00:15:24,230
And therefore, if you want to get the
kernel, which is formally the inner

288
00:15:24,230 --> 00:15:30,830
product between the transformation of
x and x dash, you will get this.

289
00:15:30,830 --> 00:15:34,400
Nothing mysterious, you're just going
to substitute for this for x, and

290
00:15:34,400 --> 00:15:38,540
substitute for it again for x dash,
multiply the corresponding terms, and

291
00:15:38,540 --> 00:15:39,070
add them up.

292
00:15:39,070 --> 00:15:40,860
And this is what you get.

293
00:15:40,860 --> 00:15:44,910
So the only lesson we're learning here
is that indeed this is just a function

294
00:15:44,910 --> 00:15:46,180
of x and x dash.

295
00:15:46,180 --> 00:15:48,510
If I didn't know this was an inner
product, I can look at this.

296
00:15:48,510 --> 00:15:51,130
This is a function I can compute.

297
00:15:51,130 --> 00:15:53,850
Fine, now we come to the trick.

298
00:15:53,850 --> 00:15:57,160


299
00:15:57,160 --> 00:16:04,970
Can we compute this kernel without
transforming x and x dash?

300
00:16:04,970 --> 00:16:08,250
So let's look at the example again.

301
00:16:08,250 --> 00:16:12,580
I'm going to now improvise a kernel.

302
00:16:12,580 --> 00:16:15,820
It doesn't transform things
to the Z space, and then

303
00:16:15,820 --> 00:16:16,750
does the inner product.

304
00:16:16,750 --> 00:16:19,390
It just tells you what the kernel is.

305
00:16:19,390 --> 00:16:23,650
And then, I'm going to convince you that
this kernel actually corresponds

306
00:16:23,650 --> 00:16:29,220
to a transformation to some Z space,
and taking an inner product there.

307
00:16:29,220 --> 00:16:31,640
So here is my kernel.

308
00:16:31,640 --> 00:16:33,430
It's function of x and x dash.

309
00:16:33,430 --> 00:16:36,070
And it happens to have that form.

310
00:16:36,070 --> 00:16:39,420
This is a special form that
will help me later on.

311
00:16:39,420 --> 00:16:42,310
But the main thing you would want to
look at is that this is not

312
00:16:42,310 --> 00:16:46,300
an inner product in the X space, in spite
of the fact that it involves one

313
00:16:46,300 --> 00:16:47,370
computationally.

314
00:16:47,370 --> 00:16:49,070
I take this, add 1, and square it.

315
00:16:49,070 --> 00:16:50,490
So this is just a function.

316
00:16:50,490 --> 00:16:52,980
I happen to formalize it in terms
of an inner product, just

317
00:16:52,980 --> 00:16:54,000
because it's simple.

318
00:16:54,000 --> 00:16:55,850
So this is a function.

319
00:16:55,850 --> 00:16:59,190
This is also not clearly an inner
product in any other space,

320
00:16:59,190 --> 00:17:00,950
transformed or otherwise.

321
00:17:00,950 --> 00:17:02,920
It is just a function.

322
00:17:02,920 --> 00:17:05,040
So now, I'm going to
take this function.

323
00:17:05,040 --> 00:17:08,069
And I'm going to write it explicitly
in terms of the components.

324
00:17:08,069 --> 00:17:10,470
I'm still working with
two-dimensional input.

325
00:17:10,470 --> 00:17:11,800
So this would be--

326
00:17:11,800 --> 00:17:14,829
the inner product here would be x_1
x_1 dash plus x_2 x_2 dash.

327
00:17:14,829 --> 00:17:17,310
So this is the quantity that I have.

328
00:17:17,310 --> 00:17:19,220
And I can definitely square things.

329
00:17:19,220 --> 00:17:22,329
And I get this quantity.

330
00:17:22,329 --> 00:17:24,819
So this is the value of the kernel.

331
00:17:24,819 --> 00:17:27,280
Now, this looks awfully familiar.

332
00:17:27,280 --> 00:17:32,690
It looks like an inner product,
except for these annoying 2's.

333
00:17:32,690 --> 00:17:35,290
This would have been as if
I transformed to the 2nd order and

334
00:17:35,290 --> 00:17:37,970
took it, except that
I have these guys.

335
00:17:37,970 --> 00:17:39,585
But is this going to discourage me?

336
00:17:39,585 --> 00:17:40,840
No.

337
00:17:40,840 --> 00:17:43,100
This still is an inner product.

338
00:17:43,100 --> 00:17:47,320
And the transformation to the space that
makes this an inner product is

339
00:17:47,320 --> 00:17:51,030
this fellow. x goes through this.

340
00:17:51,030 --> 00:17:53,280
See? I put a square root.

341
00:17:53,280 --> 00:17:54,960
Nonlinear transform, so I can put
anything, right?

342
00:17:54,960 --> 00:17:56,120
This is my transformation.

343
00:17:56,120 --> 00:17:59,790
The only test you need to ask me is
whether I applied exactly the same

344
00:17:59,790 --> 00:18:01,770
transformation to x dash.

345
00:18:01,770 --> 00:18:03,020
Yes, I did.

346
00:18:03,020 --> 00:18:05,410


347
00:18:05,410 --> 00:18:08,330
So that is indeed a transformation
of x into z.

348
00:18:08,330 --> 00:18:10,330
And when I take the inner
product, what do I get?

349
00:18:10,330 --> 00:18:12,780
I get my kernel.

350
00:18:12,780 --> 00:18:16,350
OK, good establishment
of concept here.

351
00:18:16,350 --> 00:18:20,080
But the idea is that's a lot
of fuss about nothing really.

352
00:18:20,080 --> 00:18:22,190
I could have done this
in the first place.

353
00:18:22,190 --> 00:18:26,350
Now, think of what happens if I am, instead
of taking 1 plus x x dash to the 2,

354
00:18:26,350 --> 00:18:28,760
I do it to the 100.

355
00:18:28,760 --> 00:18:32,160
Look at the difference between computing
this quantity and actually

356
00:18:32,160 --> 00:18:39,880
going to the 100-order transformation,
getting this expanded, and getting the

357
00:18:39,880 --> 00:18:43,150
other one expanded, and then
doing the inner product.

358
00:18:43,150 --> 00:18:44,800
So let's see how this works.

359
00:18:44,800 --> 00:18:47,360
That's called the polynomial kernel.

360
00:18:47,360 --> 00:18:53,700
So now I take a d-dimensional
space, not 2, but general d.

361
00:18:53,700 --> 00:18:58,020
And I would like to take
a transformation of that space into

362
00:18:58,020 --> 00:18:59,270
Qth-order polynomial.

363
00:18:59,270 --> 00:19:02,540


364
00:19:02,540 --> 00:19:05,520
And here's my kernel, the
equivalent kernel.

365
00:19:05,520 --> 00:19:08,100
I'm putting it between quotations
because the square root will happen

366
00:19:08,100 --> 00:19:10,000
here again in abundance.

367
00:19:10,000 --> 00:19:11,590
But it's just a scale.

368
00:19:11,590 --> 00:19:14,060
The main idea is still there.

369
00:19:14,060 --> 00:19:16,740
So here's my kernel.

370
00:19:16,740 --> 00:19:23,120
I get 1 plus x-- that to the Q. First,
establishing what does it take

371
00:19:23,120 --> 00:19:24,150
to compute this?

372
00:19:24,150 --> 00:19:26,780
I don't know yet whether this
is a kernel, a valid kernel.

373
00:19:26,780 --> 00:19:29,010
A valid kernel is an inner
product in some space.

374
00:19:29,010 --> 00:19:30,290
I haven't seen that yet.

375
00:19:30,290 --> 00:19:33,410
I pretty much suspect that it will be
by the previous argument, but that

376
00:19:33,410 --> 00:19:35,800
will become clearer when I do this.

377
00:19:35,800 --> 00:19:38,870
So when I evaluate this, so
this is an inner product.

378
00:19:38,870 --> 00:19:39,870
Now, I have d dimensions.

379
00:19:39,870 --> 00:19:42,580
So I have d of these guys corresponding
to each other, and then

380
00:19:42,580 --> 00:19:47,030
multiply them, raised to the power Q. How
much computation does it take you

381
00:19:47,030 --> 00:19:49,030
to do this?

382
00:19:49,030 --> 00:19:51,410
I have d multiplications here.

383
00:19:51,410 --> 00:19:54,940
That's the dimensionality
of the X space.

384
00:19:54,940 --> 00:19:59,340
And then, I need to raise it to the
power Q. Whether Q is 10 or 100 or

385
00:19:59,340 --> 00:20:01,760
a million, it's the same complexity.

386
00:20:01,760 --> 00:20:04,500
What I'm going to do, I'm going to take
the logarithm, multiply it by Q,

387
00:20:04,500 --> 00:20:06,100
and exponentiate.

388
00:20:06,100 --> 00:20:08,330
It doesn't matter what this fellow is.

389
00:20:08,330 --> 00:20:08,770
This is a number.

390
00:20:08,770 --> 00:20:10,100
I'm not expanding them.

391
00:20:10,100 --> 00:20:12,260
I'm just plugging in these,
and this becomes a number.

392
00:20:12,260 --> 00:20:15,820
Raising it to the power 100, or
raising it to the power 1000.

393
00:20:15,820 --> 00:20:18,850
So this is a very simple
operation to carry out.

394
00:20:18,850 --> 00:20:23,360
Now, think of what happens if you were
actually taking d equals 10 and Q

395
00:20:23,360 --> 00:20:25,060
equals 100.

396
00:20:25,060 --> 00:20:29,060
And you can see that if I actually
expanded this conceptually, not

397
00:20:29,060 --> 00:20:34,540
computationally, it's very convenient
because every time an x appears, the x

398
00:20:34,540 --> 00:20:37,210
dash version of it appears.

399
00:20:37,210 --> 00:20:40,740
When I multiply any combination,
that will always be the case.

400
00:20:40,740 --> 00:20:47,200
I will get a tremendous number of terms,
which are all orders up to Q, of

401
00:20:47,200 --> 00:20:49,050
different combinations of the x's.

402
00:20:49,050 --> 00:20:51,080
And I will have a huge expansion here.

403
00:20:51,080 --> 00:20:54,270
And it shouldn't be a surprise that I
will be able to decompose this into

404
00:20:54,270 --> 00:21:00,090
something of x dot something of x dash,
because every term here appears with

405
00:21:00,090 --> 00:21:01,060
both x and x dash--

406
00:21:01,060 --> 00:21:03,760
the same trick we did here,
except more elaborately.

407
00:21:03,760 --> 00:21:07,920
But if you actually go at it explicitly,
you have to give me the

408
00:21:07,920 --> 00:21:12,760
entire vector in the Z space that
results from a 100th-order polynomial

409
00:21:12,760 --> 00:21:15,560
transformation of a 10th-order guy.

410
00:21:15,560 --> 00:21:19,090
This will be an ugly
beast to deal with.

411
00:21:19,090 --> 00:21:23,210
And now I can do this by just computing
this number, just a number.

412
00:21:23,210 --> 00:21:26,210
Take your x and x dash to get this
number, raise it to the power Q, and I

413
00:21:26,210 --> 00:21:29,710
already have, as if I visited the Z
space and got that number there.

414
00:21:29,710 --> 00:21:32,840
OK?

415
00:21:32,840 --> 00:21:36,280
So if you're worried about the square
root-- because obviously,

416
00:21:36,280 --> 00:21:36,950
you will get this.

417
00:21:36,950 --> 00:21:38,370
But you will get a bunch
of combinations.

418
00:21:38,370 --> 00:21:41,630
This gets here, gets here, and
now, you're power 100.

419
00:21:41,630 --> 00:21:43,030
So there will be all kinds
of combinations.

420
00:21:43,030 --> 00:21:45,820
So you'll get a bunch of constants
in front of the terms.

421
00:21:45,820 --> 00:21:49,680
And you're going to square-root them in
order to get it to be a canonical

422
00:21:49,680 --> 00:21:50,660
transformation.

423
00:21:50,660 --> 00:21:54,290
You can adjust the scales a little bit,
not fully, by taking your kernel

424
00:21:54,290 --> 00:21:59,670
instead of being 1 plus, you have scales
'a' and 'b' that will mitigate

425
00:21:59,670 --> 00:22:03,920
a little bit the diversity of the
coefficients you get here.

426
00:22:03,920 --> 00:22:07,060
But the bottom line is that a kernel
of this form does correspond to

427
00:22:07,060 --> 00:22:08,990
an inner product in a higher space.

428
00:22:08,990 --> 00:22:14,530
And by computing it just in the X space,
using this formula, I'm doing all I

429
00:22:14,530 --> 00:22:17,250
need to do in order to carry
out the SV machinery.

430
00:22:17,250 --> 00:22:21,400


431
00:22:21,400 --> 00:22:24,850
Now, with this in mind-- I did this
by construction because it's an easy

432
00:22:24,850 --> 00:22:26,350
polynomial, and we can visualize it.

433
00:22:26,350 --> 00:22:29,290
We can get it in the 2 case,
and extrapolate mentally

434
00:22:29,290 --> 00:22:30,690
for the other cases.

435
00:22:30,690 --> 00:22:34,190
Now, we realize in this case that
we only need z to exist.

436
00:22:34,190 --> 00:22:37,860
In this case, I showed you what z is
explicitly in the case of d equals 2,

437
00:22:37,860 --> 00:22:40,450
and by hand waving in the
bigger case.

438
00:22:40,450 --> 00:22:42,990
But you can visualize what z is.

439
00:22:42,990 --> 00:22:48,580
So now, let's get carried away and try
to just get a kernel that maps us to z

440
00:22:48,580 --> 00:22:51,010
without even imagining what z is.

441
00:22:51,010 --> 00:22:53,720
So this is the case.

442
00:22:53,720 --> 00:23:00,450
We take this to be an inner
product in some space, Z.

443
00:23:00,450 --> 00:23:04,170
And once you do that, we are good with
the entire machinery, and the

444
00:23:04,170 --> 00:23:07,410
guarantees, and we'll get the support
vectors, and we'll get generalization

445
00:23:07,410 --> 00:23:10,490
bound, all of the above.

446
00:23:10,490 --> 00:23:12,560
And here's an example of a kernel.

447
00:23:12,560 --> 00:23:15,940
This will be a useful kernel.

448
00:23:15,940 --> 00:23:19,000
Let's look at it.

449
00:23:19,000 --> 00:23:20,930
It's definitely a function
of x and x dash.

450
00:23:20,930 --> 00:23:23,180
That's as much as I would require--
the minimum requirement

451
00:23:23,180 --> 00:23:24,700
for this to be true.

452
00:23:24,700 --> 00:23:29,870
And now, it doesn't even have an inner
product term clearly, either in X

453
00:23:29,870 --> 00:23:31,300
space or Z space.

454
00:23:31,300 --> 00:23:33,050
And I have no idea what that is.

455
00:23:33,050 --> 00:23:34,330
I can compute it.

456
00:23:34,330 --> 00:23:39,390
And my question is, does this actually
correspond to some Z space, an inner

457
00:23:39,390 --> 00:23:40,220
product in Z space.

458
00:23:40,220 --> 00:23:44,260
So is this equivalent to taking each
of them by itself, transforming it

459
00:23:44,260 --> 00:23:47,260
into z in that space, and taking
a straight inner product

460
00:23:47,260 --> 00:23:48,760
between z and z dash?

461
00:23:48,760 --> 00:23:52,440
Do I get the same number
by visiting some space?

462
00:23:52,440 --> 00:23:53,450
The answer is yes.

463
00:23:53,450 --> 00:23:56,420
And the interesting thing is that
that space is infinite-dimensional.

464
00:23:56,420 --> 00:23:58,970


465
00:23:58,970 --> 00:24:03,510
So by doing this operation, which is
not very difficult to compute, you

466
00:24:03,510 --> 00:24:06,310
have done an inner product in
an infinite-dimensional space.

467
00:24:06,310 --> 00:24:08,330
Congratulations!

468
00:24:08,330 --> 00:24:12,900
And you will get the full benefit of
a horrific nonlinear transformation.

469
00:24:12,900 --> 00:24:17,440
And you don't worry about the
ramifications of going to an infinite

470
00:24:17,440 --> 00:24:18,620
dimensional space.

471
00:24:18,620 --> 00:24:22,420
In the third lecture, when I introduced
linear models, if I told you

472
00:24:22,420 --> 00:24:25,950
go to an infinite-dimensional space, you
would probably be screaming at me

473
00:24:25,950 --> 00:24:29,530
because the generalization issues
become completely ridiculous.

474
00:24:29,530 --> 00:24:31,150
But here, we don't worry.

475
00:24:31,150 --> 00:24:32,390
We'll carry the machinery.

476
00:24:32,390 --> 00:24:34,240
And then, we will count the number
of support vectors.

477
00:24:34,240 --> 00:24:37,660
If I have 1000 examples and you only
have 10 support vectors, I know I'm in

478
00:24:37,660 --> 00:24:38,510
good shape.

479
00:24:38,510 --> 00:24:42,065
Well, if I get 500 support
vectors, tough luck.

480
00:24:42,065 --> 00:24:43,660
It was a nice try.

481
00:24:43,660 --> 00:24:45,710
There's no harm done.

482
00:24:45,710 --> 00:24:47,600
So let's look at the
infinite-dimensional space.

483
00:24:47,600 --> 00:24:50,310
I'm trying to convince you that
this indeed is the case.

484
00:24:50,310 --> 00:24:52,620
What I'm going to do, I'm going
to take a simple case that I can

485
00:24:52,620 --> 00:24:55,120
illustrate.

486
00:24:55,120 --> 00:25:00,370
Let's take the kernel, but apply it in
this case to 1-dimensional space.

487
00:25:00,370 --> 00:25:05,490
So x and x dash are both scalars,
I call them x and x dash.

488
00:25:05,490 --> 00:25:09,860
And I'm going to take gamma to be 1,
which is my modulating constant here,

489
00:25:09,860 --> 00:25:11,830
so I get this fellow.

490
00:25:11,830 --> 00:25:16,030
Now, let me express this
using Taylor series.

491
00:25:16,030 --> 00:25:18,990
First, I do the following.

492
00:25:18,990 --> 00:25:23,960
I expand this, so I get x squared, x
dash squared, and minus twice x x dash,

493
00:25:23,960 --> 00:25:27,410
and minus twice gets the
minus, and becomes a plus.

494
00:25:27,410 --> 00:25:32,390
So I get e to the minus x squared,
e to the minus x dash squared,

495
00:25:32,390 --> 00:25:35,020
e to the 2 x x dash.

496
00:25:35,020 --> 00:25:38,180
So that's a legitimate
expansion of this.

497
00:25:38,180 --> 00:25:44,490
So now, I take this and expand it using
Taylor, and I get this fellow.

498
00:25:44,490 --> 00:25:48,190
You take whatever the argument is,
raise it the power k, divide it by

499
00:25:48,190 --> 00:25:50,670
k factorial, sum up from
k equals 0 to infinity.

500
00:25:50,670 --> 00:25:52,790
That's the Taylor series
for the e, right?

501
00:25:52,790 --> 00:25:57,620
Now, I conveniently took out the x dash
to the k, x to the k, and 2 to

502
00:25:57,620 --> 00:25:58,980
the k, separately.

503
00:25:58,980 --> 00:26:00,510
This is just to put it in that form.

504
00:26:00,510 --> 00:26:01,780
And I get that.

505
00:26:01,780 --> 00:26:02,830
OK, that's very nice.

506
00:26:02,830 --> 00:26:05,930
You seem to be complicating matters,
rather than simplifying it.

507
00:26:05,930 --> 00:26:10,010
Remember, my purpose is to convince you
that there is a Z space in which

508
00:26:10,010 --> 00:26:11,980
this is an inner product.

509
00:26:11,980 --> 00:26:15,190
So now look at this last line.

510
00:26:15,190 --> 00:26:19,610
And miraculously, some
terms will turn blue.

511
00:26:19,610 --> 00:26:20,860
Keep your eyes on it.

512
00:26:20,860 --> 00:26:23,850


513
00:26:23,850 --> 00:26:25,990
Oh, you see where I'm going with this.

514
00:26:25,990 --> 00:26:30,750
The guys that go with
x have turned blue.

515
00:26:30,750 --> 00:26:35,850
The guys that will go with
x dash have turned red.

516
00:26:35,850 --> 00:26:38,840


517
00:26:38,840 --> 00:26:40,710
And why am I doing that?

518
00:26:40,710 --> 00:26:45,650
Because I'm going to separate this
into an inner product, something

519
00:26:45,650 --> 00:26:48,770
coming from x, and something
coming from x dash.

520
00:26:48,770 --> 00:26:50,970
And I want to make sure
that it's the same.

521
00:26:50,970 --> 00:26:53,240
Once it is the same, then
the dimensionality

522
00:26:53,240 --> 00:26:54,830
is really this summation.

523
00:26:54,830 --> 00:26:56,920
Each of these is a coordinate.

524
00:26:56,920 --> 00:27:01,040
And this is the contribution
to the inner product by

525
00:27:01,040 --> 00:27:02,300
this coordinate.

526
00:27:02,300 --> 00:27:05,910
So here, I am getting this
x dash multiplied by x.

527
00:27:05,910 --> 00:27:08,960
Both of them normalized by e to
the minus x squared.

528
00:27:08,960 --> 00:27:12,890
So if I want to see what is the
transformation of the first guy, it

529
00:27:12,890 --> 00:27:17,900
would be e to the minus x squared
multiplied by x to the k.

530
00:27:17,900 --> 00:27:19,020
That's one coordinate.

531
00:27:19,020 --> 00:27:22,670
And as k goes from 0 to infinity,
I get different coordinates.

532
00:27:22,670 --> 00:27:25,810
I would be ready to go, except
for the annoying constants.

533
00:27:25,810 --> 00:27:29,000
so let's put them in purple!

534
00:27:29,000 --> 00:27:30,780
So what do you do with them?

535
00:27:30,780 --> 00:27:32,750
You divide them between red and blue.

536
00:27:32,750 --> 00:27:35,370
Take the square root of that
and put it in the red.

537
00:27:35,370 --> 00:27:37,830
And take the other square root,
and put it in the blue.

538
00:27:37,830 --> 00:27:40,750
And now, we have formally
two identical vectors.

539
00:27:40,750 --> 00:27:42,290
One is a transformed version of x.

540
00:27:42,290 --> 00:27:44,120
And one is a transformed
version of x dash.

541
00:27:44,120 --> 00:27:45,450
And this is the inner product.

542
00:27:45,450 --> 00:27:48,200
And it happens to be in infinite-dimensional
space, because you're

543
00:27:48,200 --> 00:27:52,750
summing for 0 until infinity.

544
00:27:52,750 --> 00:27:57,380
So now, this is a very
interesting kernel.

545
00:27:57,380 --> 00:28:00,490
It's called the radial-basis-function
kernel, if that rings a bell.

546
00:28:00,490 --> 00:28:02,700
Indeed, that's the subject
of the next lecture.

547
00:28:02,700 --> 00:28:05,840
So let us look at this
kernel in action.

548
00:28:05,840 --> 00:28:08,050
And it's very interesting, because it's
a very sophisticated kernel.

549
00:28:08,050 --> 00:28:10,190
It corresponds to an infinite-dimensional
space.

550
00:28:10,190 --> 00:28:13,360
Nonetheless, we can carry it out by
computing a fairly simple exponential

551
00:28:13,360 --> 00:28:15,910
between the x points.

552
00:28:15,910 --> 00:28:18,960
So let's look at it in action.

553
00:28:18,960 --> 00:28:23,200
I'm going to take a slightly
non-separable case in the X space.

554
00:28:23,200 --> 00:28:26,190
After all, I'm taking this glorious
nonlinear transformation.

555
00:28:26,190 --> 00:28:28,890
I'd better have something which is not
linearly separable in order to show

556
00:28:28,890 --> 00:28:30,640
you the goods.

557
00:28:30,640 --> 00:28:34,050
But I'm taking it slightly,
in order to make a point.

558
00:28:34,050 --> 00:28:37,710
So this is my target function.

559
00:28:37,710 --> 00:28:40,310
So if I generate points from here, the
chances are it will not be linearly

560
00:28:40,310 --> 00:28:43,690
separable, because just this wiggling
will result in that.

561
00:28:43,690 --> 00:28:46,860
And indeed, I'm going to generate
100 points at random.

562
00:28:46,860 --> 00:28:47,770
And I get them here.

563
00:28:47,770 --> 00:28:49,890
And if you look at the 100
points, really there's no

564
00:28:49,890 --> 00:28:53,520
line to separate them.

565
00:28:53,520 --> 00:28:56,440
So now what I'm going to do, I'm going
to lighten the target function because

566
00:28:56,440 --> 00:28:58,770
the target function did its job--
generated the examples.

567
00:28:58,770 --> 00:29:01,530
But I'm going to leave it, in order
to compare it with the final

568
00:29:01,530 --> 00:29:02,520
surface that we get.

569
00:29:02,520 --> 00:29:04,410
So I'm just going to have
it as a light surface.

570
00:29:04,410 --> 00:29:06,330
You can't even see it, probably.

571
00:29:06,330 --> 00:29:07,900
It's now a green surface.

572
00:29:07,900 --> 00:29:10,620
This is the data set
that I'm working with.

573
00:29:10,620 --> 00:29:13,640
So this is a slightly
non-separable case.

574
00:29:13,640 --> 00:29:22,320
And now, I'm going to transform X into
an infinite-dimensional space.

575
00:29:22,320 --> 00:29:23,550
Someone else worries about that.

576
00:29:23,550 --> 00:29:26,660
All I'm doing in my mind, I am
effectively doing that by just

577
00:29:26,660 --> 00:29:29,300
computing the kernel instead
of just the simple inner

578
00:29:29,300 --> 00:29:30,720
product in the X space.

579
00:29:30,720 --> 00:29:33,980
And the kernel is the kernel I got from
the last slide, which happens to

580
00:29:33,980 --> 00:29:36,830
be a simple exponential, I
compute it and get that.

581
00:29:36,830 --> 00:29:39,390
What happens when you do that?

582
00:29:39,390 --> 00:29:40,530
You get the kernel.

583
00:29:40,530 --> 00:29:42,220
You pass it on to quadratic
programming.

584
00:29:42,220 --> 00:29:46,440
And quadratic programming gives
you back the support vectors.

585
00:29:46,440 --> 00:29:48,760
You get the support vectors.

586
00:29:48,760 --> 00:29:51,570
Let me magnify it.

587
00:29:51,570 --> 00:29:55,860


588
00:29:55,860 --> 00:29:57,330
So we have the two classes.

589
00:29:57,330 --> 00:30:00,910
And I darken the points that ended
up being support vectors.

590
00:30:00,910 --> 00:30:05,860
These 1, 2, 3, 4 blue guys.

591
00:30:05,860 --> 00:30:09,760
And in the red, I have 1, 2, 3, 4, 5.

592
00:30:09,760 --> 00:30:11,800
I have 9 support vectors altogether.

593
00:30:11,800 --> 00:30:14,420


594
00:30:14,420 --> 00:30:15,610
Now, it's very interesting.

595
00:30:15,610 --> 00:30:17,860
9 support vectors, how many points?

596
00:30:17,860 --> 00:30:19,510
100 points.

597
00:30:19,510 --> 00:30:21,790
Can you tell me what is the
out-of-sample error?

598
00:30:21,790 --> 00:30:24,060
Can you bound it above?

599
00:30:24,060 --> 00:30:28,170
Oh, it looks like it should
be less than 10%.

600
00:30:28,170 --> 00:30:30,540
I have gone to an infinite-dimensional space.

601
00:30:30,540 --> 00:30:33,320
You're a witness to that. Right?

602
00:30:33,320 --> 00:30:37,990
I used what is effectively an infinite
number of parameters.

603
00:30:37,990 --> 00:30:43,420
Completely suicidal in terms of
generalization, but hey, I get 9

604
00:30:43,420 --> 00:30:44,020
support vectors.

605
00:30:44,020 --> 00:30:46,100
I can claim victory.

606
00:30:46,100 --> 00:30:50,350
So now, let's look at the surface
in the Z space when I

607
00:30:50,350 --> 00:30:52,920
transform it back here.

608
00:30:52,920 --> 00:30:55,950
Again, the Z space
is a mysterious guy.

609
00:30:55,950 --> 00:30:59,940
It's a hyperplane of degree
infinity minus 1.

610
00:30:59,940 --> 00:31:01,310
That's very nice.

611
00:31:01,310 --> 00:31:04,990
And now, I'm trying to transform this
to this space and look at it.

612
00:31:04,990 --> 00:31:06,240
And it looks like this.

613
00:31:06,240 --> 00:31:09,640


614
00:31:09,640 --> 00:31:10,370
How did I get that?

615
00:31:10,370 --> 00:31:11,620
I didn't go to the Z space.

616
00:31:11,620 --> 00:31:15,870
What I did, I classified every point on
the grid, and saw when it transforms

617
00:31:15,870 --> 00:31:16,960
from -1 to +1.

618
00:31:16,960 --> 00:31:18,410
That's my only tool.

619
00:31:18,410 --> 00:31:21,050
But I can do it because the
kernel is easy to compute.

620
00:31:21,050 --> 00:31:24,940
If I went into the Z space, you would
have never heard from me again!

621
00:31:24,940 --> 00:31:28,060
So this is a good way of doing it.

622
00:31:28,060 --> 00:31:32,350
You look at it, and it's
really very pretty.

623
00:31:32,350 --> 00:31:35,420
First thing, you don't get
the green thing exactly.

624
00:31:35,420 --> 00:31:39,460
But you can see why support vectors
are called support vectors.

625
00:31:39,460 --> 00:31:43,010
They're sort of holding the guy.

626
00:31:43,010 --> 00:31:45,590
You can see it up and down, up
and down, up and down.

627
00:31:45,590 --> 00:31:47,450
That's pretty good.

628
00:31:47,450 --> 00:31:51,010
The other thing is that, when you think
of the notion of a distance, remember

629
00:31:51,010 --> 00:31:53,240
that-- this is linearly
separable in that

630
00:31:53,240 --> 00:31:55,040
space. Had better be.

631
00:31:55,040 --> 00:31:56,640
We've got the infinite-dimensional space.

632
00:31:56,640 --> 00:31:59,720
If you don't get linearly separability
there, you are really in trouble!

633
00:31:59,720 --> 00:32:02,580
And when I get the linear separability
there, I get a margin.

634
00:32:02,580 --> 00:32:04,240
I try to maximize the margin.
That has already been

635
00:32:04,240 --> 00:32:05,490
maximized by the machinery.

636
00:32:05,490 --> 00:32:06,850
So I get a respectable margin.

637
00:32:06,850 --> 00:32:10,560
And the evidence for the respectable
margin is that I do get the small

638
00:32:10,560 --> 00:32:13,350
number of support vectors,
which are here. Fine.

639
00:32:13,350 --> 00:32:17,720
And now, when I look at the distance,
the value of the margin, the value of

640
00:32:17,720 --> 00:32:19,070
the margin is in the Z space.

641
00:32:19,070 --> 00:32:21,180
I cannot see that.

642
00:32:21,180 --> 00:32:24,890
But here, you can see that if you look
by the distance, these two support

643
00:32:24,890 --> 00:32:28,340
vectors are awfully close
to the surface.

644
00:32:28,340 --> 00:32:30,300
This support vector is not that close.

645
00:32:30,300 --> 00:32:32,170
Well, maybe it will become close
when this goes to extend.

646
00:32:32,170 --> 00:32:36,950
But it's definitely further away,
in the X space as we see it.

647
00:32:36,950 --> 00:32:40,200
But again, this is not the margin.

648
00:32:40,200 --> 00:32:42,200
These guys are pre-images
of support vectors.

649
00:32:42,200 --> 00:32:43,950
They are not support vectors per se.

650
00:32:43,950 --> 00:32:47,130
And the distance that was solved
for happened in the Z space.

651
00:32:47,130 --> 00:32:48,900
Whatever happens here happens here.

652
00:32:48,900 --> 00:32:51,600
You may end up with something,
where you get support vectors far

653
00:32:51,600 --> 00:32:53,580
away, and it's like a strange thing.

654
00:32:53,580 --> 00:32:56,040
Don't sweat bullets over it.

655
00:32:56,040 --> 00:32:58,110
It's happening in a space that
we don't understand.

656
00:32:58,110 --> 00:33:01,320
As long as the machinery for the
solution is correct and I get the

657
00:33:01,320 --> 00:33:03,170
support vectors that happen
to have lambda greater

658
00:33:03,170 --> 00:33:06,280
than 0, I am in business.

659
00:33:06,280 --> 00:33:08,560
Let's shrink this back.

660
00:33:08,560 --> 00:33:10,710
So we get this solution.

661
00:33:10,710 --> 00:33:13,530
And it's a pretty nice tool to have.

662
00:33:13,530 --> 00:33:16,865
And we ask ourselves: was this
an overkill to go to an infinite

663
00:33:16,865 --> 00:33:17,880
dimensional space?

664
00:33:17,880 --> 00:33:17,900


665
00:33:17,900 --> 00:33:20,180
Yes, early on before we studied
this thing, we would say

666
00:33:20,180 --> 00:33:22,210
that's a complete overkill.

667
00:33:22,210 --> 00:33:24,400
Even for these two dimensions,
it's slightly OK.

668
00:33:24,400 --> 00:33:28,280
If you went to a 5th-order polynomial,
I would already be worried

669
00:33:28,280 --> 00:33:29,570
that you're really doing too much.

670
00:33:29,570 --> 00:33:31,190
Now, you went to an infinite one.

671
00:33:31,190 --> 00:33:32,980
But now, we're asking
a different question.

672
00:33:32,980 --> 00:33:36,190
We're asking: check the number
of support vectors.

673
00:33:36,190 --> 00:33:37,870
That is your guide.

674
00:33:37,870 --> 00:33:41,370
And that is an in-sample quantity
that you can observe.

675
00:33:41,370 --> 00:33:42,860
And that will tell you the
generalization property.

676
00:33:42,860 --> 00:33:46,940


677
00:33:46,940 --> 00:33:50,580
So now, we are completely sold
on the idea of the kernels.

678
00:33:50,580 --> 00:33:53,970
Now let's look at if I give you
a kernel, and it's a valid kernel that

679
00:33:53,970 --> 00:33:57,360
corresponds to an inner product in some
Z space, how do you formulate

680
00:33:57,360 --> 00:33:58,560
the problem?

681
00:33:58,560 --> 00:34:00,395
This is just formality.

682
00:34:00,395 --> 00:34:00,840
You already know.

683
00:34:00,840 --> 00:34:02,260
But just let's take it step by step.

684
00:34:02,260 --> 00:34:03,365
What do you do?

685
00:34:03,365 --> 00:34:04,900
You remember quadratic programming?

686
00:34:04,900 --> 00:34:06,290
Yes, I do.

687
00:34:06,290 --> 00:34:10,400
And in quadratic programming,
we have this huge matrix.

688
00:34:10,400 --> 00:34:13,580
This is the big Q matrix that you
pass on to the algorithm.

689
00:34:13,580 --> 00:34:15,790
And you compute it in terms
of inner products.

690
00:34:15,790 --> 00:34:19,510
And these were genuine inner products,
when you were working with linearly

691
00:34:19,510 --> 00:34:22,440
separable data in the X space.

692
00:34:22,440 --> 00:34:26,420
So now, the only thing you're going to
do is that, instead of passing this to

693
00:34:26,420 --> 00:34:29,449
the quadratic programming, you're
going to pass this instead.

694
00:34:29,449 --> 00:34:32,159


695
00:34:32,159 --> 00:34:34,840
That's it.

696
00:34:34,840 --> 00:34:36,900
This may not be too much
computation at all.

697
00:34:36,900 --> 00:34:39,120
I can get the exponentials,
and get this number.

698
00:34:39,120 --> 00:34:41,670
And now, quadratic program
is ready to go.

699
00:34:41,670 --> 00:34:42,639
Absolutely nothing else.

700
00:34:42,639 --> 00:34:45,100
If you look at the rest of the details,
nothing is affected by the

701
00:34:45,100 --> 00:34:50,275
transformation, other than this
quadratic-programming matrix.

702
00:34:50,275 --> 00:34:52,179
That's good.

703
00:34:52,179 --> 00:34:54,989
Now, quadratic programming
passes you the alphas.

704
00:34:54,989 --> 00:34:55,835
You need the hypothesis.

705
00:34:55,835 --> 00:34:59,330
So how do I construct the hypothesis
in terms of the kernel?

706
00:34:59,330 --> 00:35:02,830


707
00:35:02,830 --> 00:35:05,680
So this is g of x equals that.

708
00:35:05,680 --> 00:35:07,380
I'm writing it because
it's safe to write.

709
00:35:07,380 --> 00:35:08,350
There's a Z space.

710
00:35:08,350 --> 00:35:09,790
I know I am linear there.

711
00:35:09,790 --> 00:35:12,350
And this is the form that I've
already been solving in.

712
00:35:12,350 --> 00:35:15,880
Now, I just want to translate
it in terms of the kernel.

713
00:35:15,880 --> 00:35:19,270
I know that I can, because we've spent
a lot of time realizing that we don't

714
00:35:19,270 --> 00:35:22,340
need anything from the Z space other
than the inner product, and the inner

715
00:35:22,340 --> 00:35:23,620
product is the kernel.

716
00:35:23,620 --> 00:35:26,570
I just want to put the
explicit form here.

717
00:35:26,570 --> 00:35:30,620
So you want to put this in terms of
kernel of something and something.

718
00:35:30,620 --> 00:35:34,220
And you take w to be this.

719
00:35:34,220 --> 00:35:35,990
And you're not going to solve
for any of those.

720
00:35:35,990 --> 00:35:37,240
These are just for illustration.

721
00:35:37,240 --> 00:35:42,470
And then, you get: g of x
would be this fellow.

722
00:35:42,470 --> 00:35:45,820
So you took this, substituted there,
you took the inner products.

723
00:35:45,820 --> 00:35:47,570
The inner product is what the kernel is.

724
00:35:47,570 --> 00:35:50,270
You put the kernel in place
of it, and you get that.

725
00:35:50,270 --> 00:35:55,140
Now, this is very interesting because
this is your model, so to speak.

726
00:35:55,140 --> 00:35:57,180
Support vector machines is a plural.

727
00:35:57,180 --> 00:36:00,370
Support vector machines-- doesn't
dictate a particular model.

728
00:36:00,370 --> 00:36:02,740
You choose a kernel, and it'll
give you a different model.

729
00:36:02,740 --> 00:36:05,970
So if you have ever been curious, in the
middle of all of this jungle, what

730
00:36:05,970 --> 00:36:06,650
is the model?

731
00:36:06,650 --> 00:36:08,720
What is the hypothesis that
I'm working with?

732
00:36:08,720 --> 00:36:11,290
It happens to have this
functional form.

733
00:36:11,290 --> 00:36:13,280
The kernel you choose appears here.

734
00:36:13,280 --> 00:36:15,520
It gets summed up with coefficients.

735
00:36:15,520 --> 00:36:18,910
The coefficients happen to
be determined by alpha.

736
00:36:18,910 --> 00:36:21,190
They all happen to agree
in sign with the label.

737
00:36:21,190 --> 00:36:23,930
That's one of the artifacts of that,
because alphas are non-negative.

738
00:36:23,930 --> 00:36:25,860
And we have plus b.

739
00:36:25,860 --> 00:36:28,140
And again, plus b is the one
that we haven't solved for.

740
00:36:28,140 --> 00:36:31,170
But I can solve for it
using the other one.

741
00:36:31,170 --> 00:36:33,725
And I end up with this
equation for it.

742
00:36:33,725 --> 00:36:37,840
Take any support vector, small m, and
you can identify it by having its

743
00:36:37,840 --> 00:36:39,750
alpha being bigger than 0.

744
00:36:39,750 --> 00:36:42,110
You plug it in, and you have that.

745
00:36:42,110 --> 00:36:44,930
So we have the full definition
of your hypothesis.

746
00:36:44,930 --> 00:36:47,530
And you get the solution in this form.

747
00:36:47,530 --> 00:36:51,710


748
00:36:51,710 --> 00:36:54,970
And this is for any support vector
which is defined by: alpha_m

749
00:36:54,970 --> 00:36:55,990
greater than 0.

750
00:36:55,990 --> 00:36:58,490
Now, let me make a point.

751
00:36:58,490 --> 00:37:01,830


752
00:37:01,830 --> 00:37:05,520
The nonlinear transformation
that started support vector

753
00:37:05,520 --> 00:37:07,380
machines is this guy.

754
00:37:07,380 --> 00:37:11,930
So in reality, I have an infinite
dimensional nonlinear transformation.

755
00:37:11,930 --> 00:37:15,500
Each of these is a coordinate
that depends fully on x.

756
00:37:15,500 --> 00:37:19,210
So I end up with 1, x, x squared,
x cubed, x to the 4, and so on.

757
00:37:19,210 --> 00:37:24,480
And if I'm working from an x that is
more than one-dimensional, I get x_1,

758
00:37:24,480 --> 00:37:27,630
x_2 squared, x_1 x_2, whole thing.

759
00:37:27,630 --> 00:37:31,250
I just avoided the labor
by using the kernel.

760
00:37:31,250 --> 00:37:35,450
Nonetheless, when I got the solution,
I got this solution that made me

761
00:37:35,450 --> 00:37:37,640
completely forget that
I did a nonlinear

762
00:37:37,640 --> 00:37:39,640
transformation into the Z space.

763
00:37:39,640 --> 00:37:43,730
I can look at this and say: what
I'm really doing is that this is my

764
00:37:43,730 --> 00:37:48,790
transformation, so to speak, the K's
I have, however many of them as there

765
00:37:48,790 --> 00:37:49,970
are terms here.

766
00:37:49,970 --> 00:37:51,735
And each of them has a coefficient.

767
00:37:51,735 --> 00:37:54,990
This would be a legitimate
way of looking at it.

768
00:37:54,990 --> 00:37:57,960
The only thing to remember, and it's
very important to remember, is that

769
00:37:57,960 --> 00:38:03,150
this transformation depends
on your data set.

770
00:38:03,150 --> 00:38:05,570
You see this x_n?

771
00:38:05,570 --> 00:38:08,640
This one doesn't.

772
00:38:08,640 --> 00:38:13,360
This one, before you gave me the data
set, I decided that I'm going to use

773
00:38:13,360 --> 00:38:18,750
the RBF kernel-- exponential, so I get
1, x, x squared, x cubed, x to the 4.

774
00:38:18,750 --> 00:38:21,080
All of this is determined without
looking at the data set.

775
00:38:21,080 --> 00:38:26,020
This transformation, in order to get
this thing, I need to know what x_n is.

776
00:38:26,020 --> 00:38:27,630
But we have seen this before.

777
00:38:27,630 --> 00:38:30,230
Remember the hidden layer
in neural networks?

778
00:38:30,230 --> 00:38:34,810
It got a nonlinear transform
based on the data set.

779
00:38:34,810 --> 00:38:37,300
So this is not foreign to us.

780
00:38:37,300 --> 00:38:39,480
But this tells you why this
looks very simple.

781
00:38:39,480 --> 00:38:41,060
Where is the infinite-dimensional space?

782
00:38:41,060 --> 00:38:42,255
I'm only determining this.

783
00:38:42,255 --> 00:38:47,030
This is the solution after all the
manipulation has been done.

784
00:38:47,030 --> 00:38:49,240
And that is why it has this form.

785
00:38:49,240 --> 00:38:52,920
But then, it will allow us to compare
support vector machines to other

786
00:38:52,920 --> 00:38:53,440
approaches.

787
00:38:53,440 --> 00:38:57,610
For example, if I put the RBF kernel
here, the one with e to the minus x

788
00:38:57,610 --> 00:39:01,410
squared with the norm, I will
get a functional form.

789
00:39:01,410 --> 00:39:05,080
It is completely legitimate to say:
let me look at functional forms of

790
00:39:05,080 --> 00:39:09,260
that form, and try to solve the learning
problem based on these, without

791
00:39:09,260 --> 00:39:10,880
ever hearing of support vectors.

792
00:39:10,880 --> 00:39:11,900
It's just a model.

793
00:39:11,900 --> 00:39:13,780
Let me see if I can get a solution.

794
00:39:13,780 --> 00:39:17,060
And it's very interesting to go through
this exercise, and to compare

795
00:39:17,060 --> 00:39:21,080
the result of doing it this way
versus doing it the SVM route.

796
00:39:21,080 --> 00:39:23,310
You can also do that for a neural
network and other

797
00:39:23,310 --> 00:39:24,560
kernels that you have.

798
00:39:24,560 --> 00:39:27,290


799
00:39:27,290 --> 00:39:32,180
Now, the question is-- I am
completely ready here.

800
00:39:32,180 --> 00:39:35,120
If you give me the kernel,
everything is understood.

801
00:39:35,120 --> 00:39:37,470
I can solve it, and I can
interpret the solution.

802
00:39:37,470 --> 00:39:40,690
And I can judge the quality of the
solution, and all of that.

803
00:39:40,690 --> 00:39:45,680
The only problem I have is that we don't
know that the kernel is valid.

804
00:39:45,680 --> 00:39:48,490
If I improvise, I tell you
what K of x and x dash is, and

805
00:39:48,490 --> 00:39:50,250
just give you a formula.

806
00:39:50,250 --> 00:39:53,160
The whole idea of the kernel is that
you don't visit the Z space.

807
00:39:53,160 --> 00:39:56,810
So how are you going to verify that
this is a valid kernel, namely

808
00:39:56,810 --> 00:40:00,560
an inner product in some space,
without visiting that space?

809
00:40:00,560 --> 00:40:01,700
That's the question.

810
00:40:01,700 --> 00:40:05,340
How do I know that Z exists
for a given kernel?

811
00:40:05,340 --> 00:40:08,110
By the way, in support vector machines,
you will come up with your

812
00:40:08,110 --> 00:40:09,450
own kernels.

813
00:40:09,450 --> 00:40:14,170
So it's a good idea to just ask yourself,
what are the conditions to get

814
00:40:14,170 --> 00:40:15,030
the kernel right?

815
00:40:15,030 --> 00:40:19,000
In order to get it to be a valid kernel,
there are three approaches.

816
00:40:19,000 --> 00:40:21,770


817
00:40:21,770 --> 00:40:25,440
First approach, we have already seen.

818
00:40:25,440 --> 00:40:28,980
This is by construction, conceptual
construction if not explicit

819
00:40:28,980 --> 00:40:31,230
construction, like we did
with the polynomial.

820
00:40:31,230 --> 00:40:34,110
We looked at it, and we realized that
there is a polynomial thing.

821
00:40:34,110 --> 00:40:38,880
And although I didn't do it for the case
of Q equals 100, I realize that

822
00:40:38,880 --> 00:40:41,120
there will be corresponding terms, and
I will be able to separate them.

823
00:40:41,120 --> 00:40:42,900
So in my mind, that is the Z space.

824
00:40:42,900 --> 00:40:46,070
And without constructing it explicitly,
I realize that the kernel

825
00:40:46,070 --> 00:40:49,320
that I wrote will correspond to
an inner product in that space.

826
00:40:49,320 --> 00:40:53,290
This is a very effective approach, and
the polynomial transformations are the

827
00:40:53,290 --> 00:40:55,920
most famous ones there.

828
00:40:55,920 --> 00:40:59,390
The other one is the one we're going to
talk about in the next slide, which

829
00:40:59,390 --> 00:41:03,430
is using math properties of
the kernel, something

830
00:41:03,430 --> 00:41:06,040
called Mercer's condition.

831
00:41:06,040 --> 00:41:07,810
So I'll talk about it.

832
00:41:07,810 --> 00:41:10,650
I wish it was a practical condition.

833
00:41:10,650 --> 00:41:13,740
It's a very appealing condition
theoretically.

834
00:41:13,740 --> 00:41:15,930
You will find it a little bit
difficult to apply in given

835
00:41:15,930 --> 00:41:16,840
situations.

836
00:41:16,840 --> 00:41:20,660
The good news is that people have
applied it to a bunch of kernels, and

837
00:41:20,660 --> 00:41:22,160
have declared them legitimate.

838
00:41:22,160 --> 00:41:26,780
So you can pick from that catalog
without worrying about it, that these

839
00:41:26,780 --> 00:41:28,120
have already been established.

840
00:41:28,120 --> 00:41:31,260
It comes into play when you
want to test a new kernel.

841
00:41:31,260 --> 00:41:33,110
Not an easy endeavor-- not
an impossible endeavor,

842
00:41:33,110 --> 00:41:35,060
but not an easy endeavor.

843
00:41:35,060 --> 00:41:38,500
The third approach is the one
I find rather interesting.

844
00:41:38,500 --> 00:41:40,385
So how do you know that Z exists?

845
00:41:40,385 --> 00:41:44,080


846
00:41:44,080 --> 00:41:46,440
Who cares?

847
00:41:46,440 --> 00:41:49,750
This is an approach followed by people
who say: this looks like a great

848
00:41:49,750 --> 00:41:50,450
machinery you have.

849
00:41:50,450 --> 00:41:51,360
You give me the kernel.

850
00:41:51,360 --> 00:41:51,760
I do this.

851
00:41:51,760 --> 00:41:53,410
I go to that.

852
00:41:53,410 --> 00:41:56,990
So I'll just improvise a kernel,
and who cares if there is

853
00:41:56,990 --> 00:41:57,890
a Z space or not?

854
00:41:57,890 --> 00:42:00,990
I never visit it anyway.

855
00:42:00,990 --> 00:42:02,040
Wait a minute!

856
00:42:02,040 --> 00:42:05,290
You don't visit it, but it has to exist
for all the guarantees that I

857
00:42:05,290 --> 00:42:06,690
talked about.

858
00:42:06,690 --> 00:42:09,750
Quadratic programming, and you get
support vectors, and alpha greater

859
00:42:09,750 --> 00:42:13,680
than 0, and the generalization, all of that
depends on the Z space being there,

860
00:42:13,680 --> 00:42:15,650
and you're actually separating
the data there.

861
00:42:15,650 --> 00:42:19,240
Believe it or not, there's quite
a number of people who just improvise

862
00:42:19,240 --> 00:42:22,690
a kernel, apply the machinery,
and see what happens.

863
00:42:22,690 --> 00:42:24,820
And sometimes they succeed.

864
00:42:24,820 --> 00:42:29,010
I have my reservations, let
me put it this way!

865
00:42:29,010 --> 00:42:32,400
So let's go for the mathematical
route, if you actually care,

866
00:42:32,400 --> 00:42:34,070
rather than who cares!

867
00:42:34,070 --> 00:42:37,760
So if you design your own kernel, and
then you want to see what happens,

868
00:42:37,760 --> 00:42:39,690
here is the condition.

869
00:42:39,690 --> 00:42:41,620
The following statement holds.

870
00:42:41,620 --> 00:42:46,350
The kernel that you wrote down is a valid
kernel, this is, the Z space that

871
00:42:46,350 --> 00:42:52,610
you're talking about actually exists,
if and only if two conditions in

872
00:42:52,610 --> 00:42:55,200
conjunction are satisfied.

873
00:42:55,200 --> 00:42:58,280
One is the fact that the
kernel is symmetric.

874
00:42:58,280 --> 00:43:01,950
That should be abundantly obvious,
symmetric being K of x and x dash

875
00:43:01,950 --> 00:43:04,540
being equal to K of x dash and x.

876
00:43:04,540 --> 00:43:08,210
Well, this is supposed to be the dot
product in the Z space, right?

877
00:43:08,210 --> 00:43:11,350
So we're going to transform
x and x dash into z and z dash.

878
00:43:11,350 --> 00:43:17,300
While in the Z space, certainly z dot z dash
is the same as z dash dot z.

879
00:43:17,300 --> 00:43:19,280
Inner product is commutative.

880
00:43:19,280 --> 00:43:22,140
So if this has a chance, it
had better be symmetric.

881
00:43:22,140 --> 00:43:24,820
So this is definitely one
of the conditions.

882
00:43:24,820 --> 00:43:27,340
The other one is that there is a matrix
that we're going to require

883
00:43:27,340 --> 00:43:28,430
a property on.

884
00:43:28,430 --> 00:43:30,790
And that matrix looks like this.

885
00:43:30,790 --> 00:43:34,300
Similar to the one you're passing to the
quadratic programming, but without

886
00:43:34,300 --> 00:43:39,920
the y's. What you do, you just list
the value of your kernels on all the

887
00:43:39,920 --> 00:43:43,790
pairs coming from your data set.

888
00:43:43,790 --> 00:43:48,340
So if this was a genuine inner product
and you had it explicitly, each of

889
00:43:48,340 --> 00:43:49,560
these will be the inner product.

890
00:43:49,560 --> 00:43:52,180
This one would be z1 transposed z1.

891
00:43:52,180 --> 00:43:55,930
This would be z2 transposed
z1, et cetera.

892
00:43:55,930 --> 00:44:00,750
And therefore, this thing could be
decomposed as an outer product between

893
00:44:00,750 --> 00:44:03,920
z's standing and z's sitting.

894
00:44:03,920 --> 00:44:05,840
And you will get that.

895
00:44:05,840 --> 00:44:09,630
So the condition here on that matrix,
without visiting the z, is that when

896
00:44:09,630 --> 00:44:14,930
you put these numbers, to your pleasant
surprise, this needs to be positive

897
00:44:14,930 --> 00:44:16,150
semi-definite.

898
00:44:16,150 --> 00:44:19,860
That is, in matrix lingo, this matrix
should be greater than or equal to 0.

899
00:44:19,860 --> 00:44:24,380
That's what positive semi-definite
really means conceptually.

900
00:44:24,380 --> 00:44:31,010
This should be true for any
choice of the points.

901
00:44:31,010 --> 00:44:33,130
And that is Mercer's condition.

902
00:44:33,130 --> 00:44:34,220
Now, we can see the difficulty.

903
00:44:34,220 --> 00:44:38,230
If I want to satisfy that this is true
for any points I choose,

904
00:44:38,230 --> 00:44:41,960
obviously I have to have some math
helping me to corner that this has to

905
00:44:41,960 --> 00:44:44,650
be positive semi-definite
for some reason.

906
00:44:44,650 --> 00:44:46,370
But this is indeed the condition.

907
00:44:46,370 --> 00:44:49,910
And if you look at the case where you
know the transformation into the z and

908
00:44:49,910 --> 00:44:53,350
you put this as an outer product between
a bunch of z's and a bunch of

909
00:44:53,350 --> 00:44:57,250
z's, what you're going to get is
patently positive semi-definite.

910
00:44:57,250 --> 00:44:58,680
Because what is positive semi-definite?

911
00:44:58,680 --> 00:45:03,600
You put a sleeping vector here and
the same vector standing here.

912
00:45:03,600 --> 00:45:05,490
And you're guaranteed to get
a number greater than or equal

913
00:45:05,490 --> 00:45:06,540
to 0 for any vector.

914
00:45:06,540 --> 00:45:08,880
That's what positive
semi-definite means.

915
00:45:08,880 --> 00:45:12,750
If you put that and the matrix happens
to be the outer product of these guys,

916
00:45:12,750 --> 00:45:16,530
then the guy sleeping here gets
multiplied by z, and the other guy is

917
00:45:16,530 --> 00:45:17,680
a transpose of that.

918
00:45:17,680 --> 00:45:20,340
So you get a number squared, and
a number squared is always greater than

919
00:45:20,340 --> 00:45:21,200
or equal to 0.

920
00:45:21,200 --> 00:45:23,610
So the necessity part is obvious.

921
00:45:23,610 --> 00:45:25,830
Sufficiency is a very elaborate
thing to prove.

922
00:45:25,830 --> 00:45:30,030
And actually, it is proved in a fairly
elaborate integral form, not in

923
00:45:30,030 --> 00:45:31,910
a particular realization.

924
00:45:31,910 --> 00:45:33,190
But that is indeed the condition.

925
00:45:33,190 --> 00:45:36,120
And if you manage to establish this
for any kernel, then you establish

926
00:45:36,120 --> 00:45:39,810
that the Z space exists even if you
don't know what the Z space is.

927
00:45:39,810 --> 00:45:42,850


928
00:45:42,850 --> 00:45:44,110
Done with kernels.

929
00:45:44,110 --> 00:45:45,310
That's half the deal.

930
00:45:45,310 --> 00:45:48,550
And now, we are going to the
case where the data is

931
00:45:48,550 --> 00:45:50,390
not linearly separable.

932
00:45:50,390 --> 00:45:55,220
And we still insist on separating
them, with making some errors.

933
00:45:55,220 --> 00:46:00,480
And this brings us back to the old
dichotomy between two types of

934
00:46:00,480 --> 00:46:01,180
non-separable.

935
00:46:01,180 --> 00:46:02,260
We have seen this before.

936
00:46:02,260 --> 00:46:07,010
And this actually turns out to be the
subject of this lecture, if you will.

937
00:46:07,010 --> 00:46:13,530
So if the data is non-separable, that
could be slightly non-separable, like

938
00:46:13,530 --> 00:46:21,510
this, where these guys are just-- you
can take them as here and here.

939
00:46:21,510 --> 00:46:23,490
These are outliers.

940
00:46:23,490 --> 00:46:26,630
I really don't want to go to a high-dimensional
nonlinear space in order

941
00:46:26,630 --> 00:46:29,380
to just go for this guy
and go for this guy.

942
00:46:29,380 --> 00:46:31,660
It doesn't look like a plausible
thing to do.

943
00:46:31,660 --> 00:46:34,810
And even with counting support vectors,
by the time I do this and

944
00:46:34,810 --> 00:46:37,990
come back, I would have touched on so
many points that the chances are the

945
00:46:37,990 --> 00:46:40,300
number of support vectors
would be huge.

946
00:46:40,300 --> 00:46:44,000
So in this case, if there's a method
like the pocket, I would just make

947
00:46:44,000 --> 00:46:46,740
errors on those, accept an E_in
which is non-zero.

948
00:46:46,740 --> 00:46:49,350
But since the generalization is
good, E_out would be OK.

949
00:46:49,350 --> 00:46:53,220
Rather than insist on E_in being 0, and
then go for the generalization error

950
00:46:53,220 --> 00:46:56,730
being huge, because I used something
inordinately complex.

951
00:46:56,730 --> 00:46:58,860
So this is the slightly-case.

952
00:46:58,860 --> 00:47:01,410
And then, there is a seriously
non-separable case,

953
00:47:01,410 --> 00:47:04,050
as in: you get this.

954
00:47:04,050 --> 00:47:05,700
It's not a question of outliers.

955
00:47:05,700 --> 00:47:09,790
The surface is this, and you have to
go to a nonlinear transformation.

956
00:47:09,790 --> 00:47:11,460
Kernels deal with this.

957
00:47:11,460 --> 00:47:15,190


958
00:47:15,190 --> 00:47:20,210
Soft-margin support vector
machines deal with this.

959
00:47:20,210 --> 00:47:27,350
And in all reality, when you deal with
a practical data set, the chance are

960
00:47:27,350 --> 00:47:30,280
the data set will have aspects of both.

961
00:47:30,280 --> 00:47:34,620
It will have a built-in nonlinearity,
and still, even modulo that

962
00:47:34,620 --> 00:47:37,490
nonlinearity, some annoying
guys are there just to test

963
00:47:37,490 --> 00:47:40,030
your learning ability!

964
00:47:40,030 --> 00:47:45,300
And therefore, you will be combining
the kernel with the soft-margin

965
00:47:45,300 --> 00:47:49,470
support vector machines in almost all
the problems that you encounter.

966
00:47:49,470 --> 00:47:52,070
Now, let's focus on this.

967
00:47:52,070 --> 00:47:54,340
I'm now back to the X space.

968
00:47:54,340 --> 00:47:56,190
The data is not linearly separable.

969
00:47:56,190 --> 00:48:01,410
And I want to apply the support
vector machines algorithm,

970
00:48:01,410 --> 00:48:02,590
notwithstanding that.

971
00:48:02,590 --> 00:48:06,260
And after I do that, I'm not going to
even go through the route of: and by

972
00:48:06,260 --> 00:48:09,770
the way, you can transform x into z,
and by the way, you can instead of

973
00:48:09,770 --> 00:48:11,940
going to Z, you do the kernel.
You do that yourself.

974
00:48:11,940 --> 00:48:13,600
I'll just do the basic case.

975
00:48:13,600 --> 00:48:19,630
And you know how to extrapolate, to both
the Z and to the kernel case.

976
00:48:19,630 --> 00:48:23,850
So here is the idea of an error
measure, as we had before.

977
00:48:23,850 --> 00:48:25,555
I'm going to consider the
margin violation.

978
00:48:25,555 --> 00:48:27,670
Let me have a picture
and talk about it.

979
00:48:27,670 --> 00:48:31,390
So when you solve support vector
machines in a linearly separable case,

980
00:48:31,390 --> 00:48:32,540
you maximize the margin.

981
00:48:32,540 --> 00:48:35,510
And these will be the ones
that achieve the margin.

982
00:48:35,510 --> 00:48:38,690
And these guys will be
interior points.

983
00:48:38,690 --> 00:48:42,060
And now, we are going
to consider errors.

984
00:48:42,060 --> 00:48:43,550
There are many ways for
considering errors.

985
00:48:43,550 --> 00:48:47,390
I can consider the number of
points I misclassify.

986
00:48:47,390 --> 00:48:50,900
We realize that it's not a good idea to
deal with the number of points that

987
00:48:50,900 --> 00:48:53,730
are misclassified, because optimization
becomes completely

988
00:48:53,730 --> 00:48:54,730
intractable in this case.

989
00:48:54,730 --> 00:48:56,430
It's a combinatorial optimization.

990
00:48:56,430 --> 00:48:59,540
And we discussed that when we talked
about perceptron and pocket.

991
00:48:59,540 --> 00:49:03,140
And we said that the problem of optimizing--
getting the absolute optimum-- in

992
00:49:03,140 --> 00:49:05,310
this case, is generally NP-hard.

993
00:49:05,310 --> 00:49:08,380
So we are going to have
a numerical value.

994
00:49:08,380 --> 00:49:12,300
And because the margin means something
to me now-- it's not a question of

995
00:49:12,300 --> 00:49:15,090
being on the right side of the line,
it's a question of how far you are

996
00:49:15,090 --> 00:49:18,150
from the line-- that turned out to be
an important notion in support vector

997
00:49:18,150 --> 00:49:23,280
machines, I'm going to define my error
measure based on violating the margin.

998
00:49:23,280 --> 00:49:27,070
So let's see what I mean.

999
00:49:27,070 --> 00:49:30,470
This point that used to be here
has violated the margin.

1000
00:49:30,470 --> 00:49:34,010
Now, I'm not saying that once you put
this here, the same solution will hold

1001
00:49:34,010 --> 00:49:34,450
or whatever.

1002
00:49:34,450 --> 00:49:37,990
I'm just illustrating to you what
is a violation of the margin.

1003
00:49:37,990 --> 00:49:39,270
And how do I quantify it.

1004
00:49:39,270 --> 00:49:40,650
This is just an illustration.

1005
00:49:40,650 --> 00:49:42,590
So this point went in.

1006
00:49:42,590 --> 00:49:45,590
In spite of the fact that it's correctly
classified-- yes, because

1007
00:49:45,590 --> 00:49:49,130
this is the line, and it's on the blue
side of the line, so to speak.

1008
00:49:49,130 --> 00:49:50,950
So there's no change in
terms of the label.

1009
00:49:50,950 --> 00:49:53,450
If I'm working with in-sample
error, nothing has changed.

1010
00:49:53,450 --> 00:49:56,790
But now, I am not achieving the margin
that I want for this point.

1011
00:49:56,790 --> 00:50:02,170
And the amount of violation will be
decided by this displacement.

1012
00:50:02,170 --> 00:50:05,520
So here is what I'm going to do.

1013
00:50:05,520 --> 00:50:08,520
This will be the case if the margin
is satisfied for every point.

1014
00:50:08,520 --> 00:50:11,750
That is the canonical form we put.

1015
00:50:11,750 --> 00:50:15,130
And when this fails, the
margin is violated.

1016
00:50:15,130 --> 00:50:17,430
And I'd like to quantify that.

1017
00:50:17,430 --> 00:50:21,180
The way I'm going to quantify it,
I'm going to introduce a slack for

1018
00:50:21,180 --> 00:50:24,790
every point, potentially every point.
Hopefully, most of them will satisfy

1019
00:50:24,790 --> 00:50:27,360
the margin, only a few of
them will violate it.

1020
00:50:27,360 --> 00:50:31,700
And I'm going to say that the quantity
that used to be greater than or equal

1021
00:50:31,700 --> 00:50:36,340
to 1, is actually greater than
or equal to 1 minus a slack.

1022
00:50:36,340 --> 00:50:37,530
So this is what I will have.

1023
00:50:37,530 --> 00:50:42,420
The movement from here to here
resulted in the red xi.

1024
00:50:42,420 --> 00:50:45,050
And the slack is greater
than or equal to 0.

1025
00:50:45,050 --> 00:50:48,950
I'm only considering violations.

1026
00:50:48,950 --> 00:50:52,900
Now, I'm going to consider--
this is the condition.

1027
00:50:52,900 --> 00:50:56,930
And now, I'm going to penalize you
for the total violation you made.

1028
00:50:56,930 --> 00:50:58,460
What is the total violation?

1029
00:50:58,460 --> 00:51:01,260
I'm just going to add
up these violations.

1030
00:51:01,260 --> 00:51:03,180
We have seen error measures before.

1031
00:51:03,180 --> 00:51:07,300
We know that it's largely hand-waving,
because I have something in mind.

1032
00:51:07,300 --> 00:51:10,550
Either I'm thinking of an optimizer, and
I want to hand something friendly

1033
00:51:10,550 --> 00:51:14,260
to it, or I'm thinking of something
that is analytically plausible.

1034
00:51:14,260 --> 00:51:15,700
This is no different.

1035
00:51:15,700 --> 00:51:17,710
Why did I choose this
instead of squared?

1036
00:51:17,710 --> 00:51:19,170
Why did I choose this instead of that?

1037
00:51:19,170 --> 00:51:21,680
All of these are considerations that
will come up when you see the result

1038
00:51:21,680 --> 00:51:23,060
of choosing this.

1039
00:51:23,060 --> 00:51:24,080
This is reasonable.

1040
00:51:24,080 --> 00:51:26,240
This does seem like violating
the margin.

1041
00:51:26,240 --> 00:51:28,930
This does seem like measuring
the violation of the margin.

1042
00:51:28,930 --> 00:51:32,950
So in the absence of further evidence
one way or the other, this is a good

1043
00:51:32,950 --> 00:51:34,080
error measure to have.

1044
00:51:34,080 --> 00:51:36,980
And then, when I plug this error measure
into what we had, things will

1045
00:51:36,980 --> 00:51:40,290
collapse completely back to where
we solved it already.

1046
00:51:40,290 --> 00:51:43,050
So this is the big advantage here.

1047
00:51:43,050 --> 00:51:46,010
So that is going to be
my error measure.

1048
00:51:46,010 --> 00:51:49,300
So now, the new optimization I'm
going to do is the following.

1049
00:51:49,300 --> 00:51:52,980
It used to be that I was minimizing this,
because minimizing this maximized

1050
00:51:52,980 --> 00:51:53,440
the margin.

1051
00:51:53,440 --> 00:51:56,030
That was what we did the last lecture.

1052
00:51:56,030 --> 00:52:00,490
And now, I'm going to add an error term
that corresponds to the violation

1053
00:52:00,490 --> 00:52:04,540
of the margin, and it is
going to be this.

1054
00:52:04,540 --> 00:52:07,600
So this is the quantity that I promised
you captures the violation of

1055
00:52:07,600 --> 00:52:08,900
the margin.

1056
00:52:08,900 --> 00:52:15,130
And this is a constant that gives me the
relative importance of this term

1057
00:52:15,130 --> 00:52:17,250
versus this term.

1058
00:52:17,250 --> 00:52:22,040
This is no different from our
notion of augmented error.

1059
00:52:22,040 --> 00:52:25,980
In augmented error, we used to have the
in-sample performance, which I guess

1060
00:52:25,980 --> 00:52:27,800
would be the violation
of the margin here.

1061
00:52:27,800 --> 00:52:32,340
If you're violating too much, you'll
start making errors. Plus lambda

1062
00:52:32,340 --> 00:52:35,220
times a regularization term. This
looks pretty much like

1063
00:52:35,220 --> 00:52:37,860
a regularization term, like weight decay.

1064
00:52:37,860 --> 00:52:41,150
So this C is actually 1
over the other lambda.

1065
00:52:41,150 --> 00:52:44,370
But this is a standard formulation
in SVM for a good reason.

1066
00:52:44,370 --> 00:52:47,660
C will appear in a very nice
way in the solution.

1067
00:52:47,660 --> 00:52:50,370
So this is an augmented error. That
gives different weights.

1068
00:52:50,370 --> 00:52:53,400
If I have C close to infinity,
then what am I saying?

1069
00:52:53,400 --> 00:52:56,450
You'd better not violate the margins.

1070
00:52:56,450 --> 00:53:00,750
Because the slightest violation, you
mess up what you're minimizing.

1071
00:53:00,750 --> 00:53:05,440
So the end result is that you're going
to pick xi's, all of them, close to 0.

1072
00:53:05,440 --> 00:53:08,010
And then, the data had better
be linearly separable.

1073
00:53:08,010 --> 00:53:09,610
And that's what you're solving for.

1074
00:53:09,610 --> 00:53:11,680
So we go back to the hard margin.

1075
00:53:11,680 --> 00:53:14,910
If C is very, very small, then
you could be violating the

1076
00:53:14,910 --> 00:53:16,710
margin right and left.

1077
00:53:16,710 --> 00:53:20,100
So nominally, you're getting
a great margin.

1078
00:53:20,100 --> 00:53:22,580
But you're violating
it very frequently.

1079
00:53:22,580 --> 00:53:24,240
And there's a compromise here.

1080
00:53:24,240 --> 00:53:26,300
But that's what you're minimizing.

1081
00:53:26,300 --> 00:53:35,790
Subject to: this is what I had before,
and now the condition adds xi to it.

1082
00:53:35,790 --> 00:53:38,710
So I'm now requiring this
to be the case.

1083
00:53:38,710 --> 00:53:40,930
And I said that xi's are non-negative.

1084
00:53:40,930 --> 00:53:43,670
I'm only penalizing the violating
of the margin.

1085
00:53:43,670 --> 00:53:47,370
I'm not rewarding the anti-violation
of the margin.

1086
00:53:47,370 --> 00:53:50,810
If here's the thing, and one of the
points is there, good for it.

1087
00:53:50,810 --> 00:53:54,220
I'm not going to give it credit that
allows me to violate the other guys

1088
00:53:54,220 --> 00:53:55,720
because that's not going to help me.

1089
00:53:55,720 --> 00:53:58,200
So xi is non-negative.

1090
00:53:58,200 --> 00:54:04,010
And I get this condition for all
points, all N of them.

1091
00:54:04,010 --> 00:54:08,990
And finally, I have the range in which
I'm optimizing, which used to be this.

1092
00:54:08,990 --> 00:54:12,960
And now, I have the xi's being R to the
N. I guess positive, but that is

1093
00:54:12,960 --> 00:54:14,990
captured by the constraint.

1094
00:54:14,990 --> 00:54:20,290
If you look at this slide, take out the
red, and you have the problem you

1095
00:54:20,290 --> 00:54:25,460
already solved, the hard-margin SVM
in the linearly separable case.

1096
00:54:25,460 --> 00:54:27,530
So this is the added guy.

1097
00:54:27,530 --> 00:54:29,930
Now what we're going to do, we're
actually going to go through the

1098
00:54:29,930 --> 00:54:35,650
Lagrangian again, because the Lagrangian
is not that much different

1099
00:54:35,650 --> 00:54:36,210
from before.

1100
00:54:36,210 --> 00:54:38,010
So we can take it as a review.

1101
00:54:38,010 --> 00:54:41,830
And the good thing is that, if you thought
that the terms dropped right and left

1102
00:54:41,830 --> 00:54:45,220
before, wait until you see this one.

1103
00:54:45,220 --> 00:54:48,110
So here is the Lagrange formulation.

1104
00:54:48,110 --> 00:54:53,870
We have L of w, b, and alpha, and some
missing guys that will be filled.

1105
00:54:53,870 --> 00:54:55,870
And we have this and minus that.

1106
00:54:55,870 --> 00:54:58,570
So you can see that it's spread out,
because obviously I'm going to put the

1107
00:54:58,570 --> 00:54:59,580
new stuff in.

1108
00:54:59,580 --> 00:55:05,070
What I put here is exactly the
Lagrangian you worked with before.

1109
00:55:05,070 --> 00:55:06,680
This was your target.

1110
00:55:06,680 --> 00:55:09,350
This was the 0 form of
the inequality--

1111
00:55:09,350 --> 00:55:12,310
that means this minus 1 is greater
than or equal to 0.

1112
00:55:12,310 --> 00:55:15,690
You put that and multiplied it by the
Lagrange multiplier, which is

1113
00:55:15,690 --> 00:55:19,160
non-negative, minus because it's in the
form of greater than or equal to,

1114
00:55:19,160 --> 00:55:21,020
and this is your Lagrangian
that we solved.

1115
00:55:21,020 --> 00:55:24,140
And we ended up with the quadratic
programming problem we had.

1116
00:55:24,140 --> 00:55:28,840
So now, there is a new
guy which is xi.

1117
00:55:28,840 --> 00:55:31,710
That's a new variable that
I'm determining.

1118
00:55:31,710 --> 00:55:34,070
How does it appear in the Lagrangian?

1119
00:55:34,070 --> 00:55:38,430
Well, the target is no longer just
minimizing this, but minimizing this

1120
00:55:38,430 --> 00:55:41,520
plus the other guy that penalizes
the violation of the margin.

1121
00:55:41,520 --> 00:55:42,770
So let's put that.

1122
00:55:42,770 --> 00:55:47,870


1123
00:55:47,870 --> 00:55:52,130
Now, the constraint that I had used to
be: this is greater than or equal to 1.

1124
00:55:52,130 --> 00:55:55,300
So I had it as minus 1 for the 0 form.

1125
00:55:55,300 --> 00:55:59,820
The new constraint is: this is greater
than or equal to 1 minus xi.

1126
00:55:59,820 --> 00:56:03,150
So I need to put the new constant,
and when you put, it's minus

1127
00:56:03,150 --> 00:56:04,600
minus, you get the plus.

1128
00:56:04,600 --> 00:56:07,100
So that's the complete term.

1129
00:56:07,100 --> 00:56:10,470
Now, the other guy is that I have
a bunch of constraints on xi itself.

1130
00:56:10,470 --> 00:56:14,090
I need to put them in
the Lagrangian form.

1131
00:56:14,090 --> 00:56:17,700
And this would be this.

1132
00:56:17,700 --> 00:56:19,550
Not scary at all.

1133
00:56:19,550 --> 00:56:24,350
xi_n is really the constraint
on xi in the 0 form.

1134
00:56:24,350 --> 00:56:26,680
xi is greater than or equal to 0.

1135
00:56:26,680 --> 00:56:29,860
So if I wanted to put it in the 0 form,
then I put xi-- that has to be

1136
00:56:29,860 --> 00:56:32,590
greater than or equal to 0, pretty
much like this fellow had to be

1137
00:56:32,590 --> 00:56:34,390
greater than or equal to 0.

1138
00:56:34,390 --> 00:56:37,160
I need to multiply it by a Lagrange
multiplier, a new guy.

1139
00:56:37,160 --> 00:56:39,560
So I call those guys beta.

1140
00:56:39,560 --> 00:56:42,470
And I do this for all of the
constraints, N of them.

1141
00:56:42,470 --> 00:56:44,420
And I have a minus because this
is in the direction greater

1142
00:56:44,420 --> 00:56:45,370
than or equal to.

1143
00:56:45,370 --> 00:56:47,330
So there's absolutely nothing
different here.

1144
00:56:47,330 --> 00:56:50,030
And now, I add the new Lagrange
multipliers to them,

1145
00:56:50,030 --> 00:56:52,580
and I get this fellow.

1146
00:56:52,580 --> 00:56:56,500
Now, I'm proud of this
because of a reason.

1147
00:56:56,500 --> 00:57:01,070
The slides are wide-screen
for this course, right?

1148
00:57:01,070 --> 00:57:04,750
I had to have an equation that
takes the full width of that.

1149
00:57:04,750 --> 00:57:09,360
And finally, in lecture number
15, I managed to do that!

1150
00:57:09,360 --> 00:57:12,240
Now you say: forget it.

1151
00:57:12,240 --> 00:57:14,080
This is just too complicated.

1152
00:57:14,080 --> 00:57:18,460
Please bear with me, because terms
will be dropping like flies.

1153
00:57:18,460 --> 00:57:23,400
Just follow this and
see where we arrive.

1154
00:57:23,400 --> 00:57:26,560
We're going to minimize this with respect
to w and b, which we used to

1155
00:57:26,560 --> 00:57:31,270
do, and with respect to xi,
which is our new guys.

1156
00:57:31,270 --> 00:57:34,290
Minimize, and then we're going to
maximize with respect to the Lagrange

1157
00:57:34,290 --> 00:57:37,470
multipliers, the alphas which we used
to have, and the betas are the new

1158
00:57:37,470 --> 00:57:39,340
guys in town.

1159
00:57:39,340 --> 00:57:40,730
So let's do the first guy.

1160
00:57:40,730 --> 00:57:45,010
Let's do the minimization with respect
to w, which we did before.

1161
00:57:45,010 --> 00:57:48,660
Can you differentiate this
with respect to w?

1162
00:57:48,660 --> 00:57:51,020
I will get a w here.

1163
00:57:51,020 --> 00:57:53,780
This red guy doesn't contribute.

1164
00:57:53,780 --> 00:57:56,920
Here, I will get what I used to get.

1165
00:57:56,920 --> 00:57:58,850
This guy doesn't interfere.

1166
00:57:58,850 --> 00:58:00,960
This guy doesn't play a role.

1167
00:58:00,960 --> 00:58:02,160
That's encouraging.

1168
00:58:02,160 --> 00:58:07,510
I am actually getting
what I got before.

1169
00:58:07,510 --> 00:58:10,860
Let's do partial by partial b.

1170
00:58:10,860 --> 00:58:13,790


1171
00:58:13,790 --> 00:58:16,060
Does this guy play any role?

1172
00:58:16,060 --> 00:58:17,540
Does this guy play any role?

1173
00:58:17,540 --> 00:58:19,260
This guy gets multiplied by here.

1174
00:58:19,260 --> 00:58:20,180


1175
00:58:20,180 --> 00:58:20,690
Does this guy?

1176
00:58:20,690 --> 00:58:23,940
b doesn't appear here.

1177
00:58:23,940 --> 00:58:25,280
I get exactly what I got before.

1178
00:58:25,280 --> 00:58:28,240


1179
00:58:28,240 --> 00:58:31,570
So the final guy is to get the
partial by partial xi's.

1180
00:58:31,570 --> 00:58:33,800
That's the new guy.

1181
00:58:33,800 --> 00:58:35,550
So you do this.

1182
00:58:35,550 --> 00:58:36,660
Let's see what happens.

1183
00:58:36,660 --> 00:58:37,790
I'll do it one at a time.

1184
00:58:37,790 --> 00:58:39,060
There are N of those.

1185
00:58:39,060 --> 00:58:41,480
I didn't put it as a gradient,
just to make it simple.

1186
00:58:41,480 --> 00:58:43,380
So I do it one at a time.

1187
00:58:43,380 --> 00:58:49,440
And I see xi_n gets multiplied by C.
xi_n gets multiplied by alpha with

1188
00:58:49,440 --> 00:58:51,112
a negative sign.

1189
00:58:51,112 --> 00:58:53,850
xi gets multiplied by a beta
with a negative sign.

1190
00:58:53,850 --> 00:58:57,930
So if I differentiate with respect to
xi, this is what I'm going to get,

1191
00:58:57,930 --> 00:59:00,050
C minus alpha minus beta.

1192
00:59:00,050 --> 00:59:02,730
Equate that with 0.

1193
00:59:02,730 --> 00:59:05,060
Now, isn't that grand?

1194
00:59:05,060 --> 00:59:10,550
Because now, you are saying that this
quantity is always 0 for any n

1195
00:59:10,550 --> 00:59:15,150
from 1 to N. Let's look at the
ramifications as far as the Lagrangian,

1196
00:59:15,150 --> 00:59:16,770
when you substitute in the Lagrangian.

1197
00:59:16,770 --> 00:59:21,160
Here, I have a C. Here,
I have a minus alpha.

1198
00:59:21,160 --> 00:59:23,490
Here, I have a minus beta.

1199
00:59:23,490 --> 00:59:25,480
These are multiplied by xi.

1200
00:59:25,480 --> 00:59:28,650
That combination happens to be 0.

1201
00:59:28,650 --> 00:59:35,950
So conveniently, this guy and this guy
and this guy, together with beta, are

1202
00:59:35,950 --> 00:59:37,900
dropping out.

1203
00:59:37,900 --> 00:59:42,810
We are back to exactly the same
Lagrangian we had before, with exactly

1204
00:59:42,810 --> 00:59:45,520
the same solution we had before.

1205
00:59:45,520 --> 00:59:47,560
And what happened to beta?

1206
00:59:47,560 --> 00:59:49,150
Well, beta did its service.

1207
00:59:49,150 --> 00:59:52,880
And we thank it for its great service,
and we bid farewell!

1208
00:59:52,880 --> 00:59:54,310
It's gone.

1209
00:59:54,310 --> 00:59:58,580
The only ramification of beta that we
have is that because beta is greater

1210
00:59:58,580 --> 01:00:03,940
than or equal to 0, and we have this
condition, alpha is not only greater

1211
01:00:03,940 --> 01:00:09,590
than or equal to 0, which it used to
be. It also cannot be bigger than C,

1212
01:00:09,590 --> 01:00:13,030
because if it's bigger than C, this
quantity becomes negative.

1213
01:00:13,030 --> 01:00:17,440
And all of a sudden, I cannot find
a legitimate beta to make this true.

1214
01:00:17,440 --> 01:00:21,990
So the only thing out of all of this
adventure is that we're going to

1215
01:00:21,990 --> 01:00:27,390
require that alpha be at most C.
Everything before, plus this added

1216
01:00:27,390 --> 01:00:32,130
condition, that's the whole thing.

1217
01:00:32,130 --> 01:00:34,670
So you get the solution.

1218
01:00:34,670 --> 01:00:36,190
You get this.

1219
01:00:36,190 --> 01:00:42,060
That's what we saw before with respect
to alpha, and beta doesn't appear.

1220
01:00:42,060 --> 01:00:46,400
And you have alphas being non-negative
with the added red condition. That's

1221
01:00:46,400 --> 01:00:49,950
the only thing which is added,
less than or equal to C.

1222
01:00:49,950 --> 01:00:51,740
The equality constraint is there.

1223
01:00:51,740 --> 01:00:54,410
The equality constraint, that is
inherited from the condition from the

1224
01:00:54,410 --> 01:00:57,880
previous slide, same as we did before.

1225
01:00:57,880 --> 01:01:02,820
And when you get the solution,
w will be this.

1226
01:01:02,820 --> 01:01:08,200
And w will guarantee that
you're minimizing this

1227
01:01:08,200 --> 01:01:11,350
plus the new objective.

1228
01:01:11,350 --> 01:01:15,440
So if you already wrote your routine
in order to apply support

1229
01:01:15,440 --> 01:01:19,100
vector machines, all you need to do now
is go to that routine, and instead

1230
01:01:19,100 --> 01:01:22,240
of 0 less than or equal to alpha less
than or equal to infinity, make it 0

1231
01:01:22,240 --> 01:01:25,960
less than or equal to alpha less than
or equal to C. And you have the soft

1232
01:01:25,960 --> 01:01:28,820
margin support vector machines.

1233
01:01:28,820 --> 01:01:31,630
That is a good bargain.

1234
01:01:31,630 --> 01:01:36,140
Let's look at, just very quickly,
types of support vectors.

1235
01:01:36,140 --> 01:01:38,240
This is the picture.

1236
01:01:38,240 --> 01:01:41,020
And this is the picture where you
have support vectors in the hard

1237
01:01:41,020 --> 01:01:41,770
margin case.

1238
01:01:41,770 --> 01:01:45,570
There are only two types of points here,
interior, where the margin is

1239
01:01:45,570 --> 01:01:50,130
greater than 1 strictly, and the boundary
guys that happen to be support vectors

1240
01:01:50,130 --> 01:01:52,820
where the margin is exactly 1, or at
least the quantity that corresponds to

1241
01:01:52,820 --> 01:01:54,850
the margin is exactly 1.

1242
01:01:54,850 --> 01:01:57,300
And that is all I have.

1243
01:01:57,300 --> 01:02:04,000
Now, when you have the soft version,
we are going to label these guys

1244
01:02:04,000 --> 01:02:07,460
margin support vectors, just because
there will be other guys that violate

1245
01:02:07,460 --> 01:02:08,510
the margin.

1246
01:02:08,510 --> 01:02:09,700
And they will be support vectors.

1247
01:02:09,700 --> 01:02:14,370
They will get Lagrange multipliers
that are greater than 1.

1248
01:02:14,370 --> 01:02:19,120
And in this case, the margin support
vectors that used to be just alpha

1249
01:02:19,120 --> 01:02:24,450
greater than 0, they also happen to be
strictly less than C. You can look at

1250
01:02:24,450 --> 01:02:28,330
it independently in order to understand,
but let me just give you

1251
01:02:28,330 --> 01:02:30,310
the hint here.

1252
01:02:30,310 --> 01:02:38,540
When alpha hits C, beta, the
lost multiplier, hits 0.

1253
01:02:38,540 --> 01:02:43,770
We know when the Lagrange multiplier
hits 0, the corresponding slack has to

1254
01:02:43,770 --> 01:02:44,980
become positive.

1255
01:02:44,980 --> 01:02:47,810
That was one of the conditions we had.

1256
01:02:47,810 --> 01:02:52,610
And therefore, because here the slack
is 0, you actually don't have xi.

1257
01:02:52,610 --> 01:02:53,590
xi is 0.

1258
01:02:53,590 --> 01:02:58,110
You have to be clear of C. That
is the reason for it.

1259
01:02:58,110 --> 01:03:00,560
So this is the condition
for those guys.

1260
01:03:00,560 --> 01:03:02,070
And xi is 0.

1261
01:03:02,070 --> 01:03:06,740
And these are the guys you used to
solve in order to get the b.

1262
01:03:06,740 --> 01:03:09,900
These are as clean
as they used to be.

1263
01:03:09,900 --> 01:03:15,120
Now, we add the non-margin
support vectors.

1264
01:03:15,120 --> 01:03:19,220
And by those, we mean that now alpha_n
equals C. So it's positive.

1265
01:03:19,220 --> 01:03:20,160
They are support vectors.

1266
01:03:20,160 --> 01:03:21,990
Alpha_n is greater than 0.

1267
01:03:21,990 --> 01:03:24,790
But they happen to hit C.
And now, I have a slack.

1268
01:03:24,790 --> 01:03:28,140
The slack xi starts becoming positive.

1269
01:03:28,140 --> 01:03:31,830
And therefore, the margin is violated.

1270
01:03:31,830 --> 01:03:33,540
I'm less than 1.

1271
01:03:33,540 --> 01:03:35,860
So that's 1 minus xi,
and xi is positive.

1272
01:03:35,860 --> 01:03:39,130
Indeed, xi is positive in this case.

1273
01:03:39,130 --> 01:03:43,110
So let's look at those non-margin
support vectors, and see

1274
01:03:43,110 --> 01:03:44,490
what they look like.

1275
01:03:44,490 --> 01:03:48,640
Again, just for illustration, I'm
going to take these two points and

1276
01:03:48,640 --> 01:03:52,130
start making them violate the margin,
not that the new solution will be

1277
01:03:52,130 --> 01:03:54,080
exactly the same except that
these guys are inside.

1278
01:03:54,080 --> 01:03:56,230
You have to re-solve it
with C, et cetera.

1279
01:03:56,230 --> 01:04:00,130
But I'm just illustrating, just
to show you the point.

1280
01:04:00,130 --> 01:04:02,320
So this is one way to
violate the margin.

1281
01:04:02,320 --> 01:04:06,000
You violated the margin, but you're
still classifying them correctly.

1282
01:04:06,000 --> 01:04:09,690
These are non-margin support
vectors, one type.

1283
01:04:09,690 --> 01:04:12,860
You can violate it further, and cross.

1284
01:04:12,860 --> 01:04:15,360
So now, these points
are misclassified.

1285
01:04:15,360 --> 01:04:17,480
And they are still non-margin
support vectors.

1286
01:04:17,480 --> 01:04:19,690
And now, the E_in is affected.

1287
01:04:19,690 --> 01:04:21,410
And you can go wild.

1288
01:04:21,410 --> 01:04:25,040
And you're just completely deep.

1289
01:04:25,040 --> 01:04:28,710
As long as you're violating,
you are a support vector.

1290
01:04:28,710 --> 01:04:30,995
Not a clean support vector, but
a support vector nonetheless.

1291
01:04:30,995 --> 01:04:33,610


1292
01:04:33,610 --> 01:04:38,070
Now, the value of C is a very important
parameter here because it

1293
01:04:38,070 --> 01:04:43,200
tells us how much violation we have,
versus the width of the yellow region.

1294
01:04:43,200 --> 01:04:47,790
And this is a quantity that will be
decided in a practical problem using

1295
01:04:47,790 --> 01:04:53,430
old-fashioned cross-validation.

1296
01:04:53,430 --> 01:04:56,810
This is a point, a parameter
that we need to determine.

1297
01:04:56,810 --> 01:04:59,880
And whenever we have one parameter to
determine, and we want to pick it

1298
01:04:59,880 --> 01:05:02,060
optimally, we can use
cross-validation.

1299
01:05:02,060 --> 01:05:05,760
So as you see, validation and
cross-validation are a layer on top of this.

1300
01:05:05,760 --> 01:05:09,240
Here, I'm using a very elaborate
algorithm which is support vector

1301
01:05:09,240 --> 01:05:13,500
machines, yet I am resorting to
cross-validation in order to

1302
01:05:13,500 --> 01:05:15,165
determine C.

1303
01:05:15,165 --> 01:05:20,870
I'll make two quick technical
remarks and end the lecture here.

1304
01:05:20,870 --> 01:05:23,790
These are just practical points
in case they bother you.

1305
01:05:23,790 --> 01:05:26,800
If you didn't see them, they are
not going to bother you.

1306
01:05:26,800 --> 01:05:28,970
Here is the idea.

1307
01:05:28,970 --> 01:05:31,060
With the hard margin, I
apply this machinery.

1308
01:05:31,060 --> 01:05:31,920
I get the dual, pass it

1309
01:05:31,920 --> 01:05:34,120
to quadratic programming.

1310
01:05:34,120 --> 01:05:39,820
So I'm asking myself, if the data is
not linearly separable, what gives?

1311
01:05:39,820 --> 01:05:40,720
Because think about it.

1312
01:05:40,720 --> 01:05:43,070
I never told you to check that the
data is linearly separable.

1313
01:05:43,070 --> 01:05:46,930
I gave you the data, formulated--
minimize this subject to that.

1314
01:05:46,930 --> 01:05:49,810
Now, if the data is not linearly
separable, subject to that will be

1315
01:05:49,810 --> 01:05:50,775
impossible to satisfy.

1316
01:05:50,775 --> 01:05:53,180
There will be no feasible solution.

1317
01:05:53,180 --> 01:05:56,390
Nonetheless, this didn't prevent me from
getting a dual and passing it to

1318
01:05:56,390 --> 01:05:57,700
quadratic programming.

1319
01:05:57,700 --> 01:05:59,970
And maybe quadratic programming
will give me back a solution.

1320
01:05:59,970 --> 01:06:03,240
So now, I'm in a strange world.

1321
01:06:03,240 --> 01:06:08,960
The key thing to realize is that the
translation from the primary form,

1322
01:06:08,960 --> 01:06:14,750
minimizing w transposed w, to the dual
form, maximizing with respect to alpha--

1323
01:06:14,750 --> 01:06:20,110
the Lagrangian-- that step is
mathematically valid only if there is

1324
01:06:20,110 --> 01:06:21,920
a feasible solution.

1325
01:06:21,920 --> 01:06:23,790
The KKT conditions are necessary.

1326
01:06:23,790 --> 01:06:26,480
So they have to be satisfied
if the point is there.

1327
01:06:26,480 --> 01:06:30,130
Obviously if there is no point in the
domain, then I'm now working pretty

1328
01:06:30,130 --> 01:06:33,080
much like the guy who improvised
a kernel that does not

1329
01:06:33,080 --> 01:06:34,460
correspond to a Z space.

1330
01:06:34,460 --> 01:06:36,360
Yeah, you can plug it in
and get a solution.

1331
01:06:36,360 --> 01:06:37,750
No guarantees there.

1332
01:06:37,750 --> 01:06:41,850
In this case, actually, if you go
to the Lagrangian, the quadratic

1333
01:06:41,850 --> 01:06:43,480
programming will try to converge to

1334
01:06:43,480 --> 01:06:45,840
something in infinity.

1335
01:06:45,840 --> 01:06:48,140
But you need not to worry
about this case at all.

1336
01:06:48,140 --> 01:06:51,740
Let's say that you have a buggy
quadratic programming.

1337
01:06:51,740 --> 01:06:55,280
And you innocently translate
the problem into the dual.

1338
01:06:55,280 --> 01:06:56,990
You pass on to quadratic programming.

1339
01:06:56,990 --> 01:06:59,900
Quadratic program passes
alphas back to you.

1340
01:06:59,900 --> 01:07:02,410
Now, it's impossible that all of
a sudden, the data became linearly

1341
01:07:02,410 --> 01:07:03,680
separable, right?

1342
01:07:03,680 --> 01:07:04,890
You don't have to worry.

1343
01:07:04,890 --> 01:07:08,400
You can always check if the solution
separates the data.

1344
01:07:08,400 --> 01:07:12,720
You can evaluate the solution on every
point, compare it with the label.

1345
01:07:12,720 --> 01:07:16,240
And when you realize that it's not
agreeing with the label, you realize

1346
01:07:16,240 --> 01:07:17,720
that something is wrong.

1347
01:07:17,720 --> 01:07:20,230
So you don't have to go through the
combinatorial problem: is this

1348
01:07:20,230 --> 01:07:22,090
linearly separable in the first place?

1349
01:07:22,090 --> 01:07:25,220
Should I run the perceptron first
to see if it converges before?

1350
01:07:25,220 --> 01:07:26,210
No, no, no, no.

1351
01:07:26,210 --> 01:07:28,740
Just be lazy if you want,
and go through this.

1352
01:07:28,740 --> 01:07:30,300
And when it comes back, check.

1353
01:07:30,300 --> 01:07:32,140
If it's linearly separable,
things are valid.

1354
01:07:32,140 --> 01:07:33,400
There is a feasible solution.

1355
01:07:33,400 --> 01:07:35,650
The dual solution is valid and
quadratic programming works.

1356
01:07:35,650 --> 01:07:37,350
If it's not, then something
went wrong.

1357
01:07:37,350 --> 01:07:40,080
Chances are you won't get to that stage,
because quadratic programming

1358
01:07:40,080 --> 01:07:41,040
will be complaining.

1359
01:07:41,040 --> 01:07:44,410
But quadratic programming will be
complaining anyway, as you may have

1360
01:07:44,410 --> 01:07:46,640
experienced when you tried it.

1361
01:07:46,640 --> 01:07:49,980
Tells you: ill conditioned, that, there,
and sometimes you have tweaks.

1362
01:07:49,980 --> 01:07:52,890
Let me put a bound on this,
in order not to make it go into

1363
01:07:52,890 --> 01:07:54,190
a bad region and whatnot.

1364
01:07:54,190 --> 01:07:56,230
So it's not a perfect package.

1365
01:07:56,230 --> 01:08:00,300
So this is just a reminder that we will
never be susceptible to a big

1366
01:08:00,300 --> 01:08:04,130
mistake, like getting a solution
when none exists.

1367
01:08:04,130 --> 01:08:08,750
The last point is when we transform to
the Z space, you may have noticed that

1368
01:08:08,750 --> 01:08:14,340
some of the transformations had
a constant coordinate, 1.

1369
01:08:14,340 --> 01:08:19,250
1 in our mind used to
correspond to w_0.

1370
01:08:19,250 --> 01:08:22,450
We made it a point at the beginning
of discussing support vectors

1371
01:08:22,450 --> 01:08:24,090
that there is no w_0.

1372
01:08:24,090 --> 01:08:26,689
We took it out and called
it b, the bias.

1373
01:08:26,689 --> 01:08:28,870
We treated it differently.

1374
01:08:28,870 --> 01:08:32,340
So now, we are working
with both w_0 and b.

1375
01:08:32,340 --> 01:08:35,109
Because if you have a constant,
you may not call it w_0.

1376
01:08:35,109 --> 01:08:36,130
But effectively, it's w_0.

1377
01:08:36,130 --> 01:08:38,720
It's the guy that gets multiplied
by the constant.

1378
01:08:38,720 --> 01:08:39,439
So what gives?

1379
01:08:39,439 --> 01:08:42,460
Now, I have two guys that
play the same role.

1380
01:08:42,460 --> 01:08:46,460
And you don't have to worry about
that. Have the Z space have 20

1381
01:08:46,460 --> 01:08:48,210
constant coordinates.

1382
01:08:48,210 --> 01:08:49,100
Don't worry about it.

1383
01:08:49,100 --> 01:08:50,390
Because of what?

1384
01:08:50,390 --> 01:08:53,700
Because when you get the solution,
all of the corresponding

1385
01:08:53,700 --> 01:08:55,229
weights will go to 0.

1386
01:08:55,229 --> 01:08:58,930
And all the bulk of the
bias will go to the b.

1387
01:08:58,930 --> 01:09:00,540
How do I know that?

1388
01:09:00,540 --> 01:09:06,020
Because you are charged for the size
of w_0 because it's part of w.

1389
01:09:06,020 --> 01:09:09,590
You are minimizing half w transposed w.

1390
01:09:09,590 --> 01:09:12,180
You're not charged for the size of b.

1391
01:09:12,180 --> 01:09:14,720
So obviously, if you want to minimize,
and you can do it with both,

1392
01:09:14,720 --> 01:09:17,140
everything will go to the b,
and this guy will go to 0.

1393
01:09:17,140 --> 01:09:19,399
So no need to worry about that.

1394
01:09:19,399 --> 01:09:23,210
With that, we'll stop here,
and we'll take questions

1395
01:09:23,210 --> 01:09:24,460
after a short break.

1396
01:09:24,460 --> 01:09:28,279


1397
01:09:28,279 --> 01:09:31,569
So let's start the Q&amp;A. And
we have an in-house question.

1398
01:09:31,569 --> 01:09:38,460


1399
01:09:38,460 --> 01:09:39,109
STUDENT: Hi.

1400
01:09:39,109 --> 01:09:44,779
It seems intuitive to me that the
number of support vectors goes

1401
01:09:44,779 --> 01:09:49,439
linearly with the dimension of the
space that you're looking at.

1402
01:09:49,439 --> 01:09:54,840
For example, in an N-dimensional
Euclidean space, you need N-vectors to

1403
01:09:54,840 --> 01:10:00,560
define an N-minus-1 dimensional hyperplane,
and one other to define the

1404
01:10:00,560 --> 01:10:02,875
thickness of the fat plane, right?

1405
01:10:02,875 --> 01:10:09,690
PROFESSOR: It's not that
easy, because I could get two clusters

1406
01:10:09,690 --> 01:10:14,260
that are far away of points, and then
two points that are +1 and -1

1407
01:10:14,260 --> 01:10:16,150
that are close to each other.

1408
01:10:16,150 --> 01:10:19,000
And in order to separate, I have
to be sandwiched here.

1409
01:10:19,000 --> 01:10:25,230
And then, I am guided by these guys--
I have the orientation here.

1410
01:10:25,230 --> 01:10:28,890
And the orientation is decided by the
two points that are around me.

1411
01:10:28,890 --> 01:10:34,210
So in spite of the fact that in a general
case, I will do that, I could

1412
01:10:34,210 --> 01:10:38,910
construct cases where it's not
linear in the dimension.

1413
01:10:38,910 --> 01:10:39,700
Let's put it this way.

1414
01:10:39,700 --> 01:10:41,730
STUDENT: So you're saying
it's less than linear?

1415
01:10:41,730 --> 01:10:42,990
It's better than linear?

1416
01:10:42,990 --> 01:10:45,260
PROFESSOR: It's better.

1417
01:10:45,260 --> 01:10:49,090
And obviously if it was completely
linear, and I transform it to the

1418
01:10:49,090 --> 01:10:50,930
infinite-dimensional space, I obviously
would be in trouble.

1419
01:10:50,930 --> 01:10:52,510
STUDENT: Yes, that's my aim.

1420
01:10:52,510 --> 01:10:54,470
PROFESSOR: Ah, so that was
that question, yes.

1421
01:10:54,470 --> 01:10:57,070
STUDENT: But it should go positively
with the dimension.

1422
01:10:57,070 --> 01:10:59,355
PROFESSOR: It's likely
to increase with

1423
01:10:59,355 --> 01:11:01,310
the increasing dimension.

1424
01:11:01,310 --> 01:11:07,900
And the exact form depends on the data
set, and depends on the position of the

1425
01:11:07,900 --> 01:11:09,320
data set, including the
interior points.

1426
01:11:09,320 --> 01:11:13,390


1427
01:11:13,390 --> 01:11:14,030
Let me put it this way.

1428
01:11:14,030 --> 01:11:17,200
If I give you, even without considering
the interior points.

1429
01:11:17,200 --> 01:11:21,560
Let's say I give you two points,
one +1 and one -1.

1430
01:11:21,560 --> 01:11:23,890
There is an optimal separating plane.

1431
01:11:23,890 --> 01:11:27,440
And how many support vectors
am I going to get?

1432
01:11:27,440 --> 01:11:30,070
I cannot get more than two because I
only have two, even if I go to 100

1433
01:11:30,070 --> 01:11:30,990
dimension space.

1434
01:11:30,990 --> 01:11:37,690
So the linearity is an impression that
requires further assumptions.

1435
01:11:37,690 --> 01:11:39,435
But in general, it will not hold.

1436
01:11:39,435 --> 01:11:45,240
STUDENT: Yes and for example, the RBF
kernel, it may in its form look like

1437
01:11:45,240 --> 01:11:46,080
infinite-dimensional.

1438
01:11:46,080 --> 01:11:50,900
But in reality, I think its effective
dimension is very small because the

1439
01:11:50,900 --> 01:11:54,890
higher-order terms decay very fast.

1440
01:11:54,890 --> 01:11:59,590
So with both an exponential term
and a factorial term--

1441
01:11:59,590 --> 01:12:01,405
PROFESSOR: Completely agree, and
indeed that affects it because you

1442
01:12:01,405 --> 01:12:04,365
are actually measuring a distance
proper, a Euclidean distance proper in

1443
01:12:04,365 --> 01:12:05,070
that space.

1444
01:12:05,070 --> 01:12:08,830
So if a dimension is very small, then
it doesn't affect it very much.

1445
01:12:08,830 --> 01:12:12,280


1446
01:12:12,280 --> 01:12:18,030
Whether it's really infinite dimension
or infinite dimension in disguise.

1447
01:12:18,030 --> 01:12:20,120
In general, when you have an infinite-dimensional
space, the only way to

1448
01:12:20,120 --> 01:12:23,470
really define an inner product
is to have a decaying term so

1449
01:12:23,470 --> 01:12:25,070
that the thing converges.

1450
01:12:25,070 --> 01:12:27,600
So this is essential when
you want to compute it.

1451
01:12:27,600 --> 01:12:30,250
STUDENT: So it doesn't converge, so
you changed the negative sign in the

1452
01:12:30,250 --> 01:12:32,870
RBF kernel into positive signs?

1453
01:12:32,870 --> 01:12:34,610
Then, it won't be a valid kernel.

1454
01:12:34,610 --> 01:12:36,960
PROFESSOR: But the inner product now
is not well defined, right?

1455
01:12:36,960 --> 01:12:39,800
STUDENT: So it won't be a valid
kernel, and you'll get horrible--

1456
01:12:39,800 --> 01:12:42,560
PROFESSOR: Yeah, because a valid
kernel, I have to be able to

1457
01:12:42,560 --> 01:12:43,720
evaluate the inner product.

1458
01:12:43,720 --> 01:12:48,720
And lack of convergence would
not allow that, right?

1459
01:12:48,720 --> 01:12:50,080
STUDENT: Yeah, OK, thanks.

1460
01:12:50,080 --> 01:12:51,330
PROFESSOR: Absolutely.

1461
01:12:51,330 --> 01:12:53,314


1462
01:12:53,314 --> 01:12:55,050
MODERATOR: People are curious.

1463
01:12:55,050 --> 01:13:01,020
How can you generalize SVM's
to a regression case?

1464
01:13:01,020 --> 01:13:05,710
PROFESSOR: There's a huge body of
knowledge for generalizing it.

1465
01:13:05,710 --> 01:13:07,700
And I didn't touch on
it for two reasons.

1466
01:13:07,700 --> 01:13:11,800
Again pretty much like when I did the
VC analysis, it's more technical.

1467
01:13:11,800 --> 01:13:16,060
And I get the basic concept without
having to go through the technicality.

1468
01:13:16,060 --> 01:13:20,580
The other aspect is that the major
success of support vector machines is

1469
01:13:20,580 --> 01:13:22,890
really in classification.

1470
01:13:22,890 --> 01:13:27,790
They're not as successful, competitively,
in regression.

1471
01:13:27,790 --> 01:13:29,630
That's the practical experience.

1472
01:13:29,630 --> 01:13:37,060
So I have found that it's not worth the
amount of time to go into that.

1473
01:13:37,060 --> 01:13:40,500
MODERATOR: So is it safe to assume,
then, that if you do the

1474
01:13:40,500 --> 01:13:43,370
transformation to an infinite-dimensional
space, the data will be

1475
01:13:43,370 --> 01:13:45,880
linearly separable there?

1476
01:13:45,880 --> 01:13:47,850
PROFESSOR: It is
safe but not certain.

1477
01:13:47,850 --> 01:13:52,050
I can create situations that
are opposed to that.

1478
01:13:52,050 --> 01:13:55,740
But again, this is one of the reasons
why I made the final remarks there.

1479
01:13:55,740 --> 01:13:59,790
Because let's say that I took my data
and applied RBF, I needn't know

1480
01:13:59,790 --> 01:14:02,040
whether they would be linearly separable
in that space or not.

1481
01:14:02,040 --> 01:14:05,970
I just apply the machinery, but I can
always find the solution back and see

1482
01:14:05,970 --> 01:14:09,300
if the points are classified
correctly.

1483
01:14:09,300 --> 01:14:11,790
MODERATOR: So a technical question
on the quadratic programming.

1484
01:14:11,790 --> 01:14:17,910
Usually, if the matrix you give is
not positive definite, there will be

1485
01:14:17,910 --> 01:14:19,530
complaints by the--

1486
01:14:19,530 --> 01:14:23,250
or there would not be complaints
in particular?

1487
01:14:23,250 --> 01:14:25,430
PROFESSOR: My experience with quadratic
programming, there are tons

1488
01:14:25,430 --> 01:14:26,130
of packages there.

1489
01:14:26,130 --> 01:14:30,570
So I'm describing a subset of them
necessarily, the ones I tried.

1490
01:14:30,570 --> 01:14:32,490
They tend to complain.

1491
01:14:32,490 --> 01:14:38,040
And it's almost like when you use MatLab
and it tells you-- please get me

1492
01:14:38,040 --> 01:14:38,840
the inverse.

1493
01:14:38,840 --> 01:14:43,190
And it tells you the condition
number is bad, and so on.

1494
01:14:43,190 --> 01:14:47,750
So in most of the cases, even with the
complaints, the solution is fine.

1495
01:14:47,750 --> 01:14:52,230
It just has a certain reliability
that it has to have in

1496
01:14:52,230 --> 01:14:53,330
order not to complain.

1497
01:14:53,330 --> 01:14:56,870
So invariably, when you use quadratic
programming, there will be a complaint

1498
01:14:56,870 --> 01:14:58,530
one way or another.

1499
01:14:58,530 --> 01:15:02,840
But I have learned not to be completely
discouraged by that, and

1500
01:15:02,840 --> 01:15:04,780
tweak, limit variables, and whatnot.

1501
01:15:04,780 --> 01:15:07,440
But this is just completely
a practical situation,

1502
01:15:07,440 --> 01:15:10,080
depending on the package.

1503
01:15:10,080 --> 01:15:11,660
MODERATOR: Going back to
a previous question.

1504
01:15:11,660 --> 01:15:15,920
So when you said safe but not certain,
so does that mean just in very

1505
01:15:15,920 --> 01:15:18,500
degenerate cases, or--?

1506
01:15:18,500 --> 01:15:23,360
PROFESSOR: With a real data set
that is not completely

1507
01:15:23,360 --> 01:15:29,530
ridiculous, I have never
seen it happen.

1508
01:15:29,530 --> 01:15:30,980
In some sense, you can have--

1509
01:15:30,980 --> 01:15:33,840
especially with the radial basis function,
you can have one on top of each point,

1510
01:15:33,840 --> 01:15:37,790
so you can separate whatever is there.

1511
01:15:37,790 --> 01:15:39,650
So I have not encountered it.

1512
01:15:39,650 --> 01:15:45,690


1513
01:15:45,690 --> 01:15:46,280
MODERATOR: Another question.

1514
01:15:46,280 --> 01:15:50,280
Is it possible to combine kernels to
produce new, different kernels?

1515
01:15:50,280 --> 01:15:53,450
How useful is it to do
these kind of things?

1516
01:15:53,450 --> 01:15:55,440
PROFESSOR: You can do it as long
as the combination is legitimate,

1517
01:15:55,440 --> 01:15:57,780
that it maintains that there
is a Z space in which

1518
01:15:57,780 --> 01:15:58,970
this is an inner product.

1519
01:15:58,970 --> 01:16:02,720
That's really the requirement.

1520
01:16:02,720 --> 01:16:10,470
If you have that, then there are many
variations of the methods, the basic

1521
01:16:10,470 --> 01:16:11,790
SVM method that are in the literature.

1522
01:16:11,790 --> 01:16:13,760
People tried several things.

1523
01:16:13,760 --> 01:16:18,700
And as long as what you're doing is
legitimate, so that you have the

1524
01:16:18,700 --> 01:16:23,810
generalization guarantees
of SVM, it can be done.

1525
01:16:23,810 --> 01:16:26,060
MODERATOR: Since we're only talking
about inner products and they

1526
01:16:26,060 --> 01:16:32,410
usually induce a norm, are we always
preferring the Euclidean norm, or can

1527
01:16:32,410 --> 01:16:34,610
it still be changed and still
use inner products?

1528
01:16:34,610 --> 01:16:37,420
PROFESSOR: The way I derived
it is based on Euclidean norm and

1529
01:16:37,420 --> 01:16:39,530
straightforward inner product.

1530
01:16:39,530 --> 01:16:43,660
There are obviously variations of that
that you can get, and still have the

1531
01:16:43,660 --> 01:16:47,150
machinery go through with
modified quantities.

1532
01:16:47,150 --> 01:16:49,010
So it's not impossible.

1533
01:16:49,010 --> 01:16:52,415
But you just need to make sure that
the quadratic programming problem

1534
01:16:52,415 --> 01:16:56,740
you're solving corresponds to the
version of the norm that you used and

1535
01:16:56,740 --> 01:16:59,670
the version of the inner
product that you used.

1536
01:16:59,670 --> 01:17:03,610
MODERATOR: What would you say is the
scale of the problems that can be

1537
01:17:03,610 --> 01:17:07,765
solved by SVM's, in the number
of points?

1538
01:17:07,765 --> 01:17:10,390
PROFESSOR: The scale of problems
that can be solved by

1539
01:17:10,390 --> 01:17:12,770
quadratic programming is a more pointed
question, because that's the

1540
01:17:12,770 --> 01:17:14,300
bottleneck.

1541
01:17:14,300 --> 01:17:22,600
It depends on if you're using MatLab
versus something else, they get

1542
01:17:22,600 --> 01:17:24,970
saturated at different stages.

1543
01:17:24,970 --> 01:17:28,860
I would say if you get to 10,000 points,
that's pretty formidable.

1544
01:17:28,860 --> 01:17:32,600
And if you're below 1000,
you should be OK.

1545
01:17:32,600 --> 01:17:37,010
But some packages will still
give you a hard time.

1546
01:17:37,010 --> 01:17:40,580
There are packages specifically
for SVM that use heuristics.

1547
01:17:40,580 --> 01:17:43,810
So they don't specifically pass on the
thing to quadratic programming

1548
01:17:43,810 --> 01:17:47,510
directly, but try to break it into
pieces, get support vectors for each

1549
01:17:47,510 --> 01:17:50,090
case, and then get the union
and so on, the hierarchical

1550
01:17:50,090 --> 01:17:51,300
methods and other methods.

1551
01:17:51,300 --> 01:17:55,080
So they are basically heuristic
methods for solving SVM when

1552
01:17:55,080 --> 01:17:57,070
straightforward quadratic
programming will fail.

1553
01:17:57,070 --> 01:18:00,750
And these are also available, and should
be used when you have too many

1554
01:18:00,750 --> 01:18:02,000
data points.

1555
01:18:02,000 --> 01:18:07,130


1556
01:18:07,130 --> 01:18:08,910
MODERATOR: I think that's it.

1557
01:18:08,910 --> 01:18:09,180
PROFESSOR: Very good.

1558
01:18:09,180 --> 01:18:10,430
We'll see you on Thursday.

1559
01:18:10,430 --> 01:18:18,866

