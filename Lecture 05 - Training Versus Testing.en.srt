1
00:00:00,000 --> 00:00:00,570


2
00:00:00,570 --> 00:00:03,270
ANNOUNCER: The following program
is brought to you by Caltech.

3
00:00:03,270 --> 00:00:15,410


4
00:00:15,410 --> 00:00:16,660
YASER ABU-MOSTAFA: Welcome back.

5
00:00:16,660 --> 00:00:19,150


6
00:00:19,150 --> 00:00:23,580
Last time, we talked about
error and noise.

7
00:00:23,580 --> 00:00:27,940
And these are two notions that relate
the learning problem to practical

8
00:00:27,940 --> 00:00:30,540
situations.

9
00:00:30,540 --> 00:00:36,050
In the case of error measures, we
realized that in order to specify the

10
00:00:36,050 --> 00:00:40,360
error that is caused by your hypothesis,
we should try to estimate

11
00:00:40,360 --> 00:00:45,820
the cost of using your h, instead of f
which should have been used in the

12
00:00:45,820 --> 00:00:47,050
first place.

13
00:00:47,050 --> 00:00:50,920
And that is something the user can
specify, of the price to pay when they

14
00:00:50,920 --> 00:00:52,780
use h instead of f.

15
00:00:52,780 --> 00:00:57,450
And that is the principled way
of defining an error measure.

16
00:00:57,450 --> 00:01:01,580
In the absence of that, which happens
for quite a bit of time, we go to plan

17
00:01:01,580 --> 00:01:06,560
B and resort to analytic properties, or
practical properties of optimization,

18
00:01:06,560 --> 00:01:09,570
in order to choose the error measure.

19
00:01:09,570 --> 00:01:14,510
Once you define the error between the
performance of your hypothesis versus

20
00:01:14,510 --> 00:01:19,010
the target function on a particular
point, you can plug this in, into

21
00:01:19,010 --> 00:01:23,120
different error quantities, like the
in-sample error and the out-of-sample

22
00:01:23,120 --> 00:01:26,880
error, and get those values in
terms of the error measure

23
00:01:26,880 --> 00:01:28,670
by getting an average.

24
00:01:28,670 --> 00:01:34,180
In the case of the training set, you
estimate the error on the training

25
00:01:34,180 --> 00:01:38,900
points, and then you average with
respect to the N

26
00:01:38,900 --> 00:01:40,510
examples that you have.

27
00:01:40,510 --> 00:01:44,330
And in the case of out-of-sample,
theoretically the definition would be

28
00:01:44,330 --> 00:01:48,810
that you also evaluate the error between
h and f on a particular point

29
00:01:48,810 --> 00:01:52,740
x, give the weight of x according to its
probability, and get the expected

30
00:01:52,740 --> 00:01:56,720
value with respect to this x.

31
00:01:56,720 --> 00:02:01,170
The notion of noisy targets came from
the fact that what we are trying to

32
00:02:01,170 --> 00:02:04,700
learn may not be a deterministic
function, the only function in

33
00:02:04,700 --> 00:02:09,740
mathematics, where y is uniquely
determined by the value of x. But

34
00:02:09,740 --> 00:02:12,430
rather, when y is affected by x--

35
00:02:12,430 --> 00:02:17,130
y is distributed according to
a probability distribution, which gives

36
00:02:17,130 --> 00:02:18,830
you y given x.

37
00:02:18,830 --> 00:02:23,110
And we talked about, for example, in
the case of credit application, two

38
00:02:23,110 --> 00:02:26,210
identical applications may lead
to different credit behavior.

39
00:02:26,210 --> 00:02:29,780
Therefore, the credit behavior is
a probabilistic thing, not

40
00:02:29,780 --> 00:02:32,510
a deterministic function of
the credit application.

41
00:02:32,510 --> 00:02:36,360
You can go back to our first example,
let's say, of the movie rentals.

42
00:02:36,360 --> 00:02:40,210
If you rate a movie, you may rate the
same movie at different times

43
00:02:40,210 --> 00:02:42,510
differently, depending on your
mood and other factors.

44
00:02:42,510 --> 00:02:45,200
So there's always a noise factor
in these practical problems.

45
00:02:45,200 --> 00:02:49,450
And that is captured by the transitional
probability from x to y, probability

46
00:02:49,450 --> 00:02:52,290
of y given x.

47
00:02:52,290 --> 00:02:55,130
When we look at the diagram involving
this probability--

48
00:02:55,130 --> 00:02:58,400
so now we replace the target function,
which used to be a function, by

49
00:02:58,400 --> 00:03:00,970
a probability distribution, which
can be modeled as a target

50
00:03:00,970 --> 00:03:02,360
function plus noise.

51
00:03:02,360 --> 00:03:06,440
And these feed into the generation
of the training examples.

52
00:03:06,440 --> 00:03:10,680
And when you look at the unknown input
distribution, which we introduced

53
00:03:10,680 --> 00:03:14,230
technically in order to get the benefit
of Hoeffding inequality,

54
00:03:14,230 --> 00:03:16,810
that also feeds into the
training example.

55
00:03:16,810 --> 00:03:18,180
This determines x.

56
00:03:18,180 --> 00:03:20,210
And this determines y given x.

57
00:03:20,210 --> 00:03:23,070
And then you generate these examples
independently, according to this

58
00:03:23,070 --> 00:03:24,510
distribution.

59
00:03:24,510 --> 00:03:28,780
So when we had x being the only
probabilistic thing, and y being

60
00:03:28,780 --> 00:03:33,020
a deterministic function of x, then
x_1 was independent of x_2,

61
00:03:33,020 --> 00:03:33,670
independent of x_N.

62
00:03:33,670 --> 00:03:36,830
And then you compute each y, according
to the function, on the

63
00:03:36,830 --> 00:03:38,460
corresponding x.

64
00:03:38,460 --> 00:03:44,090
When you have the noisy version, then
the pair x_1 and y_1 is generated

65
00:03:44,090 --> 00:03:48,020
according to the joint probability
distribution, which is P of x, the

66
00:03:48,020 --> 00:03:51,170
original one, times P of y given
x, the one you introduced to

67
00:03:51,170 --> 00:03:52,850
accommodate the noise.

68
00:03:52,850 --> 00:03:55,630
And then the independence lies
between different pairs.

69
00:03:55,630 --> 00:04:04,820
So x_1, y_1 would be independent of x_2,
y_2, independent of x_3, y_3 and so on.

70
00:04:04,820 --> 00:04:09,220
And when you get the expected values
for errors, you now have to take into

71
00:04:09,220 --> 00:04:12,310
consideration the probability with
respect to both x and y.

72
00:04:12,310 --> 00:04:16,450
So what used to be the expected value
with respect to x, is now the expected

73
00:04:16,450 --> 00:04:18,329
value with respect to x and y.

74
00:04:18,329 --> 00:04:22,110
And then you plug in x into h, and
correspond it to the probabilistic

75
00:04:22,110 --> 00:04:24,210
value of y that happened to occur.

76
00:04:24,210 --> 00:04:29,640
And that would be now the out-of-sample
error in this case.

77
00:04:29,640 --> 00:04:33,690
Now in this lecture, I'm going to start
the theory track that will last

78
00:04:33,690 --> 00:04:37,440
for this particular route three
lectures, followed by another theory

79
00:04:37,440 --> 00:04:41,170
lecture on a related but
different topic.

80
00:04:41,170 --> 00:04:46,400
And the idea is to relate training to
testing, in-sample and out-of-sample, in

81
00:04:46,400 --> 00:04:48,260
a realistic way.

82
00:04:48,260 --> 00:04:52,080
So the outline will be the following.

83
00:04:52,080 --> 00:04:55,830
We'll spend some time talking about
training versus testing, a very

84
00:04:55,830 --> 00:04:57,160
intuitive notion.

85
00:04:57,160 --> 00:05:00,630
But we'd like to put the mathematical
framework that describes what is

86
00:05:00,630 --> 00:05:02,470
training versus testing.

87
00:05:02,470 --> 00:05:06,670
And then we will introduce quantities
that will be mathematically helpful in

88
00:05:06,670 --> 00:05:09,300
characterizing that relationship.

89
00:05:09,300 --> 00:05:12,320
And after I give you a number of
examples to make sure that the notion

90
00:05:12,320 --> 00:05:15,910
is clear, we are going to introduce
the key notion, the break point.

91
00:05:15,910 --> 00:05:19,520
And the break point is the one that will
later result in the VC dimension,

92
00:05:19,520 --> 00:05:22,230
the main notion in the
theory of learning.

93
00:05:22,230 --> 00:05:23,580
And finally, I end up with a puzzle.

94
00:05:23,580 --> 00:05:26,890
It's an interesting puzzle that will
hopefully fix the ideas that we talked

95
00:05:26,890 --> 00:05:28,980
about in the lecture.

96
00:05:28,980 --> 00:05:29,970


97
00:05:29,970 --> 00:05:32,390
So now let's talk about training
versus testing.

98
00:05:32,390 --> 00:05:37,010
And I'm going to take a very simple
example that you can relate to.

99
00:05:37,010 --> 00:05:40,430
Let's say that I'm giving
you a final exam.

100
00:05:40,430 --> 00:05:42,270
So now I want to help you out.

101
00:05:42,270 --> 00:05:47,850
So before the final exam, I give you
some practice problems and solutions,

102
00:05:47,850 --> 00:05:50,890
so you can work on and prepare
yourself for the final exam.

103
00:05:50,890 --> 00:05:53,390
That is very typical.

104
00:05:53,390 --> 00:05:57,710
Now if you look at the practice problems
and solutions, this would be

105
00:05:57,710 --> 00:06:00,740
your training set, so to speak.

106
00:06:00,740 --> 00:06:01,870
You're going to look at the question.

107
00:06:01,870 --> 00:06:02,700
You're going to answer.

108
00:06:02,700 --> 00:06:04,100
You're going to compare it
with the real answer.

109
00:06:04,100 --> 00:06:07,190
And then you are going to adjust your
hypothesis, your understanding of the

110
00:06:07,190 --> 00:06:10,520
material, in order to do it better, and
go through them and perhaps go through

111
00:06:10,520 --> 00:06:15,230
them again, until you get them right or
mostly right or figure out

112
00:06:15,230 --> 00:06:15,930
the material.

113
00:06:15,930 --> 00:06:20,070
And now you are more ready
for the final exam.

114
00:06:20,070 --> 00:06:23,260
Now the reason I gave you the practice
problems and solutions is to help you

115
00:06:23,260 --> 00:06:27,000
do better on the final, right?

116
00:06:27,000 --> 00:06:32,070
Why don't I just give you the
problems on the final, then?

117
00:06:32,070 --> 00:06:34,930
Excellent idea, I can see!

118
00:06:34,930 --> 00:06:36,690
Now the problem is obvious.

119
00:06:36,690 --> 00:06:40,540
The problem is that doing well
on the final is not the

120
00:06:40,540 --> 00:06:43,420
goal, in and of itself.

121
00:06:43,420 --> 00:06:50,810
The goal is for you to learn the
material, to have a small E_out.

122
00:06:50,810 --> 00:06:55,720
The final exam is only
a way of gauging how well

123
00:06:55,720 --> 00:06:57,930
you actually learned.

124
00:06:57,930 --> 00:07:01,920
And in order for it to gauge how well
you actually learned, I have to give

125
00:07:01,920 --> 00:07:07,060
you the final at the point you have
already fixed your hypothesis.

126
00:07:07,060 --> 00:07:07,880
You prepared.

127
00:07:07,880 --> 00:07:08,590
You studied.

128
00:07:08,590 --> 00:07:09,630
You discussed with people.

129
00:07:09,630 --> 00:07:11,960
You now sit down to take
the final exam.

130
00:07:11,960 --> 00:07:13,380
So you have one hypothesis.

131
00:07:13,380 --> 00:07:16,010
And you go through the exam.

132
00:07:16,010 --> 00:07:20,210
And therefore, your answer on, let's
say, the 50 questions of the final--

133
00:07:20,210 --> 00:07:23,040
hopefully, it's not going to be
that long if there's a final--

134
00:07:23,040 --> 00:07:26,850
will reflect what your understanding
will be outside.

135
00:07:26,850 --> 00:07:29,160
So the distinction is conceptual.

136
00:07:29,160 --> 00:07:32,420
And now, let's put mathematically
what is training versus testing?

137
00:07:32,420 --> 00:07:35,250
It will be an extremely simple
distinction, although it's

138
00:07:35,250 --> 00:07:37,230
an important distinction.

139
00:07:37,230 --> 00:07:37,510


140
00:07:37,510 --> 00:07:44,860
Here is what testing is, in terms
of a mathematical description.

141
00:07:44,860 --> 00:07:46,700
You have seen this before.

142
00:07:46,700 --> 00:07:49,180
This is the plain-vanilla Hoeffding.

143
00:07:49,180 --> 00:07:56,110
This part is how well you did
on the final exam.

144
00:07:56,110 --> 00:07:59,330
This is how well you understand
the material proper.

145
00:07:59,330 --> 00:08:01,360
And since you have only one
hypothesis-- this is

146
00:08:01,360 --> 00:08:02,600
a final, you are fixed,

147
00:08:02,600 --> 00:08:05,140
and you just take the exam.

148
00:08:05,140 --> 00:08:09,440
Your performance on the exam tracks well
how you understand the material.

149
00:08:09,440 --> 00:08:13,100
And therefore, the difference
between them is small.

150
00:08:13,100 --> 00:08:20,020
And the probability that it's not small
is becoming less and less, when

151
00:08:20,020 --> 00:08:22,410
the number of questions,
in this case, goes up.

152
00:08:22,410 --> 00:08:24,940
So that is what testing is.

153
00:08:24,940 --> 00:08:27,140
How about training?

154
00:08:27,140 --> 00:08:32,240
Almost identical, except
for one thing--

155
00:08:32,240 --> 00:08:34,220
this fellow.

156
00:08:34,220 --> 00:08:37,039
Because in the case of training,
this is how you performed on

157
00:08:37,039 --> 00:08:38,720
the practice problems.

158
00:08:38,720 --> 00:08:42,260
In the practice problems, you had
the answers, and you modified your

159
00:08:42,260 --> 00:08:42,960
hypothesis.

160
00:08:42,960 --> 00:08:45,280
And you looked at it, and you
got an answer wrong.

161
00:08:45,280 --> 00:08:46,600
So you modified your hypothesis again.

162
00:08:46,600 --> 00:08:47,580
You are learning better.

163
00:08:47,580 --> 00:08:49,250
That's all very nice.

164
00:08:49,250 --> 00:08:52,310
But now the practice set
is contaminated.

165
00:08:52,310 --> 00:08:54,320
You pretty much almost
memorized what it is.

166
00:08:54,320 --> 00:08:57,440
And there's a price to pay for that, in
terms of how your performance on the

167
00:08:57,440 --> 00:09:01,470
practice, which is E_in in this case,
tracks how well you understand the

168
00:09:01,470 --> 00:09:03,490
material, which is still E_out.

169
00:09:03,490 --> 00:09:06,110
And the price you pay is
how much you explored.

170
00:09:06,110 --> 00:09:09,750
And that was reflected by
the simple M, which was the number

171
00:09:09,750 --> 00:09:14,240
of hypotheses in the very simple
derivation we did.

172
00:09:14,240 --> 00:09:19,340
So if you want an executive summary of
this lecture, we are just going to try

173
00:09:19,340 --> 00:09:25,280
to get M to be replaced by something
more friendly, because you realize that

174
00:09:25,280 --> 00:09:29,390
M-- if you just measure the
complexity of your hypothesis set by

175
00:09:29,390 --> 00:09:33,740
the number of hypotheses-- this is next
to useless in almost all cases.

176
00:09:33,740 --> 00:09:38,200
Something as simple as the perceptron
has M equals infinity.

177
00:09:38,200 --> 00:09:41,960
And therefore, this guarantee
is no guarantee at all.

178
00:09:41,960 --> 00:09:46,840
If we can replace M with another
quantity, and justify that, and

179
00:09:46,840 --> 00:09:51,040
that quantity is not infinite even if
the hypothesis set is infinite, then

180
00:09:51,040 --> 00:09:51,870
we are in business.

181
00:09:51,870 --> 00:09:56,140
And we can start talking about the
feasibility of learning in an actual

182
00:09:56,140 --> 00:10:01,115
model, and be able to establish
the notion in a way that we can apply

183
00:10:01,115 --> 00:10:02,340
to a real situation.

184
00:10:02,340 --> 00:10:02,680


185
00:10:02,680 --> 00:10:04,430
That's the plan.

186
00:10:04,430 --> 00:10:04,920


187
00:10:04,920 --> 00:10:08,260
We're talking about M, so the
first question is to ask, where

188
00:10:08,260 --> 00:10:10,250
did this M come from?

189
00:10:10,250 --> 00:10:13,030
If we are going to replace it, we need
to understand where it came from, to

190
00:10:13,030 --> 00:10:15,650
understand the context
for replacing it.

191
00:10:15,650 --> 00:10:21,180
Well, there are bad events that
we have talked about.

192
00:10:21,180 --> 00:10:26,430
And the bad events are called
B, because they are bad.

193
00:10:26,430 --> 00:10:26,800


194
00:10:26,800 --> 00:10:28,380
That's good!

195
00:10:28,380 --> 00:10:31,080
And then-- these are the bad events.

196
00:10:31,080 --> 00:10:33,830
What is the bad event that
we were trying to avoid?

197
00:10:33,830 --> 00:10:38,500
We were trying to avoid the situation
where your in-sample performance does

198
00:10:38,500 --> 00:10:40,820
not track the out-of-sample
performance.

199
00:10:40,820 --> 00:10:44,280
If their difference is bigger than
epsilon, this is a bad situation.

200
00:10:44,280 --> 00:10:46,320
And we're trying to say that
the probability of a bad

201
00:10:46,320 --> 00:10:47,680
situation is small.

202
00:10:47,680 --> 00:10:49,030
That was the starting point.

203
00:10:49,030 --> 00:10:51,260


204
00:10:51,260 --> 00:10:55,830
Now we applied the union bound,
and we got the probability

205
00:10:55,830 --> 00:10:59,830
of several bad events.

206
00:10:59,830 --> 00:11:02,740
This is the bad event for
the first hypothesis.

207
00:11:02,740 --> 00:11:06,515
You can see here that there
is m, a small m.

208
00:11:06,515 --> 00:11:10,960
m is 1, 2, 3, 4, up to M.
So there are M hypotheses, capital

209
00:11:10,960 --> 00:11:12,670
M hypotheses that I'm talking about.

210
00:11:12,670 --> 00:11:17,570
And I would like the probability of
any of them happening to be small.

211
00:11:17,570 --> 00:11:18,680
Why is that?

212
00:11:18,680 --> 00:11:23,170
Because your learning algorithm is free
to pick whichever hypothesis it

213
00:11:23,170 --> 00:11:25,570
wants, based on the examples.

214
00:11:25,570 --> 00:11:31,780
So if you tell me that the probability
of any of the bad events is small,

215
00:11:31,780 --> 00:11:36,240
then whichever hypothesis your algorithm
picks, they will be OK.

216
00:11:36,240 --> 00:11:38,690
And I want that guarantee to be there.

217
00:11:38,690 --> 00:11:44,700
So let's try to understand the
probability of the B_1 or B_2 or B_M.

218
00:11:44,700 --> 00:11:46,910
What does it look like?

219
00:11:46,910 --> 00:11:52,580
Well, if you look at a Venn diagram,
and you place B_1 and B_2 and B_3 as

220
00:11:52,580 --> 00:11:55,400
areas here, these areas--

221
00:11:55,400 --> 00:11:56,480
these are different events.

222
00:11:56,480 --> 00:12:00,980
They could be disjoint, in which case
the circles will be far apart.

223
00:12:00,980 --> 00:12:04,480
Or they could be coincident, which
will be on top of each other.

224
00:12:04,480 --> 00:12:07,330
They could be independent, which means
that they are proportionately

225
00:12:07,330 --> 00:12:08,070
overlapping.

226
00:12:08,070 --> 00:12:10,300
There could be many situations.

227
00:12:10,300 --> 00:12:14,460
Now the point of the bound is that we
would like to make that statement

228
00:12:14,460 --> 00:12:18,400
regardless of the correlations
between the events.

229
00:12:18,400 --> 00:12:24,010
And therefore, we use the union bound,
which actually bounds it by the total

230
00:12:24,010 --> 00:12:27,470
area of the first one, plus the total
area of the second one, et cetera, as

231
00:12:27,470 --> 00:12:29,390
if they were disjoint.

232
00:12:29,390 --> 00:12:32,920
Well, that will always hold regardless
of the level of overlap.

233
00:12:32,920 --> 00:12:37,700
But you can see that this is a poor
bound because in this case, we are

234
00:12:37,700 --> 00:12:41,730
estimating it to be about three times
the area, when it's actually closer to

235
00:12:41,730 --> 00:12:45,250
just the area, because the overlap
is so significant.

236
00:12:45,250 --> 00:12:49,220
And therefore, we would like to be able
to take into consideration the

237
00:12:49,220 --> 00:12:52,650
overlaps, because with no overlaps,
you just get M terms.

238
00:12:52,650 --> 00:12:56,440
And you're stuck with M, and
infinity, in almost all the interesting

239
00:12:56,440 --> 00:12:58,830
hypothesis sets.

240
00:12:58,830 --> 00:13:01,500
Now of course, you can go--

241
00:13:01,500 --> 00:13:02,940
in principle, you can go

242
00:13:02,940 --> 00:13:05,840
and I give you the hypothesis set,
which is the perceptron.

243
00:13:05,840 --> 00:13:10,330
And you can try to formalize, what
is this bad event in terms of the

244
00:13:10,330 --> 00:13:11,200
perceptron.

245
00:13:11,200 --> 00:13:15,310
And what happens when you go to the
other perceptron, and try to get the

246
00:13:15,310 --> 00:13:19,560
full joint distribution of all of these
guys, and solve this exactly.

247
00:13:19,560 --> 00:13:22,090
Well, you can, in principle--
theoretically.

248
00:13:22,090 --> 00:13:24,910
It's a complete nightmare,
completely undoable.

249
00:13:24,910 --> 00:13:28,800
And if we have to do this for every
hypothesis set you propose, there

250
00:13:28,800 --> 00:13:30,010
wouldn't be learning theory around.

251
00:13:30,010 --> 00:13:30,860
People will just

252
00:13:30,860 --> 00:13:32,110
give up.

253
00:13:32,110 --> 00:13:35,430
So what we are going to do, we are
going to try to abstract from the

254
00:13:35,430 --> 00:13:41,900
hypothesis set a quantity that is
sufficient to characterize the

255
00:13:41,900 --> 00:13:46,510
overlaps, and get us a good bound,
without having to go through the

256
00:13:46,510 --> 00:13:51,020
intricate details of analyzing how the
bad events are correlated.

257
00:13:51,020 --> 00:13:52,670
That would be the goal.

258
00:13:52,670 --> 00:13:56,360
And we will achieve it, through
a very simple argument.

259
00:13:56,360 --> 00:13:57,150


260
00:13:57,150 --> 00:14:00,370
So that's where M comes from.

261
00:14:00,370 --> 00:14:02,760
When we asked, can we improve on M?

262
00:14:02,760 --> 00:14:03,980
Maybe M is the best we can do.

263
00:14:03,980 --> 00:14:05,650
It's not like we
wish to improve it,

264
00:14:05,650 --> 00:14:06,690
so it has to be improved.

265
00:14:06,690 --> 00:14:08,620
Maybe that's the best we can say.

266
00:14:08,620 --> 00:14:12,250
If you have an infinite hypothesis, then
you're stuck, and that's that.

267
00:14:12,250 --> 00:14:16,600
But it turns out that, no, the overlap
situation we talked about is actually

268
00:14:16,600 --> 00:14:17,520
very common.

269
00:14:17,520 --> 00:14:22,050
Yes, we can improve on M. And the reason
is that the bad events are

270
00:14:22,050 --> 00:14:25,610
extremely overlapping in
a practical situation.

271
00:14:25,610 --> 00:14:31,050
Let's take the example we know, which
is the perceptron, to understand

272
00:14:31,050 --> 00:14:32,010
what this is.

273
00:14:32,010 --> 00:14:35,560
I'm going through the example because
now we have lots of binary things--

274
00:14:35,560 --> 00:14:39,420
+1 versus -1 for the target,
+1 versus -1 for the

275
00:14:39,420 --> 00:14:42,870
hypothesis, agreeing versus
disagreeing, et cetera.

276
00:14:42,870 --> 00:14:46,450
I want to pin down exactly what is
the bad event, in terms of this picture,

277
00:14:46,450 --> 00:14:49,050
so that we understand what
we are talking about.

278
00:14:49,050 --> 00:14:49,680


279
00:14:49,680 --> 00:14:55,110
Here is the target function
for a perceptron.

280
00:14:55,110 --> 00:14:59,000
And it returns +1 for some
guys, -1 for some guys.

281
00:14:59,000 --> 00:14:59,620


282
00:14:59,620 --> 00:15:01,060
That's easy.

283
00:15:01,060 --> 00:15:04,090
And then you have a hypothesis,
a perceptron.

284
00:15:04,090 --> 00:15:05,790
And this is not the final hypothesis.

285
00:15:05,790 --> 00:15:08,390
This is a badly performing hypothesis.

286
00:15:08,390 --> 00:15:09,930
But it is a general perceptron.

287
00:15:09,930 --> 00:15:14,110
If you find any vector of weights,
you'll find another blue line.

288
00:15:14,110 --> 00:15:14,910


289
00:15:14,910 --> 00:15:20,010
So now in terms of this picture, could
someone tell me what is E_out?

290
00:15:20,010 --> 00:15:23,930
What is the out-of-sample error for this
hypothesis, when it's applied to

291
00:15:23,930 --> 00:15:26,010
this target function?

292
00:15:26,010 --> 00:15:26,500


293
00:15:26,500 --> 00:15:29,290
It's not that difficult.

294
00:15:29,290 --> 00:15:34,120
It is actually just these areas,
the differential areas.

295
00:15:34,120 --> 00:15:35,880
This is where they disagree.

296
00:15:35,880 --> 00:15:36,980
One is saying +1.

297
00:15:36,980 --> 00:15:38,730
One is saying -1.

298
00:15:38,730 --> 00:15:42,100
So these two areas-- if you get the total
area if it's uniform, the total

299
00:15:42,100 --> 00:15:42,650
probability

300
00:15:42,650 --> 00:15:46,500
if it's not-- then this will give
you the value of E_out.

301
00:15:46,500 --> 00:15:48,740
That's one quantity.

302
00:15:48,740 --> 00:15:49,990
How about E_in?

303
00:15:49,990 --> 00:15:52,600


304
00:15:52,600 --> 00:15:53,980
For E_in, you need a sample.

305
00:15:53,980 --> 00:15:55,410
So first, you generate a sample.

306
00:15:55,410 --> 00:15:57,180
Here's a constellation of points.

307
00:15:57,180 --> 00:16:00,470
Some of these points, as you
see, will fall into the bad

308
00:16:00,470 --> 00:16:02,810
region, here and here.

309
00:16:02,810 --> 00:16:04,200
And I color them red.

310
00:16:04,200 --> 00:16:10,290
So the fraction of red compared to
all the sample gives you E_in.

311
00:16:10,290 --> 00:16:10,800


312
00:16:10,800 --> 00:16:11,620
That is understood.

313
00:16:11,620 --> 00:16:12,880
This is E_in and E_out.

314
00:16:12,880 --> 00:16:15,130
And these are the guys that I
want to track each other.

315
00:16:15,130 --> 00:16:15,490
OK, fine.

316
00:16:15,490 --> 00:16:17,000
I understand this part.

317
00:16:17,000 --> 00:16:18,730
And in words.

318
00:16:18,730 --> 00:16:23,950
Now you'll look at: what is the change
E_in and E_out, when you change your

319
00:16:23,950 --> 00:16:25,310
hypothesis?

320
00:16:25,310 --> 00:16:28,240
So here's your first hypothesis.

321
00:16:28,240 --> 00:16:29,750
Now take another perceptron.

322
00:16:29,750 --> 00:16:33,480


323
00:16:33,480 --> 00:16:36,530
You probably already suspect that
this is hugely overlapping.

324
00:16:36,530 --> 00:16:39,500
Whatever you're talking about, it must
be overlapping, because they're so

325
00:16:39,500 --> 00:16:40,570
close to each other.

326
00:16:40,570 --> 00:16:45,050
But let's pin down the specific event
that, we would like to argue, is

327
00:16:45,050 --> 00:16:46,960
overlapping.

328
00:16:46,960 --> 00:16:53,240
So the change in E_out when you go from,
let's say, the blue hypothesis,

329
00:16:53,240 --> 00:16:56,620
this blue hypothesis, to
the green hypothesis--

330
00:16:56,620 --> 00:17:02,830
the change in E_out would be the area
of this yellow thing, not very much.

331
00:17:02,830 --> 00:17:05,319
A very thin area.

332
00:17:05,319 --> 00:17:08,569
That's where E_out changed, right?

333
00:17:08,569 --> 00:17:09,329


334
00:17:09,329 --> 00:17:16,175
So if you look at the area, that gives you
delta E_out. If you look at the delta E_in,

335
00:17:16,175 --> 00:17:18,150
the change of the labels
of data points--

336
00:17:18,150 --> 00:17:23,910
if one of the data points happens
to fall in this yellow region, then

337
00:17:23,910 --> 00:17:27,858
its error status will change from one
hypothesis to another, because one

338
00:17:27,858 --> 00:17:30,680
hypothesis got it right, and
the other one got it wrong.

339
00:17:30,680 --> 00:17:33,820
Now the chances of a point
falling here is small.

340
00:17:33,820 --> 00:17:37,570
So you can see why we are arguing that
the change delta E_out and the change

341
00:17:37,570 --> 00:17:38,810
delta E_in is small.

342
00:17:38,810 --> 00:17:42,950
The area is small, and the probability
of a point falling there is small.

343
00:17:42,950 --> 00:17:47,760
Moreover, they are actually moving in
the same direction because the change

344
00:17:47,760 --> 00:17:51,460
is actually depending on the
area of the yellow part.

345
00:17:51,460 --> 00:17:52,730
So this--

346
00:17:52,730 --> 00:17:54,310
let's say that this is increasing.

347
00:17:54,310 --> 00:17:58,240
If they increase, they increase both,
because I get a net positive area for

348
00:17:58,240 --> 00:17:58,950
the delta E_out.

349
00:17:58,950 --> 00:18:01,800
And the probability of falling
there also increases.

350
00:18:01,800 --> 00:18:02,670


351
00:18:02,670 --> 00:18:06,680
Now, the reason I'm saying that, is
because what we care about are these.

352
00:18:06,680 --> 00:18:12,070
We would like to make the statement
that, how E_in tracks E_out for the

353
00:18:12,070 --> 00:18:17,590
first hypothesis, for the blue
perceptron, is comparable to how E_in

354
00:18:17,590 --> 00:18:19,500
tracks E_out for the second one.

355
00:18:19,500 --> 00:18:20,850
Why are we interested in that?

356
00:18:20,850 --> 00:18:26,620
Because we would like to argue that this
exceeding epsilon happens often,

357
00:18:26,620 --> 00:18:28,240
when this exceeds epsilon.

358
00:18:28,240 --> 00:18:30,040
The events are overlapping.

359
00:18:30,040 --> 00:18:32,440
We are not looking for the absolute
value of those, we are just saying

360
00:18:32,440 --> 00:18:34,860
that, if this exceeds epsilon,
this also exceeds

361
00:18:34,860 --> 00:18:36,450
epsilon most of the time.

362
00:18:36,450 --> 00:18:39,460
And therefore, the picture we had
last time is actually true.

363
00:18:39,460 --> 00:18:40,680
These guys are overlapping.

364
00:18:40,680 --> 00:18:42,080
The bad events are overlapping.

365
00:18:42,080 --> 00:18:46,440
And at least we stand a hope that we
will get something better than just

366
00:18:46,440 --> 00:18:51,110
counting the number of hypotheses, for
the complexity we are seeking.

367
00:18:51,110 --> 00:18:54,310
So we can improve M. That's good news.

368
00:18:54,310 --> 00:18:57,080
We can improve M. We're going
to replace it with something.

369
00:18:57,080 --> 00:18:59,380
What are we going to replace it with?

370
00:18:59,380 --> 00:19:03,150
I'm going to introduce to you now the
notion that will replace

371
00:19:03,150 --> 00:19:09,420
M. It is not going to be completely
obvious that we can actually replace M

372
00:19:09,420 --> 00:19:10,850
with this quantity.

373
00:19:10,850 --> 00:19:11,400


374
00:19:11,400 --> 00:19:12,600
That will require a proof.

375
00:19:12,600 --> 00:19:14,620
And that will take us
into next lecture.

376
00:19:14,620 --> 00:19:18,860
The purpose here is to define the
quantity, and make you understand it

377
00:19:18,860 --> 00:19:22,210
well, because this is the quantity that
will end up characterizing the

378
00:19:22,210 --> 00:19:24,830
complexity of any model you use.

379
00:19:24,830 --> 00:19:26,660
So we want to understand it well.

380
00:19:26,660 --> 00:19:29,950
And we are going to motivate that it can
replace M. It will be plausible.

381
00:19:29,950 --> 00:19:30,630
It makes sense.

382
00:19:30,630 --> 00:19:31,930
It's not a crazy quantity.

383
00:19:31,930 --> 00:19:35,290
It also counts the number
of hypotheses, of sorts.

384
00:19:35,290 --> 00:19:39,370
And therefore, let's define the quantity
and become familiar with it.

385
00:19:39,370 --> 00:19:43,410
And then next time, we will like the
quantity so much that we'll bite the

386
00:19:43,410 --> 00:19:47,260
bullet, and go through the proof that we
can actually replace M with

387
00:19:47,260 --> 00:19:48,920
this quantity.

388
00:19:48,920 --> 00:19:49,110


389
00:19:49,110 --> 00:19:51,430
So what is the quantity?

390
00:19:51,430 --> 00:19:53,040
The quantity is based
on the following.

391
00:19:53,040 --> 00:19:56,990
When we count the number of hypotheses,
we obviously take into

392
00:19:56,990 --> 00:19:59,680
consideration the entire input space.

393
00:19:59,680 --> 00:20:01,610
What does that mean?

394
00:20:01,610 --> 00:20:04,840
These are four different perceptrons.

395
00:20:04,840 --> 00:20:06,270
So I take the input space.

396
00:20:06,270 --> 00:20:09,330
And the reason these guys are
different is because they are different

397
00:20:09,330 --> 00:20:12,400
on at least one point
in the input space.

398
00:20:12,400 --> 00:20:14,710
That's what makes two
functions different.

399
00:20:14,710 --> 00:20:18,970
And because the input space is infinite,
continuous, that's why we

400
00:20:18,970 --> 00:20:21,790
get an infinite number of hypotheses.

401
00:20:21,790 --> 00:20:27,900
So let's say that, instead of counting
the number of hypotheses on the entire

402
00:20:27,900 --> 00:20:35,660
input space, I'm going to restrict
my attention only to the sample.

403
00:20:35,660 --> 00:20:36,800


404
00:20:36,800 --> 00:20:42,760
So I generate only the input points,
which are finite points, put them on

405
00:20:42,760 --> 00:20:43,760
the diagram.

406
00:20:43,760 --> 00:20:46,510
So I have this constellation
of points.

407
00:20:46,510 --> 00:20:50,840
And when I look at these points alone,
regardless of the entire input space,

408
00:20:50,840 --> 00:20:52,760
those perceptrons will classify them.

409
00:20:52,760 --> 00:20:59,190
These guys will turn into red and blue,
according to the regions they fall in.

410
00:20:59,190 --> 00:21:02,880
Now, in order to fully understand what
it means to count only on the number

411
00:21:02,880 --> 00:21:05,270
of points, we have to wipe
out the input space.

412
00:21:05,270 --> 00:21:06,730
So that's what I'm going to do.

413
00:21:06,730 --> 00:21:08,930
That's what you have.

414
00:21:08,930 --> 00:21:10,940
So you can imagine the perceptron
is somewhere.

415
00:21:10,940 --> 00:21:12,570
And it's splitting the points.

416
00:21:12,570 --> 00:21:14,160
And now what I'm counting is--

417
00:21:14,160 --> 00:21:17,590
for this constellation, which is
a fixed constellation of points, how

418
00:21:17,590 --> 00:21:22,200
many patterns of red
and blue can I get?

419
00:21:22,200 --> 00:21:22,960


420
00:21:22,960 --> 00:21:27,630
Now when you do this, you're not
counting the hypotheses proper,

421
00:21:27,630 --> 00:21:29,470
because the hypothesis are
defined on the input space.

422
00:21:29,470 --> 00:21:32,160
You are counting them
on a restricted set.

423
00:21:32,160 --> 00:21:33,810
But still, you're counting.

424
00:21:33,810 --> 00:21:35,550
You're counting the number
of hypotheses.

425
00:21:35,550 --> 00:21:38,620
For example, if I give you a hypothesis
set where you get all

426
00:21:38,620 --> 00:21:42,780
possible combinations of red and blue,
that's a powerful hypothesis.

427
00:21:42,780 --> 00:21:46,400
If I give you a hypothesis where you get
only few, that's not so powerful

428
00:21:46,400 --> 00:21:47,490
a hypothesis.

429
00:21:47,490 --> 00:21:51,320
So the count here also corresponds in
our mind to the strength, or the power,

430
00:21:51,320 --> 00:21:57,840
of the hypothesis set, which in our mind
is what we try to capture by the crude M.

431
00:21:57,840 --> 00:22:01,500
So we are going to
count the number of hypotheses.

432
00:22:01,500 --> 00:22:02,820
I'm putting them between quotations.

433
00:22:02,820 --> 00:22:03,590
Why?

434
00:22:03,590 --> 00:22:07,010
Because now the hypotheses are defined
only on a subset of the points.

435
00:22:07,010 --> 00:22:09,630
So I'm going to give them a different
name, when I define them only on

436
00:22:09,630 --> 00:22:13,220
a subset of the points, in order not to
confuse the hypotheses, on the general

437
00:22:13,220 --> 00:22:15,060
input space, with this case.

438
00:22:15,060 --> 00:22:18,810
I'm going to call them dichotomies.

439
00:22:18,810 --> 00:22:21,040
And the idea is that
I give you N points.

440
00:22:21,040 --> 00:22:25,020
And there is a dichotomy between what
goes into red, and what goes into blue.

441
00:22:25,020 --> 00:22:27,310
That's where the name came from.

442
00:22:27,310 --> 00:22:31,680
So when you look only at the points, and
you look at this, which ones are

443
00:22:31,680 --> 00:22:36,170
blue and which ones are
red, are a dichotomy.

444
00:22:36,170 --> 00:22:41,940
And if you want to understand
it, let's look at this.

445
00:22:41,940 --> 00:22:46,600
Let's say that you're looking
at the full input space.

446
00:22:46,600 --> 00:22:48,480
And this is your perceptron.

447
00:22:48,480 --> 00:22:50,680
And this is the function
it's implementing.

448
00:22:50,680 --> 00:22:53,520
And then you put a constellation
of points.

449
00:22:53,520 --> 00:22:58,060
The way to understand dichotomies is to
think that I have an opaque sheet

450
00:22:58,060 --> 00:23:03,020
of paper, that has holes in it.

451
00:23:03,020 --> 00:23:07,710
And you put that opaque sheet of paper
on top of your input space.

452
00:23:07,710 --> 00:23:09,240
So you don't see the input space.

453
00:23:09,240 --> 00:23:13,170
You only see it through the
eyes of those points.

454
00:23:13,170 --> 00:23:14,930
So what do you see when you put this?

455
00:23:14,930 --> 00:23:16,410
You end up with this here.

456
00:23:16,410 --> 00:23:17,540
You don't see anything.

457
00:23:17,540 --> 00:23:19,270
You don't see where the hypothesis is.

458
00:23:19,270 --> 00:23:22,460
You just see that these guys
turned blue, and these guys

459
00:23:22,460 --> 00:23:25,320
turned red or pink.

460
00:23:25,320 --> 00:23:30,880
Now as you vary the perceptron, as you
vary the line here, you are not going

461
00:23:30,880 --> 00:23:35,720
to notice it here, until the line
crosses one of the points.

462
00:23:35,720 --> 00:23:39,590
So I could be running around here, here,
here, and here, and generating

463
00:23:39,590 --> 00:23:43,420
an infinite number of hypotheses, for
which I'm charging a huge M.

464
00:23:43,420 --> 00:23:45,340
And this guy is sitting here, looking.

465
00:23:45,340 --> 00:23:45,950
Nothing happened.

466
00:23:45,950 --> 00:23:46,720
It's the same thing.

467
00:23:46,720 --> 00:23:49,220
I'm counting it as 1.

468
00:23:49,220 --> 00:23:52,140
And then when you cross, you end
up with another pattern.

469
00:23:52,140 --> 00:23:54,080
So all of a sudden, these
guys are blue.

470
00:23:54,080 --> 00:23:54,870
And these guys are red.

471
00:23:54,870 --> 00:23:59,840
That's when, let's say, this guy
is horizontal here rather

472
00:23:59,840 --> 00:24:01,660
than vertical here.

473
00:24:01,660 --> 00:24:04,680
So you can always think that we reduced
the situation to where we're

474
00:24:04,680 --> 00:24:08,860
going to look at the problem exactly
as it is, except through this sheet

475
00:24:08,860 --> 00:24:11,590
that has only N holes.

476
00:24:11,590 --> 00:24:12,840


477
00:24:12,840 --> 00:24:15,710


478
00:24:15,710 --> 00:24:16,035


479
00:24:16,035 --> 00:24:21,860
Let's put, in mathematical terms, the
dichotomies which are the mini

480
00:24:21,860 --> 00:24:27,290
hypotheses, the hypotheses restricted
to the data points.

481
00:24:27,290 --> 00:24:29,990
A hypothesis formally is a function.

482
00:24:29,990 --> 00:24:35,940
And the function takes the full
input space X, and produces

483
00:24:35,940 --> 00:24:36,890
-1, +1.

484
00:24:36,890 --> 00:24:38,770
That's the blue and red
region that we saw.

485
00:24:38,770 --> 00:24:41,370


486
00:24:41,370 --> 00:24:44,220
A dichotomy, on the other hand,
is also a hypothesis.

487
00:24:44,220 --> 00:24:46,820
We can even give it the same name,
because it's returning the same values

488
00:24:46,820 --> 00:24:51,390
for the points it's allowed
to return values on.

489
00:24:51,390 --> 00:24:55,380
But the domain of it is not
the full input space, but very

490
00:24:55,380 --> 00:24:57,710
specifically, x_1 up to x_N.

491
00:24:57,710 --> 00:24:58,750
These are--

492
00:24:58,750 --> 00:25:03,180
each one of these points belongs to
X, to the input space.

493
00:25:03,180 --> 00:25:05,530
But now I'm restricting
my function here.

494
00:25:05,530 --> 00:25:09,660
And again, the result is -1,
+1, exactly as it was here.

495
00:25:09,660 --> 00:25:11,710
That's what a dichotomy is.

496
00:25:11,710 --> 00:25:13,350


497
00:25:13,350 --> 00:25:16,650
Now if I ask you how many hypotheses
there are, let's say for the

498
00:25:16,650 --> 00:25:20,030
perceptron case? Very easy.

499
00:25:20,030 --> 00:25:21,460
It can be infinite.

500
00:25:21,460 --> 00:25:23,395
In the case of the perceptron,
it's infinite.

501
00:25:23,395 --> 00:25:23,760
Why?

502
00:25:23,760 --> 00:25:27,360
Because this guy is seriously
infinite.

503
00:25:27,360 --> 00:25:32,150
So the number of functions is
just infinite, by a margin!

504
00:25:32,150 --> 00:25:33,510
So that's fine.

505
00:25:33,510 --> 00:25:37,630
Now if you ask yourself, what is
the number of dichotomies?

506
00:25:37,630 --> 00:25:42,500
Let's look at the notation first,
and then answer the question.

507
00:25:42,500 --> 00:25:45,130
The dichotomy is a function
h applied to one of those.

508
00:25:45,130 --> 00:25:50,090
So when I talk about it, the value, I
would say h of x_1 or h of x_2, one

509
00:25:50,090 --> 00:25:51,870
value at a time.

510
00:25:51,870 --> 00:25:55,620
If I decide to use the fancy notation,
I say I'm going to apply small h to

511
00:25:55,620 --> 00:25:58,920
the entire vector, x_1, x_2, up to x_N.

512
00:25:58,920 --> 00:26:02,870
I would be meaning that you tell me the
values of h of x on each of them.

513
00:26:02,870 --> 00:26:05,800
So you return a vector of
the values, h of x_1, h of

514
00:26:05,800 --> 00:26:07,800
x_2, up to h of x_N.

515
00:26:07,800 --> 00:26:11,030
That's not an unusual notation.

516
00:26:11,030 --> 00:26:15,855
Now if you apply the entire set of
hypotheses H to that, what you

517
00:26:15,855 --> 00:26:19,810
are doing is that you are applying
each member here, which is h, to

518
00:26:19,810 --> 00:26:20,700
the entire vector.

519
00:26:20,700 --> 00:26:24,740
Each time you apply one of those
guys, you get -1, -1, +1,

520
00:26:24,740 --> 00:26:27,410
+1, -1, +1, -1, et cetera.

521
00:26:27,410 --> 00:26:29,890
So you get a full dichotomy.

522
00:26:29,890 --> 00:26:33,780
And then you apply another h, and you
get another dichotomy, and so on.

523
00:26:33,780 --> 00:26:38,690
However, as you vary h, which has
an infinite number of guys, many of these

524
00:26:38,690 --> 00:26:42,410
guys will return exactly the same
dichotomy, because the dichotomies

525
00:26:42,410 --> 00:26:42,980
are very restricted.

526
00:26:42,980 --> 00:26:44,830
I have these N points only.

527
00:26:44,830 --> 00:26:47,720
And I'm returning +1 or -1
on them only.

528
00:26:47,720 --> 00:26:51,360
So how many different ones
can I possibly get?

529
00:26:51,360 --> 00:26:58,940
At most, 2 to the N. If H is
extremely expressive, it will get

530
00:26:58,940 --> 00:27:03,860
you all 2 to the N. If not, it will get
you smaller than 2 to the N. So I

531
00:27:03,860 --> 00:27:06,310
can start with the most infinite
type of hypothesis.

532
00:27:06,310 --> 00:27:10,100
And if I translate it into dichotomies,
I have an upper bound of

533
00:27:10,100 --> 00:27:12,810
2 to the N for the number
of dichotomies I have.

534
00:27:12,810 --> 00:27:14,060


535
00:27:14,060 --> 00:27:16,220


536
00:27:16,220 --> 00:27:20,890
So this thing now becomes a candidate
for replacing the number of

537
00:27:20,890 --> 00:27:21,190
hypotheses.

538
00:27:21,190 --> 00:27:23,510
Instead of the number of hypotheses,
we're talking about the number of

539
00:27:23,510 --> 00:27:25,900
dichotomies.

540
00:27:25,900 --> 00:27:28,690
Now we define the actual quantity.

541
00:27:28,690 --> 00:27:30,640
Capital M is red.

542
00:27:30,640 --> 00:27:32,350
And I keep it red throughout.

543
00:27:32,350 --> 00:27:36,710
And we are going now to define small
m, which I will also keep as red.

544
00:27:36,710 --> 00:27:40,930
That will hopefully, and provably
as we will see next time,

545
00:27:40,930 --> 00:27:42,780
replace M.

546
00:27:42,780 --> 00:27:44,320
It's called the growth function.

547
00:27:44,320 --> 00:27:47,190
What is the idea of
the growth function?

548
00:27:47,190 --> 00:27:51,940
The growth function counts
the most dichotomies you

549
00:27:51,940 --> 00:27:58,930
can get, using your hypothesis
set on any N points.

550
00:27:58,930 --> 00:28:01,650
So here is the game.

551
00:28:01,650 --> 00:28:06,220
I give you a budget N.
That's my decision.

552
00:28:06,220 --> 00:28:11,120
You choose where to place
the points, x_1 up to x_N.

553
00:28:11,120 --> 00:28:15,740
Your choice is based on your attempt
to find as many dichotomies as

554
00:28:15,740 --> 00:28:20,590
possible, on the N points, using
the hypothesis set.

555
00:28:20,590 --> 00:28:23,570
So it would silly, for example, to take
the points and put them, let's

556
00:28:23,570 --> 00:28:27,140
say, on a line, because now you are
restricted in separating them.

557
00:28:27,140 --> 00:28:31,360
But you can see the most I can
get if I put them in this general

558
00:28:31,360 --> 00:28:32,340
constellation.

559
00:28:32,340 --> 00:28:35,290
And then you count the number of
dichotomies you are going to get.

560
00:28:35,290 --> 00:28:39,430
And what you're going to report to me is
the value of the growth function on

561
00:28:39,430 --> 00:28:41,630
the N that I passed on to you.

562
00:28:41,630 --> 00:28:45,760
So I give you N, you go through this
exercise, and you return a number that

563
00:28:45,760 --> 00:28:47,340
is the growth function.

564
00:28:47,340 --> 00:28:49,040


565
00:28:49,040 --> 00:28:51,560
Let's put it formally.

566
00:28:51,560 --> 00:28:55,750
The growth function is going to be
called m, in red as I promised.

567
00:28:55,750 --> 00:28:58,510
And it is the maximum.

568
00:28:58,510 --> 00:29:00,145
Maximum with respect to what?

569
00:29:00,145 --> 00:29:02,680


570
00:29:02,680 --> 00:29:08,570
With respect to any choice of
N points from the input space.

571
00:29:08,570 --> 00:29:09,460
That is your part.

572
00:29:09,460 --> 00:29:13,190
I gave you the N. So I
told you what N is.

573
00:29:13,190 --> 00:29:17,980
And then you chose x_1 up to x_N with
a view to maximizing something.

574
00:29:17,980 --> 00:29:19,230
What are you maximizing?

575
00:29:19,230 --> 00:29:21,850


576
00:29:21,850 --> 00:29:25,360
Well, we had this funny notation.

577
00:29:25,360 --> 00:29:30,100
H applied to this entire
vector is actually the set of

578
00:29:30,100 --> 00:29:33,910
dichotomies, the vectors, -1, +1,
-1, +1, and then the

579
00:29:33,910 --> 00:29:39,000
next guy and the next guy--
the actual vectors here.

580
00:29:39,000 --> 00:29:41,880
When you put this cardinality on top
of them, you're just counting them.

581
00:29:41,880 --> 00:29:45,290
You're asking yourself: how
many dichotomies do I get?

582
00:29:45,290 --> 00:29:49,880
So you're maximizing, with respect to the
choice of x_1 up to x_N, this thing.

583
00:29:49,880 --> 00:29:55,560
That will give you the most expressive
facet of the hypothesis set on N

584
00:29:55,560 --> 00:29:56,710
points, that number.

585
00:29:56,710 --> 00:29:57,690
I tell you 10.

586
00:29:57,690 --> 00:29:59,500
And you come back with the number 500.

587
00:29:59,500 --> 00:30:03,730
It means that by your choice of the x_1 up
to x_10, you managed to generate 500

588
00:30:03,730 --> 00:30:08,850
different guys, according to the
hypothesis set that I gave you.

589
00:30:08,850 --> 00:30:12,490
Now because of this, you can see now
that there is an added notation here.

590
00:30:12,490 --> 00:30:16,470
It used to be m, but it actually
depends on the hypothesis set, right?

591
00:30:16,470 --> 00:30:18,790
It's the growth function for
your hypothesis set.

592
00:30:18,790 --> 00:30:24,990
So I'm making that dependency explicit,
by putting a subscript H.

593
00:30:24,990 --> 00:30:27,580
Furthermore, this is
a full-fledged function.

594
00:30:27,580 --> 00:30:28,720
M was a number.

595
00:30:28,720 --> 00:30:29,910
I give you a hypothesis set.

596
00:30:29,910 --> 00:30:30,480
It's an number.

597
00:30:30,480 --> 00:30:33,490
Well, it happens to be infinite,
but it's a number.

598
00:30:33,490 --> 00:30:36,360
Here, I'm giving you
a full function.

599
00:30:36,360 --> 00:30:39,620
That is, I tell you N, you tell me
what the growth function is.

600
00:30:39,620 --> 00:30:41,220
So it's a little bit more complicated.

601
00:30:41,220 --> 00:30:46,420
And because it is this way, m_H is
actually a function of N. That's the

602
00:30:46,420 --> 00:30:48,570
growth function.

603
00:30:48,570 --> 00:30:51,330
So that is the notion.

604
00:30:51,330 --> 00:30:52,160


605
00:30:52,160 --> 00:30:54,700
Now what can we say about
the growth function?

606
00:30:54,700 --> 00:30:58,790
Well, if the number of dichotomies is
at most 2 to the N, because that's as

607
00:30:58,790 --> 00:31:04,270
many +1, -1, N-tuples you can
produce, then the maximum of them is

608
00:31:04,270 --> 00:31:10,070
also bounded by the same thing, at most
2 to the N. Well, if we are going

609
00:31:10,070 --> 00:31:15,050
to replace M with m, I would say
2 to the N is an improvement

610
00:31:15,050 --> 00:31:16,660
over infinity.

611
00:31:16,660 --> 00:31:18,030
If we can afford to do it.

612
00:31:18,030 --> 00:31:20,450
Maybe it's not a great improvement,
nonetheless improvement.

613
00:31:20,450 --> 00:31:23,440


614
00:31:23,440 --> 00:31:27,370
Now, let's apply the definition to
the case of perceptrons, in order to

615
00:31:27,370 --> 00:31:30,100
give it flesh, so we understand
what the notion is.

616
00:31:30,100 --> 00:31:32,470
It's not just an abstract quantity.

617
00:31:32,470 --> 00:31:34,630


618
00:31:34,630 --> 00:31:37,620
We take the perceptrons, and we would
like to get the growth function

619
00:31:37,620 --> 00:31:38,360
of the perceptrons.

620
00:31:38,360 --> 00:31:41,040
Well, getting the growth function of
the perceptron is quite a task.

621
00:31:41,040 --> 00:31:44,110
If I tell you what is M
for the perceptron? Infinity.

622
00:31:44,110 --> 00:31:45,550
And then you go home.

623
00:31:45,550 --> 00:31:47,200
What is the growth function
of the perceptron?

624
00:31:47,200 --> 00:31:50,640
You have to tell me what is the growth
function at N equals 1, what is at N

625
00:31:50,640 --> 00:31:52,980
equals 2, at N equals
3, at N equals 4.

626
00:31:52,980 --> 00:31:54,720
It's a whole function.

627
00:31:54,720 --> 00:31:57,360
So we say, 1 and 2 is easy.

628
00:31:57,360 --> 00:32:02,430
Let's start with N equals 3.

629
00:32:02,430 --> 00:32:04,190
So I'm choosing 3 points.

630
00:32:04,190 --> 00:32:09,010
And I chose them wisely, so that I can
maximize the number of dichotomies.

631
00:32:09,010 --> 00:32:13,050
And now I'm asking myself, what is the
value of the growth function for the

632
00:32:13,050 --> 00:32:15,920
perceptron for the value
N equals 3?

633
00:32:15,920 --> 00:32:18,600
Well, it's not that difficult.

634
00:32:18,600 --> 00:32:22,890
You can see, I can actually get
everything there is to get.

635
00:32:22,890 --> 00:32:23,170
Why?

636
00:32:23,170 --> 00:32:29,330
Because I can have my line here, or I
can have my line here, or I can have

637
00:32:29,330 --> 00:32:31,120
my line here.

638
00:32:31,120 --> 00:32:35,820
That's 3 possibilities times 2 because
I can make it +1 versus two -1's,

639
00:32:35,820 --> 00:32:37,590
or -1 versus two +1's.

640
00:32:37,590 --> 00:32:37,830


641
00:32:37,830 --> 00:32:39,850
We are counting 6 so far.

642
00:32:39,850 --> 00:32:42,560
And then I can have my hypothesis
sitting here.

643
00:32:42,560 --> 00:32:44,270
That will make them all +1.

644
00:32:44,270 --> 00:32:48,176
Or I can have it sitting here, which
makes them all -1.

645
00:32:48,176 --> 00:32:48,590
That's 8.

646
00:32:48,590 --> 00:32:49,910
That's all of them.

647
00:32:49,910 --> 00:32:54,530
The perceptron hypothesis is as strong
as you can get, if you only restrict

648
00:32:54,530 --> 00:32:56,750
your attention to 3 points.

649
00:32:56,750 --> 00:32:58,860
So the answer would be what?

650
00:32:58,860 --> 00:33:00,460
Is it already 8?

651
00:33:00,460 --> 00:33:01,690
Wait a minute.

652
00:33:01,690 --> 00:33:06,870
Someone else chose the points co-linear,
and then found out that if

653
00:33:06,870 --> 00:33:11,780
you want these guys to go to the -1
class, and this guy to go to the +1

654
00:33:11,780 --> 00:33:16,000
class, there is no perceptron
that is capable of doing this.

655
00:33:16,000 --> 00:33:17,000
Correct?

656
00:33:17,000 --> 00:33:20,320
You cannot pass a line that will make
these two guys go to +1, and this

657
00:33:20,320 --> 00:33:21,350
guy go to -1,

658
00:33:21,350 --> 00:33:23,450
if these are co-linear.

659
00:33:23,450 --> 00:33:25,750
Does this bother us?

660
00:33:25,750 --> 00:33:26,780
No.

661
00:33:26,780 --> 00:33:29,630
Because we are taking the maximum.

662
00:33:29,630 --> 00:33:32,670
So this, the quantity you computed
here, since you got to the 8-- you

663
00:33:32,670 --> 00:33:33,810
cannot go above 8.

664
00:33:33,810 --> 00:33:34,670
That defines it.

665
00:33:34,670 --> 00:33:37,830
And indeed, you can with authority
answer the question that the growth

666
00:33:37,830 --> 00:33:43,290
function for this case,
m at N equals 3, is 8.

667
00:33:43,290 --> 00:33:48,010
Now let's see if we are still in
luck when we go to N equals 4.

668
00:33:48,010 --> 00:33:50,820
What is the growth function
for 4 points?

669
00:33:50,820 --> 00:33:53,230
We'll choose the point in
general position again.

670
00:33:53,230 --> 00:33:55,590
We are not going to have any
co-linearity, in order to

671
00:33:55,590 --> 00:33:57,110
maximize our chances.

672
00:33:57,110 --> 00:34:00,160
But then we are stuck with
the following problem.

673
00:34:00,160 --> 00:34:04,170
Even if you choose the points in
general position, there is this

674
00:34:04,170 --> 00:34:05,580
constellation--

675
00:34:05,580 --> 00:34:08,810
there is this particular pattern on the
constellation, which is -1,

676
00:34:08,810 --> 00:34:11,060
-1, and +1, +1.

677
00:34:11,060 --> 00:34:13,780
Can you generate this
using a perceptron?

678
00:34:13,780 --> 00:34:17,790
No. And the opposite of it,
you cannot either.

679
00:34:17,790 --> 00:34:22,090
If this was -1, -1, and
this one, +1, +1.

680
00:34:22,090 --> 00:34:26,710
Can you find any other 4 points, where
you can generate everything?

681
00:34:26,710 --> 00:34:27,260
No.

682
00:34:27,260 --> 00:34:30,820
I can play around, and there is always
2 missing guys, or even worse.

683
00:34:30,820 --> 00:34:34,350
If I choose the points unwisely,
I will be missing more of them.

684
00:34:34,350 --> 00:34:38,429
So the maximum you are getting is that
you are missing 2 out of all the

685
00:34:38,429 --> 00:34:39,570
possibilities.

686
00:34:39,570 --> 00:34:44,000
And the growth function here is 14, not
16, as it might have been if you

687
00:34:44,000 --> 00:34:45,659
had the maximum.

688
00:34:45,659 --> 00:34:49,710
Now this is a very satisfactory
result, because perceptrons are

689
00:34:49,710 --> 00:34:51,389
pretty limited models.

690
00:34:51,389 --> 00:34:54,230
We use them because they are
simple, and there's a nice algorithm

691
00:34:54,230 --> 00:34:55,770
that goes with them.

692
00:34:55,770 --> 00:35:00,550
So we have to expect that the quantity
we are measuring the sophistication of

693
00:35:00,550 --> 00:35:04,150
the perceptrons with, which is the
growth function, had better not be the

694
00:35:04,150 --> 00:35:05,040
maximum possible.

695
00:35:05,040 --> 00:35:07,610
Because if it's the maximum possible,
then we are declaring: perceptrons are

696
00:35:07,610 --> 00:35:08,960
as strong as can be.

697
00:35:08,960 --> 00:35:10,720
Now they break.

698
00:35:10,720 --> 00:35:12,540
And they are limited.

699
00:35:12,540 --> 00:35:15,560
And if I pick another model, which,
let's say-- just for the extreme

700
00:35:15,560 --> 00:35:17,520
case-- the set of all hypotheses.

701
00:35:17,520 --> 00:35:20,660
What would be the growth function
for the set of all hypotheses?

702
00:35:20,660 --> 00:35:23,490
It would be 2 to the N, because
I can generate anything.

703
00:35:23,490 --> 00:35:27,170
So now, according to this measure that
I just introduced, the set of all

704
00:35:27,170 --> 00:35:30,020
hypotheses is stronger
than the perceptrons.

705
00:35:30,020 --> 00:35:32,360
Satisfactory result, simple
but satisfactory.

706
00:35:32,360 --> 00:35:35,660


707
00:35:35,660 --> 00:35:37,330
Now what I'm going to do--

708
00:35:37,330 --> 00:35:40,980
I'm going to take some examples, in
which we can compute the growth

709
00:35:40,980 --> 00:35:45,450
function completely for all values of
N. You can see that if I continued

710
00:35:45,450 --> 00:35:47,240
with this and say, let's
go with the perceptron.

711
00:35:47,240 --> 00:35:48,260
5 points.

712
00:35:48,260 --> 00:35:49,990
You put the 5 points,
and then you try.

713
00:35:49,990 --> 00:35:51,360
Am I missing this?

714
00:35:51,360 --> 00:35:53,550
Or maybe if I change the position
of the points.

715
00:35:53,550 --> 00:35:55,390
It's just a nightmare, just to get 5.

716
00:35:55,390 --> 00:35:59,580
And basically, if you just do it by
brute force, it's not going to happen.

717
00:35:59,580 --> 00:36:02,690
So I'm taking examples where we can
actually, by a simple counting

718
00:36:02,690 --> 00:36:07,030
argument, get the value of the growth
function for the entire domain,

719
00:36:07,030 --> 00:36:11,230
N from 1 up to infinity, in
order to get a better feel for the

720
00:36:11,230 --> 00:36:11,860
growth function.

721
00:36:11,860 --> 00:36:13,150
That's the purpose of this portion.

722
00:36:13,150 --> 00:36:17,020


723
00:36:17,020 --> 00:36:19,700
Our first model, I'm going
to call positive rays.

724
00:36:19,700 --> 00:36:24,270
Let's look at what positive
rays look like.

725
00:36:24,270 --> 00:36:25,840
They are defined on the real line.

726
00:36:25,840 --> 00:36:30,010
So the input space is
R, the real numbers.

727
00:36:30,010 --> 00:36:31,800
And they are very simple.

728
00:36:31,800 --> 00:36:36,790
From a point on, which we are going to
call 'a'-- this is the parameter that

729
00:36:36,790 --> 00:36:39,520
decides one hypothesis versus
the other in this

730
00:36:39,520 --> 00:36:41,500
particular hypothesis set.

731
00:36:41,500 --> 00:36:44,170
All the points that are
bigger go to +1.

732
00:36:44,170 --> 00:36:46,890
All the points that are
smaller go to -1.

733
00:36:46,890 --> 00:36:50,390
And it's called positive ray, because
here is the ray-- very simple

734
00:36:50,390 --> 00:36:52,980
hypothesis set.

735
00:36:52,980 --> 00:36:56,930
Now in order to define the growth
function, I need a bunch of points.

736
00:36:56,930 --> 00:36:59,100
So I'm going to generate some points.

737
00:36:59,100 --> 00:37:02,250
I'm going to call them x_1 up to x_N.

738
00:37:02,250 --> 00:37:05,550
And I am going to choose them
as general as possible.

739
00:37:05,550 --> 00:37:08,140
I guess there is very little generality
when you're talking about a line.

740
00:37:08,140 --> 00:37:09,910
Just make sure that they don't
fall on each other.

741
00:37:09,910 --> 00:37:13,160
If they fall on each other, you cannot
really dichotomize them at all.

742
00:37:13,160 --> 00:37:15,070
If you put them separately,
you'll be OK.

743
00:37:15,070 --> 00:37:17,130
So you have these N points.

744
00:37:17,130 --> 00:37:20,300
Now when you apply your hypothesis,
the particular hypothesis that is

745
00:37:20,300 --> 00:37:25,970
drawn on the slide, to these points,
you are going to get this pattern.

746
00:37:25,970 --> 00:37:28,060
True?

747
00:37:28,060 --> 00:37:32,640
And you're asking yourself, how many
different patterns I can get on these

748
00:37:32,640 --> 00:37:37,410
N points by varying my hypothesis,
which means that I'm varying

749
00:37:37,410 --> 00:37:39,250
the value of 'a'?

750
00:37:39,250 --> 00:37:43,090
That is the parameter that gives me
one hypothesis versus the other.

751
00:37:43,090 --> 00:37:50,130
Formally, the hypothesis set is a set
from the real numbers to -1, +1.

752
00:37:50,130 --> 00:37:53,200
And I can actually find
an analytic formula here.

753
00:37:53,200 --> 00:37:56,630
If you want an analytic formula,
you remember the sign?

754
00:37:56,630 --> 00:37:57,890
This is, I think--

755
00:37:57,890 --> 00:38:00,900
If you apply it, that's exactly
what I described.

756
00:38:00,900 --> 00:38:05,420
Now we ask ourselves, what
is the growth function?

757
00:38:05,420 --> 00:38:05,930


758
00:38:05,930 --> 00:38:09,000
Here is a very simple argument.

759
00:38:09,000 --> 00:38:13,940
If you have N points, the value of the
dichotomy-- which ones go to blue

760
00:38:13,940 --> 00:38:19,490
and which ones go to red-- depends on
which segment between the points

761
00:38:19,490 --> 00:38:21,050
'a' will fall in.

762
00:38:21,050 --> 00:38:23,410
If 'a' falls here, you get this pattern.

763
00:38:23,410 --> 00:38:26,530
If 'a' falls here, this guy will be red.

764
00:38:26,530 --> 00:38:28,590
And the rest of the guys will be blue.

765
00:38:28,590 --> 00:38:30,260
So I get a different dichotomy.

766
00:38:30,260 --> 00:38:34,890
I get different dichotomies when
I choose a different line segment.

767
00:38:34,890 --> 00:38:38,310
How many line segments are
there to choose from?

768
00:38:38,310 --> 00:38:39,650
I have N points.

769
00:38:39,650 --> 00:38:46,240
I have N minus 1 sandwiched ones, and
one here when all of them are red,

770
00:38:46,240 --> 00:38:49,130
and one here when all of them are blue.

771
00:38:49,130 --> 00:38:49,890
Right?

772
00:38:49,890 --> 00:38:52,250
So I have N plus 1 choices.

773
00:38:52,250 --> 00:38:55,050
And that's exactly the number of
dichotomies I'm going to get on N

774
00:38:55,050 --> 00:38:57,350
points, regardless of what N is.

775
00:38:57,350 --> 00:39:01,420
So I found that the growth function,
for this thing, is exactly

776
00:39:01,420 --> 00:39:02,670
N plus 1.

777
00:39:02,670 --> 00:39:05,020


778
00:39:05,020 --> 00:39:09,020
Let's take a more sophisticated model,
and see if we get a bigger growth

779
00:39:09,020 --> 00:39:11,860
function. Because that's
the whole idea, right?

780
00:39:11,860 --> 00:39:15,280
The next guy is positive intervals.

781
00:39:15,280 --> 00:39:16,210


782
00:39:16,210 --> 00:39:17,260
What are these?

783
00:39:17,260 --> 00:39:20,470
They're like the other guys, except
they're a little bit more elaborate.

784
00:39:20,470 --> 00:39:23,000
Instead of having a ray,
you have an interval.

785
00:39:23,000 --> 00:39:25,020
Again, you're talking
about the real line.

786
00:39:25,020 --> 00:39:28,010
And you are going to define
an interval from here to here.

787
00:39:28,010 --> 00:39:31,150
And anything that lies within
here, will map to +1

788
00:39:31,150 --> 00:39:32,270
and will become blue.

789
00:39:32,270 --> 00:39:36,210
And anything outside, whether it's right
or left, will go to -1.

790
00:39:36,210 --> 00:39:36,830


791
00:39:36,830 --> 00:39:40,350
That's obviously more powerful than the
previous one, because you can think

792
00:39:40,350 --> 00:39:42,760
of the positive as having
an infinite interval.

793
00:39:42,760 --> 00:39:44,940
That's fine.

794
00:39:44,940 --> 00:39:47,330
So you put the points.

795
00:39:47,330 --> 00:39:49,030
We have done this before.

796
00:39:49,030 --> 00:39:52,310
And they get classified this way.

797
00:39:52,310 --> 00:39:57,430
And I'm asking myself, how many
different dichotomies I can get now by

798
00:39:57,430 --> 00:40:01,380
choosing really 2 parameters, the
beginning of the interval and the end

799
00:40:01,380 --> 00:40:02,200
of the interval.

800
00:40:02,200 --> 00:40:05,370
These are my 2 parameters, that will tell
me one hypothesis versus the other.

801
00:40:05,370 --> 00:40:08,270
How many different patterns can I get?

802
00:40:08,270 --> 00:40:10,200
Again, the function is very simple.

803
00:40:10,200 --> 00:40:14,570
It's defined on the real numbers.
And now the counting argument, which is

804
00:40:14,570 --> 00:40:16,020
an interesting one.

805
00:40:16,020 --> 00:40:18,010


806
00:40:18,010 --> 00:40:23,880
The way you get a different dichotomy
is by choosing 2 different line

807
00:40:23,880 --> 00:40:29,400
segments, to put the ends
of the interval in.

808
00:40:29,400 --> 00:40:32,220
If I start the interval here and
end it here, I get something.

809
00:40:32,220 --> 00:40:35,030
If I start the interval here and end
it here, I get something else.

810
00:40:35,030 --> 00:40:39,030
If I start the interval here and
end here, I get something else.

811
00:40:39,030 --> 00:40:44,330
And that is exactly one-to-one mapping
between the dichotomies and the choice

812
00:40:44,330 --> 00:40:45,580
of 2 segments.

813
00:40:45,580 --> 00:40:48,110


814
00:40:48,110 --> 00:40:51,920
So if this is the case, then I can
very simply say that the growth

815
00:40:51,920 --> 00:40:56,770
function, in this case, is the number
of ways to pick 2 segments out the

816
00:40:56,770 --> 00:40:59,590
N plus 1 segments.

817
00:40:59,590 --> 00:41:04,780
And that would be N plus 1 choose 2.

818
00:41:04,780 --> 00:41:07,780
There is only 1 missing.

819
00:41:07,780 --> 00:41:10,510
When you count, there are 2 rules--

820
00:41:10,510 --> 00:41:14,030
make sure that you count everything,
and make sure that you don't count

821
00:41:14,030 --> 00:41:15,400
anything twice.

822
00:41:15,400 --> 00:41:16,520
Very simple.

823
00:41:16,520 --> 00:41:19,210
So we counted almost everything.

824
00:41:19,210 --> 00:41:21,355
But the missing guy here is what?

825
00:41:21,355 --> 00:41:23,860


826
00:41:23,860 --> 00:41:26,800
Let's say that all of them are blue.

827
00:41:26,800 --> 00:41:28,010
Is this counted already?

828
00:41:28,010 --> 00:41:31,650
Yes, because I can choose this
segment and this segment.

829
00:41:31,650 --> 00:41:34,990
And that is already counted in this.

830
00:41:34,990 --> 00:41:38,270
But if they're all red,
what does that mean?

831
00:41:38,270 --> 00:41:40,850
It means that the beginning of the
interval, and the end of the interval,

832
00:41:40,850 --> 00:41:42,270
happen to be within the same segment.

833
00:41:42,270 --> 00:41:44,040
So they didn't capture any point.

834
00:41:44,040 --> 00:41:46,120
And that, I didn't count.

835
00:41:46,120 --> 00:41:49,050
And it doesn't matter which segment
they're in, because I will get just

836
00:41:49,050 --> 00:41:49,920
the all reds.

837
00:41:49,920 --> 00:41:51,160
So it's one dichotomy.

838
00:41:51,160 --> 00:41:55,100
So all I need to do is just add 1.

839
00:41:55,100 --> 00:41:57,070
And that's the number.

840
00:41:57,070 --> 00:41:59,930
Do a little algebra, and you get this.

841
00:41:59,930 --> 00:42:04,210
That is the growth function
for this hypothesis set.

842
00:42:04,210 --> 00:42:06,840
And now I'm happy, because
I see it's quadratic.

843
00:42:06,840 --> 00:42:09,490
It's more powerful than the previous
guy, which was linear.

844
00:42:09,490 --> 00:42:12,210


845
00:42:12,210 --> 00:42:17,240
Now let's up the ante, and
go to the third one.

846
00:42:17,240 --> 00:42:19,970
Convex sets.

847
00:42:19,970 --> 00:42:25,120
This time, I'm taking the
plane, rather than the line.

848
00:42:25,120 --> 00:42:27,130
So it's R squared.

849
00:42:27,130 --> 00:42:31,550
And my hypotheses are simply
the convex regions.

850
00:42:31,550 --> 00:42:37,260
If you look at the values of x at
which the hypothesis is +1, this

851
00:42:37,260 --> 00:42:40,830
has to be a convex region,
any convex region.

852
00:42:40,830 --> 00:42:46,100
A convex region is a region where,
if you pick any 2 points within the

853
00:42:46,100 --> 00:42:50,630
region, the entirety of the line segment
connecting them lies within

854
00:42:50,630 --> 00:42:51,160
the region.

855
00:42:51,160 --> 00:42:52,740
That's the definition.

856
00:42:52,740 --> 00:42:57,090
So this is my artwork
for a convex region.

857
00:42:57,090 --> 00:42:58,530
You take any 2 points and--

858
00:42:58,530 --> 00:42:59,890
So this is an example of that.

859
00:42:59,890 --> 00:43:01,090
The blue is the +1.

860
00:43:01,090 --> 00:43:02,070
And the red is the -1.

861
00:43:02,070 --> 00:43:03,430
That's the entire space.

862
00:43:03,430 --> 00:43:06,070
So this is a valid hypothesis.

863
00:43:06,070 --> 00:43:12,100
Now you can see that there is
an enormous variety of convex sets that

864
00:43:12,100 --> 00:43:13,550
qualify as hypotheses.

865
00:43:13,550 --> 00:43:15,610
But there are some which
don't qualify.

866
00:43:15,610 --> 00:43:19,610
For example, this one is not convex,
because of this fellow.

867
00:43:19,610 --> 00:43:21,700
Here's the line segment, and
it went out of the region.

868
00:43:21,700 --> 00:43:22,400
So that's not convex.

869
00:43:22,400 --> 00:43:24,650
We understand what the
hypothesis set is.

870
00:43:24,650 --> 00:43:26,100
Now we come to the task.

871
00:43:26,100 --> 00:43:28,710
What is the growth function
for this hypothesis set?

872
00:43:28,710 --> 00:43:32,480


873
00:43:32,480 --> 00:43:35,900
In other to answer this, what you
need is-- you put your points.

874
00:43:35,900 --> 00:43:37,750
I give you N, and you place them.

875
00:43:37,750 --> 00:43:40,790
So here is a cloud of points.

876
00:43:40,790 --> 00:43:43,920
I give you N, and you say, it seems
like putting them in general

877
00:43:43,920 --> 00:43:45,950
position is a good idea.

878
00:43:45,950 --> 00:43:48,200
So let's put them in
a general position.

879
00:43:48,200 --> 00:43:51,040
And let's try to see how many patterns
I can get out of these,

880
00:43:51,040 --> 00:43:54,580
using convex regions.

881
00:43:54,580 --> 00:43:57,370
Man, this is going to be tough
because I can see--

882
00:43:57,370 --> 00:43:58,310
Let's see.

883
00:43:58,310 --> 00:44:00,800
First, I cannot get all
of them, right?

884
00:44:00,800 --> 00:44:06,530
Because let's say I take the outermost
points, and map them all to +1.

885
00:44:06,530 --> 00:44:10,140
This will force all the internal points
to be +1, because I'm using

886
00:44:10,140 --> 00:44:11,790
a convex region.

887
00:44:11,790 --> 00:44:16,670
Therefore, I cannot get +1's for
the out guys, and any -1

888
00:44:16,670 --> 00:44:18,080
whatsoever inside.

889
00:44:18,080 --> 00:44:20,490
So that excludes a lot of dichotomies.

890
00:44:20,490 --> 00:44:23,960
Now I have to do real
counting.

891
00:44:23,960 --> 00:44:25,160
But wait a minute.

892
00:44:25,160 --> 00:44:28,290
The criterion for choosing the cloud of
points was not to make them look good

893
00:44:28,290 --> 00:44:32,290
and general, but to maximize
your growth function.

894
00:44:32,290 --> 00:44:35,910
Is there another choice for the
points that gives me more

895
00:44:35,910 --> 00:44:38,240
hypotheses than these?

896
00:44:38,240 --> 00:44:41,900
As a matter of fact, is there another
choice, for where I put the points, that

897
00:44:41,900 --> 00:44:46,680
will give me all possible dichotomies
using convex regions?

898
00:44:46,680 --> 00:44:50,960
If you succeed in that, then you
don't care about this cloud.

899
00:44:50,960 --> 00:44:53,690
The other one will count, because
you are taking the maximum.

900
00:44:53,690 --> 00:44:56,400
Here is the way to do it.

901
00:44:56,400 --> 00:45:01,671
Take a circle, and put your points
on the perimeter of that circle.

902
00:45:01,671 --> 00:45:04,760


903
00:45:04,760 --> 00:45:10,480
Now I maintain that you can get any
dichotomy you want on these points.

904
00:45:10,480 --> 00:45:11,480
What is the argument?

905
00:45:11,480 --> 00:45:14,490
Well, pick your favorite one.

906
00:45:14,490 --> 00:45:16,890
I have a bunch of blues
and a bunch of reds.

907
00:45:16,890 --> 00:45:18,930
Can I realize this using
a convex region?

908
00:45:18,930 --> 00:45:21,550
Yes.

909
00:45:21,550 --> 00:45:23,490
I just connect these guys.

910
00:45:23,490 --> 00:45:26,030
And the interior of this
goes to +1.

911
00:45:26,030 --> 00:45:28,060
And whatever is outside
goes to -1.

912
00:45:28,060 --> 00:45:30,320
And I am assured it's convex, because
the points are on the

913
00:45:30,320 --> 00:45:33,650
perimeter of a circle.

914
00:45:33,650 --> 00:45:34,420


915
00:45:34,420 --> 00:45:36,040
That means what?

916
00:45:36,040 --> 00:45:40,450
That means that the growth
function is 2 to the N,

917
00:45:40,450 --> 00:45:43,180
notwithstanding the other guy.

918
00:45:43,180 --> 00:45:46,220
You realize now a weakness in
defining the growth function as the

919
00:45:46,220 --> 00:45:50,200
maximum, because in a real learning
situation, the chances are the points

920
00:45:50,200 --> 00:45:53,500
you're going to get are not going to
end up on a perimeter of a circle.

921
00:45:53,500 --> 00:45:55,350
They are likely to be
all over the place.

922
00:45:55,350 --> 00:45:58,600
And some of them will be interior
points, in which case you're not

923
00:45:58,600 --> 00:46:01,000
going to get all possibilities.

924
00:46:01,000 --> 00:46:05,130
But we don't want to keep studying the
particular probability distribution,

925
00:46:05,130 --> 00:46:07,590
and the particular data
set you get, and so on.

926
00:46:07,590 --> 00:46:09,860
We would like to have
a simple quantity.

927
00:46:09,860 --> 00:46:12,240
And therefore, we're taking the maximum
overall, which will have

928
00:46:12,240 --> 00:46:14,350
a simple combinatorial property.

929
00:46:14,350 --> 00:46:18,820
The price we pay is that, the chances are
the bound we are going to get is

930
00:46:18,820 --> 00:46:21,280
not going to be as tight as possible.

931
00:46:21,280 --> 00:46:22,470
But that's a normal price.

932
00:46:22,470 --> 00:46:25,880
If you want a general result that
applies to all situations, it's not

933
00:46:25,880 --> 00:46:28,560
going to be all that tight
in any given situation.

934
00:46:28,560 --> 00:46:30,760
That is the normal tradeoff.

935
00:46:30,760 --> 00:46:36,010
But here, the growth function is
indeed 2 to the N.

936
00:46:36,010 --> 00:46:39,460
Just as a term, when you get all
possible hypotheses, all possible

937
00:46:39,460 --> 00:46:44,750
dichotomies, you say that the hypothesis
set shattered the points--

938
00:46:44,750 --> 00:46:46,860
broke them in every possible way.

939
00:46:46,860 --> 00:46:48,995
So we can say, can we shatter
this set, et cetera?

940
00:46:48,995 --> 00:46:49,720
That's what it means.

941
00:46:49,720 --> 00:46:53,790
You get all possible combinations
on them. Just as a term.

942
00:46:53,790 --> 00:46:57,080
Now let's look at the 3 growth functions
on one slide, in order to be

943
00:46:57,080 --> 00:46:58,250
able to compare.

944
00:46:58,250 --> 00:47:04,190
We started with the positive rays, and
we got a linear growth function.

945
00:47:04,190 --> 00:47:07,480
And then we went on to the
positive intervals.

946
00:47:07,480 --> 00:47:11,020
And we had a quadratic function.

947
00:47:11,020 --> 00:47:13,750
And that is good, because we are getting
more sophisticated and the

948
00:47:13,750 --> 00:47:15,850
growth function is getting bigger.

949
00:47:15,850 --> 00:47:18,150
And then we went to convex
sets, which are--

950
00:47:18,150 --> 00:47:20,940
It's powerful and two-dimensional
and all, but not that powerful.

951
00:47:20,940 --> 00:47:22,650
Convex sets are still--

952
00:47:22,650 --> 00:47:26,050
It's really, although we got a bigger
one, it's inordinately bigger.

953
00:47:26,050 --> 00:47:27,710
Maybe we should have gotten N cubed.

954
00:47:27,710 --> 00:47:29,340
But that's what we have.

955
00:47:29,340 --> 00:47:30,400
At least it goes this way.

956
00:47:30,400 --> 00:47:33,380
So sometimes that thing
will be too much.

957
00:47:33,380 --> 00:47:36,560
But in general, you can see the trend
that, with more sophisticated, you get

958
00:47:36,560 --> 00:47:38,360
a bigger growth function.

959
00:47:38,360 --> 00:47:39,830


960
00:47:39,830 --> 00:47:42,410
Now let's go back to the big picture,
to see where that growth

961
00:47:42,410 --> 00:47:44,020
function will fit.

962
00:47:44,020 --> 00:47:45,110


963
00:47:45,110 --> 00:47:47,010
Remember this inequality?

964
00:47:47,010 --> 00:47:47,930
Oh, yes.

965
00:47:47,930 --> 00:47:49,110
We have seen it.

966
00:47:49,110 --> 00:47:50,030
We have seen it often.

967
00:47:50,030 --> 00:47:53,110
We are tired of it!

968
00:47:53,110 --> 00:48:00,960
What we are trying to do is replace M.
And we decided to replace

969
00:48:00,960 --> 00:48:04,130
it with the growth function m.

970
00:48:04,130 --> 00:48:05,770
M can be infinity.

971
00:48:05,770 --> 00:48:09,880
m is a finite number, at most
2 to the N, so that's good.

972
00:48:09,880 --> 00:48:12,770
What happens if we replace
M with small m?

973
00:48:12,770 --> 00:48:16,380
Let's say that we can do that, which
we'll establish in the next lecture.

974
00:48:16,380 --> 00:48:19,040
What will happen?

975
00:48:19,040 --> 00:48:28,660
If your growth function happens to
be polynomial, you are in great shape.

976
00:48:28,660 --> 00:48:30,550
Why is that?

977
00:48:30,550 --> 00:48:35,540
Because if you look at this quantity,
this is a negative exponential.

978
00:48:35,540 --> 00:48:37,540
epsilon can be very, very small.

979
00:48:37,540 --> 00:48:40,190
epsilon squared can be really,
really, really small.

980
00:48:40,190 --> 00:48:44,720
But this remains a negative exponential
in N. And for any choice

981
00:48:44,720 --> 00:48:50,460
of epsilon you wish, this will kill the
heck out of any polynomial you put

982
00:48:50,460 --> 00:48:53,890
here, eventually.

983
00:48:53,890 --> 00:48:54,760
Right?

984
00:48:54,760 --> 00:48:58,130
I can put a 1000th-order polynomial,
and can have epsilon equal 10

985
00:48:58,130 --> 00:48:59,640
to the minus 6.

986
00:48:59,640 --> 00:49:03,020
And if you're patient enough, or if your
customer has enough data, which

987
00:49:03,020 --> 00:49:07,200
would be an enormous amount of data, you
will eventually get this to win.

988
00:49:07,200 --> 00:49:10,540
And you will get the probability to be
diminishingly small, which means that

989
00:49:10,540 --> 00:49:12,830
you can generalize.

990
00:49:12,830 --> 00:49:16,370
That's a very attractive observation,
because now all you need

991
00:49:16,370 --> 00:49:21,350
to do is just declare that this is
polynomial, and you're in business.

992
00:49:21,350 --> 00:49:25,000
We saw that it's not that easy
to evaluate this explicitly.

993
00:49:25,000 --> 00:49:29,310
But maybe, there is a trick that will
make us able to declare that it is

994
00:49:29,310 --> 00:49:30,660
polynomial.

995
00:49:30,660 --> 00:49:34,630
And once you declare that a hypothesis
set has a polynomial growth function,

996
00:49:34,630 --> 00:49:38,870
we can declare that learning is feasible
using that hypothesis, period.

997
00:49:38,870 --> 00:49:42,150
We may become finicky and ask ourselves,
how many examples do you

998
00:49:42,150 --> 00:49:43,430
need for what, et cetera?

999
00:49:43,430 --> 00:49:45,170
But at least, we know we can do it.

1000
00:49:45,170 --> 00:49:48,450
If you're given enough examples, you
will be able to generalize from

1001
00:49:48,450 --> 00:49:54,750
a finite set, albeit big, to the general
space with a probability assurance.

1002
00:49:54,750 --> 00:49:55,380


1003
00:49:55,380 --> 00:49:57,560
So that's pretty good.

1004
00:49:57,560 --> 00:49:58,150


1005
00:49:58,150 --> 00:50:00,000
I'm happy that this is the case.

1006
00:50:00,000 --> 00:50:04,345
So maybe we can, as I mentioned, just
prove that m_H is polynomial, the

1007
00:50:04,345 --> 00:50:05,580
growth function is polynomial.

1008
00:50:05,580 --> 00:50:06,570
Can we do that?

1009
00:50:06,570 --> 00:50:07,540
Maybe we can.

1010
00:50:07,540 --> 00:50:08,600
Maybe we cannot.

1011
00:50:08,600 --> 00:50:11,700
Here's the key notion that
will enable us to do that.

1012
00:50:11,700 --> 00:50:14,580


1013
00:50:14,580 --> 00:50:17,690
We are going to define what
is called the break point.

1014
00:50:17,690 --> 00:50:20,470
You give me a hypothesis set, and
I tell you it has a break point.

1015
00:50:20,470 --> 00:50:20,950


1016
00:50:20,950 --> 00:50:22,200
Perceptrons, 4.

1017
00:50:22,200 --> 00:50:24,650


1018
00:50:24,650 --> 00:50:27,370
Another set, the break point is 7.

1019
00:50:27,370 --> 00:50:30,410
Just one number.

1020
00:50:30,410 --> 00:50:33,150
That's much better than giving
me a full growth function for

1021
00:50:33,150 --> 00:50:35,430
every N. Just one number.

1022
00:50:35,430 --> 00:50:37,930
So what is the break point?

1023
00:50:37,930 --> 00:50:40,400
The definition is the following.

1024
00:50:40,400 --> 00:50:46,120
It's the point at which you fail
to get all possible dichotomies.

1025
00:50:46,120 --> 00:50:49,370
So you can see that, if the break point
is 3, this is not a very fancy

1026
00:50:49,370 --> 00:50:50,070
hypothesis set.

1027
00:50:50,070 --> 00:50:54,120
I can't even generate all 8
possibilities on 3 points.

1028
00:50:54,120 --> 00:50:57,810
If the break point is 100, well, that's
a pretty respectable guy, because I can

1029
00:50:57,810 --> 00:51:02,130
generate everything up to 99 points,
all 2 to the 99 of them.

1030
00:51:02,130 --> 00:51:03,730
And then I start failing at 100.

1031
00:51:03,730 --> 00:51:06,500
So you can see that the break point
also has a correspondence to the

1032
00:51:06,500 --> 00:51:09,730
complexity of the hypothesis set.

1033
00:51:09,730 --> 00:51:15,615
If no data set of size k can
be shattered by H--

1034
00:51:15,615 --> 00:51:21,080
that is, if there are no choice of
k points in which you are able to

1035
00:51:21,080 --> 00:51:23,700
generate all possible dichotomies.

1036
00:51:23,700 --> 00:51:26,898
Then you call k a break point for H.

1037
00:51:26,898 --> 00:51:29,400


1038
00:51:29,400 --> 00:51:31,270
So let's look at--

1039
00:51:31,270 --> 00:51:32,450
what is the--

1040
00:51:32,450 --> 00:51:32,620


1041
00:51:32,620 --> 00:51:33,880
So that's what it means.

1042
00:51:33,880 --> 00:51:36,980
You can't shatter, so less than
2 to the k, which are all the

1043
00:51:36,980 --> 00:51:39,830
possibilities for k data points.

1044
00:51:39,830 --> 00:51:44,580
So for the 2D perceptron, can you
think of what is the break point?

1045
00:51:44,580 --> 00:51:45,370
We did it already.

1046
00:51:45,370 --> 00:51:47,540
We didn't explicitly say
it in those terms.

1047
00:51:47,540 --> 00:51:50,660
But this is the hint.

1048
00:51:50,660 --> 00:51:51,990
For 3, we did everything.

1049
00:51:51,990 --> 00:51:55,560
For 4, we knew we cannot
do everything.

1050
00:51:55,560 --> 00:52:00,790
So it doesn't matter whether
it's 14 or 15 or 12 or 5.

1051
00:52:00,790 --> 00:52:02,230
As long as it breaks, it breaks.

1052
00:52:02,230 --> 00:52:03,800
It's not 16.

1053
00:52:03,800 --> 00:52:08,490
And therefore, in this case,
the break point is 4.

1054
00:52:08,490 --> 00:52:12,720
That number 4 will characterize the
perceptrons. Just to tell me, I

1055
00:52:12,720 --> 00:52:14,090
have a hypothesis set.

1056
00:52:14,090 --> 00:52:15,450
And it is defined--

1057
00:52:15,450 --> 00:52:17,190
I don't want to know the input space.

1058
00:52:17,190 --> 00:52:17,410


1059
00:52:17,410 --> 00:52:17,850
Wait a minute.

1060
00:52:17,850 --> 00:52:18,790
OK, I'm not going to tell
you the input space.

1061
00:52:18,790 --> 00:52:20,070
I'm going to tell you the hypotheses.

1062
00:52:20,070 --> 00:52:21,650
The hypotheses are produced by the--

1063
00:52:21,650 --> 00:52:22,820
I don't want to hear it.

1064
00:52:22,820 --> 00:52:27,290
Just tell me the break point, and I will
tell you the learning behavior.

1065
00:52:27,290 --> 00:52:32,410
Also, if you have a break point,

1066
00:52:32,410 --> 00:52:34,980
every bigger point is also
a break point.

1067
00:52:34,980 --> 00:52:38,750
That is, if you cannot get all
possibilities on 10 points, then you

1068
00:52:38,750 --> 00:52:40,520
certainly cannot get
all of them on 11.

1069
00:52:40,520 --> 00:52:42,140
If you could get them on 11,
just kill one.

1070
00:52:42,140 --> 00:52:43,710
And you will have gotten them on 10.

1071
00:52:43,710 --> 00:52:44,960


1072
00:52:44,960 --> 00:52:46,940


1073
00:52:46,940 --> 00:52:49,230
Let's look at the 3 examples, and
find what are the break points.

1074
00:52:49,230 --> 00:52:51,760


1075
00:52:51,760 --> 00:52:52,020


1076
00:52:52,020 --> 00:52:53,740
Positive rays had this guy.

1077
00:52:53,740 --> 00:52:54,880
This is a formula.

1078
00:52:54,880 --> 00:52:58,830
We can plug in for N. And we
ask ourselves, when do I get to the

1079
00:52:58,830 --> 00:53:04,070
point where I no longer get 2 to the N,
numerically for a particular value.

1080
00:53:04,070 --> 00:53:05,460


1081
00:53:05,460 --> 00:53:07,100
What is the break point here?

1082
00:53:07,100 --> 00:53:07,990
N equals 1.

1083
00:53:07,990 --> 00:53:08,720
I get 1 plus 1.

1084
00:53:08,720 --> 00:53:09,370
That's 2.

1085
00:53:09,370 --> 00:53:11,170
That also happens to be 2 to the 1.

1086
00:53:11,170 --> 00:53:12,230


1087
00:53:12,230 --> 00:53:14,830
2: N plus 1 is 3.

1088
00:53:14,830 --> 00:53:16,800
Oh, that's less than 4.

1089
00:53:16,800 --> 00:53:19,750
So 2 must be a break point.

1090
00:53:19,750 --> 00:53:21,230
This is

1091
00:53:21,230 --> 00:53:23,720
since we invested in computing the
function, we are just lazy now and

1092
00:53:23,720 --> 00:53:24,920
just substituting.

1093
00:53:24,920 --> 00:53:28,080
But you could go for the original thing,
and say that's obvious.

1094
00:53:28,080 --> 00:53:30,760
Because this particular combination
of points--

1095
00:53:30,760 --> 00:53:33,830
if I want the rightmost point to be
red, and the left one to be blue,

1096
00:53:33,830 --> 00:53:37,610
there is no way for the positive
ray to generate that.

1097
00:53:37,610 --> 00:53:39,280
And therefore, that 2 is a break point.

1098
00:53:39,280 --> 00:53:42,620
There's something where I fail.

1099
00:53:42,620 --> 00:53:44,120
Let's go for this one.

1100
00:53:44,120 --> 00:53:46,140
We need faster calculators now.

1101
00:53:46,140 --> 00:53:50,310
1, 1/2, et cetera.

1102
00:53:50,310 --> 00:53:51,070


1103
00:53:51,070 --> 00:53:51,970


1104
00:53:51,970 --> 00:53:52,520
Wow.

1105
00:53:52,520 --> 00:53:53,240
It's exactly.

1106
00:53:53,240 --> 00:53:54,860
When I put 1, it gives me 2.

1107
00:53:54,860 --> 00:53:56,440
It must be the correct formula.

1108
00:53:56,440 --> 00:53:57,440
Let's write 2.

1109
00:53:57,440 --> 00:53:57,660


1110
00:53:57,660 --> 00:53:59,290
At 4, I get 2.

1111
00:53:59,290 --> 00:54:00,245
And--

1112
00:54:00,245 --> 00:54:01,590
it calculates. What is the break point?

1113
00:54:01,590 --> 00:54:05,760


1114
00:54:05,760 --> 00:54:08,260
It must be bigger than the other guy,
because it's more elaborate.

1115
00:54:08,260 --> 00:54:10,400
And you realize it's 3.

1116
00:54:10,400 --> 00:54:12,260
If you put 2 points,
you will get the 4.

1117
00:54:12,260 --> 00:54:15,630
And if you put 3, you'll get
7, which is short of 8.

1118
00:54:15,630 --> 00:54:18,320
Again, that's not a mystery.

1119
00:54:18,320 --> 00:54:21,140
That's what you cannot get
using the interval.

1120
00:54:21,140 --> 00:54:24,350
You cannot get the middle point to be
red while the other ones are blue.

1121
00:54:24,350 --> 00:54:26,610
So you cannot get all possibilities
on 3 points.

1122
00:54:26,610 --> 00:54:27,860
Therefore, 3 is a break point.

1123
00:54:27,860 --> 00:54:30,820


1124
00:54:30,820 --> 00:54:35,660
What is the break point
for the convex sets?

1125
00:54:35,660 --> 00:54:37,600
Tell me how many points where I can fail.

1126
00:54:37,600 --> 00:54:38,820
Well, I'm never going to fail.

1127
00:54:38,820 --> 00:54:42,750
So if you like, you can
say this is infinity.

1128
00:54:42,750 --> 00:54:43,340
Let's define it

1129
00:54:43,340 --> 00:54:44,560
this way.

1130
00:54:44,560 --> 00:54:47,230
So also, the break point--
just a single number--

1131
00:54:47,230 --> 00:54:48,540
has the property we want.

1132
00:54:48,540 --> 00:54:52,090
It gets more sophisticated as the
model gets more sophisticated.

1133
00:54:52,090 --> 00:54:55,380
So what is the main result?

1134
00:54:55,380 --> 00:54:59,570
The main result is that the
first part will be--

1135
00:54:59,570 --> 00:55:04,280
if you don't have a break point,
I have news for you.

1136
00:55:04,280 --> 00:55:07,220
The growth function is
2 to the N. OK, yes.

1137
00:55:07,220 --> 00:55:08,250
That's the definition.

1138
00:55:08,250 --> 00:55:09,740
Thank you.

1139
00:55:09,740 --> 00:55:11,780
So that cannot possibly
be the main result.

1140
00:55:11,780 --> 00:55:14,840
So what is the main result?

1141
00:55:14,840 --> 00:55:18,630
The main result is that if you
have a break point, any

1142
00:55:18,630 --> 00:55:23,480
break point, 1, 5, 7000.

1143
00:55:23,480 --> 00:55:25,300
Just tell me that there
is a break point.

1144
00:55:25,300 --> 00:55:29,450
You don't even have to tell
me what is the break point.

1145
00:55:29,450 --> 00:55:32,040
We are going to make a statement
about the growth function.

1146
00:55:32,040 --> 00:55:34,040
The growth function is--

1147
00:55:34,040 --> 00:55:35,515
do I hear a drum roll?

1148
00:55:35,515 --> 00:55:39,400
[MAKES DRUM SOUND]

1149
00:55:39,400 --> 00:55:43,750
It's guaranteed to be polynomial in N.

1150
00:55:43,750 --> 00:55:46,630
Wow, we have come a long way.

1151
00:55:46,630 --> 00:55:48,630
I used to ask you what
are the hypotheses,

1152
00:55:48,630 --> 00:55:49,430
and count them.

1153
00:55:49,430 --> 00:55:51,760
That was hopeless because
it's infinity.

1154
00:55:51,760 --> 00:55:53,960
We defined the growth function,
and we have to evaluate it.

1155
00:55:53,960 --> 00:55:55,820
That was painful.

1156
00:55:55,820 --> 00:55:57,340
Then we found the break point.

1157
00:55:57,340 --> 00:55:59,140
Maybe it's easier to compute
the break point.

1158
00:55:59,140 --> 00:56:03,100
I just want to find a clever way,
and say that I cannot get it.

1159
00:56:03,100 --> 00:56:07,820
Now all I need to hear from you
is that there is a break point.

1160
00:56:07,820 --> 00:56:11,310
And I'm in business as far as the
generalization is concerned, because I

1161
00:56:11,310 --> 00:56:14,400
know that regardless of what polynomial
you get, you will be able

1162
00:56:14,400 --> 00:56:15,640
to learn eventually.

1163
00:56:15,640 --> 00:56:18,690
I will become more particular, and ask you
what is the break point, when I try to

1164
00:56:18,690 --> 00:56:21,710
find the budget of examples you need
in order to get a particular

1165
00:56:21,710 --> 00:56:22,530
performance.

1166
00:56:22,530 --> 00:56:25,810
But in principle, if I just want to say
you can use this hypothesis set,

1167
00:56:25,810 --> 00:56:29,560
and you can learn, I just want you
to tell me I have a break point.

1168
00:56:29,560 --> 00:56:32,040
That' all I want.

1169
00:56:32,040 --> 00:56:35,200
This is a remarkable result.

1170
00:56:35,200 --> 00:56:38,190
And I have to give you a puzzle
to appreciate it.

1171
00:56:38,190 --> 00:56:40,220
The idea of the puzzle
is the following.

1172
00:56:40,220 --> 00:56:44,420
If I just tell you that there's
a break point, the constraint on the

1173
00:56:44,420 --> 00:56:46,530
number of dichotomies you get,

1174
00:56:46,530 --> 00:56:48,260
because there is a break point,

1175
00:56:48,260 --> 00:56:50,240
is enormous.

1176
00:56:50,240 --> 00:56:53,840
If I tell you a break point is,
let's say, 3, how many

1177
00:56:53,840 --> 00:56:55,750
can you get on 100 points?

1178
00:56:55,750 --> 00:57:00,940
On those 100 points, for any choice of
3 guys, you cannot have all

1179
00:57:00,940 --> 00:57:07,230
possible combinations-- at any 3 points,
all 100 choose 3 of them.

1180
00:57:07,230 --> 00:57:10,570
So the combinatorial restriction
is enormous.

1181
00:57:10,570 --> 00:57:14,780
And you will end up losing possible
dichotomies in droves, because of that

1182
00:57:14,780 --> 00:57:15,780
restriction.

1183
00:57:15,780 --> 00:57:18,830
And therefore, the thing that used to
be 2 to the N, if it's unrestricted,

1184
00:57:18,830 --> 00:57:21,460
will collapse to polynomial.

1185
00:57:21,460 --> 00:57:24,990
Let's take a puzzle, and try to
compute this in a particular case.

1186
00:57:24,990 --> 00:57:27,850
Here is the puzzle.

1187
00:57:27,850 --> 00:57:30,180
We have only 3 points.

1188
00:57:30,180 --> 00:57:36,120
And for this hypothesis set, I'm telling
you that the break point is 2.

1189
00:57:36,120 --> 00:57:42,410
So you cannot get all possible four
dichotomies on any 2 points.

1190
00:57:42,410 --> 00:57:47,050
If you put x_1 and x_2, you cannot get
-1 -1, -1 +1,

1191
00:57:47,050 --> 00:57:50,080
+1 -1, and +1 +1. All of them.

1192
00:57:50,080 --> 00:57:50,580
You cannot get it.

1193
00:57:50,580 --> 00:57:53,370
One of them has to be missing.

1194
00:57:53,370 --> 00:57:57,620
So I'm asking you, given that this is
the constraint, how many dichotomies can

1195
00:57:57,620 --> 00:57:59,910
you get on 3 points?

1196
00:57:59,910 --> 00:58:02,100
You can see, this is what I'm trying to
do because I'm telling you that the

1197
00:58:02,100 --> 00:58:04,490
restriction on 2 will--

1198
00:58:04,490 --> 00:58:06,910
If I didn't have the restriction,
I would be putting eight.

1199
00:58:06,910 --> 00:58:08,080
So I'm just telling you this case.

1200
00:58:08,080 --> 00:58:09,830
So how many do I get?

1201
00:58:09,830 --> 00:58:14,440
For visual clarity, I'm going to
express them as either black or white

1202
00:58:14,440 --> 00:58:16,400
circles, just for you to be able to--

1203
00:58:16,400 --> 00:58:18,120
instead of writing -1 or +1.

1204
00:58:18,120 --> 00:58:18,700


1205
00:58:18,700 --> 00:58:19,830
This dichotomy is fine.

1206
00:58:19,830 --> 00:58:21,390
It doesn't violate anything.

1207
00:58:21,390 --> 00:58:23,910
I've only one possibility.

1208
00:58:23,910 --> 00:58:26,870
So we keep adding.

1209
00:58:26,870 --> 00:58:27,860
Everything is fine.

1210
00:58:27,860 --> 00:58:31,530
As a matter of fact, everything will
remain fine until we get to four, because

1211
00:58:31,530 --> 00:58:34,120
the whole idea is that I cannot
get all four on any of them.

1212
00:58:34,120 --> 00:58:37,050
So if I have less than four, I cannot
possibly get four combinations.

1213
00:58:37,050 --> 00:58:38,290
You see what the point is.

1214
00:58:38,290 --> 00:58:39,360
This is still allowed.

1215
00:58:39,360 --> 00:58:40,950
I'm going through it as a binary one.

1216
00:58:40,950 --> 00:58:43,710
So this is 0 0 0, 0 0 1, et cetera.

1217
00:58:43,710 --> 00:58:46,460


1218
00:58:46,460 --> 00:58:47,710
I'm still OK, right?

1219
00:58:47,710 --> 00:58:52,720


1220
00:58:52,720 --> 00:58:54,970
Am I still OK?

1221
00:58:54,970 --> 00:59:00,210
[MAKES BUZZER SOUND]

1222
00:59:00,210 --> 00:59:01,400
You have violated the constraint.

1223
00:59:01,400 --> 00:59:04,980
You cannot put the last row, because
it now violates the constraint.

1224
00:59:04,980 --> 00:59:06,930
I have to take it out.

1225
00:59:06,930 --> 00:59:09,410
So let's take it out.

1226
00:59:09,410 --> 00:59:10,380
Try the next guy.

1227
00:59:10,380 --> 00:59:11,630
Maybe we are in luck.

1228
00:59:11,630 --> 00:59:15,186


1229
00:59:15,186 --> 00:59:16,436
Are we OK?

1230
00:59:16,436 --> 00:59:19,628


1231
00:59:19,628 --> 00:59:20,130
OK.

1232
00:59:20,130 --> 00:59:20,860
That's promising.

1233
00:59:20,860 --> 00:59:21,610


1234
00:59:21,610 --> 00:59:23,750
So let's go for the next guy.

1235
00:59:23,750 --> 00:59:26,280
Maybe we'll get it.

1236
00:59:26,280 --> 00:59:27,900
Are we OK?

1237
00:59:27,900 --> 00:59:34,100
[MAKES BUZZER SOUND]

1238
00:59:34,100 --> 00:59:35,930
Tough.

1239
00:59:35,930 --> 00:59:36,300


1240
00:59:36,300 --> 00:59:37,780
So we have to take out the last row.

1241
00:59:37,780 --> 00:59:43,210


1242
00:59:43,210 --> 00:59:46,040
How about this one?

1243
00:59:46,040 --> 00:59:47,290
Nope.

1244
00:59:47,290 --> 00:59:48,870


1245
00:59:48,870 --> 00:59:49,310


1246
00:59:49,310 --> 00:59:50,710
We take it out.

1247
00:59:50,710 --> 00:59:53,520
We don't have too many
options left, right?

1248
00:59:53,520 --> 00:59:54,360
Actually, this is the last guy.

1249
00:59:54,360 --> 00:59:55,900
It had better work.

1250
00:59:55,900 --> 00:59:58,190
Does it work?

1251
00:59:58,190 --> 00:59:59,440
No.

1252
00:59:59,440 --> 01:00:03,540


1253
01:00:03,540 --> 01:00:04,790


1254
01:00:04,790 --> 01:00:07,430


1255
01:00:07,430 --> 01:00:09,200
So that's what we can do.

1256
01:00:09,200 --> 01:00:11,930
We lost half of them.

1257
01:00:11,930 --> 01:00:15,350
Now you may think, maybe you
messed it up because you

1258
01:00:15,350 --> 01:00:16,560
started very regularly.

1259
01:00:16,560 --> 01:00:16,910
Just started

1260
01:00:16,910 --> 01:00:17,990
from all 0, 0 0 1.

1261
01:00:17,990 --> 01:00:21,190
But if I started differently,
I may be able to achieve more.

1262
01:00:21,190 --> 01:00:23,080
It's conceivable.

1263
01:00:23,080 --> 01:00:26,110
Please don't lose sleep over it.

1264
01:00:26,110 --> 01:00:33,750
The only row you are going to be able
to add to this table is this one.

1265
01:00:33,750 --> 01:00:35,540


1266
01:00:35,540 --> 01:00:36,590
This is indeed the solution.

1267
01:00:36,590 --> 01:00:38,220
And you can verify it at home.

1268
01:00:38,220 --> 01:00:39,200


1269
01:00:39,200 --> 01:00:43,180
Now we know that indeed the
break point is a very good restriction.

1270
01:00:43,180 --> 01:00:47,270
And we are going, in the next lecture,
to prove that it actually leads to

1271
01:00:47,270 --> 01:00:50,550
a polynomial growth, which is
the main result we want.

1272
01:00:50,550 --> 01:00:51,880
Let me stop here.

1273
01:00:51,880 --> 01:00:54,382
And we will take the questions
after a short break.

1274
01:00:54,382 --> 01:01:02,254


1275
01:01:02,254 --> 01:01:03,950
Let's start with the questions.

1276
01:01:03,950 --> 01:01:07,670
MODERATOR: The first question is,
what if the target or the

1277
01:01:07,670 --> 01:01:07,680


1278
01:01:07,680 --> 01:01:09,740
hypotheses are not binary?

1279
01:01:09,740 --> 01:01:12,190
PROFESSOR: There is a counterpart

1280
01:01:12,190 --> 01:01:15,560
for the entire theory that
I'm going to develop, for

1281
01:01:15,560 --> 01:01:19,840
real-valued functions and other
types of functions.

1282
01:01:19,840 --> 01:01:25,370
The development of the theory is
technical enough, that I'm going to

1283
01:01:25,370 --> 01:01:29,620
develop it only for the binary case,
because it is manageable.

1284
01:01:29,620 --> 01:01:32,820
And it carries all of the
concepts that you need.

1285
01:01:32,820 --> 01:01:36,260
The other case is more technical.

1286
01:01:36,260 --> 01:01:42,020
And I don't find the value of going to
that level of technicality useful, in

1287
01:01:42,020 --> 01:01:43,930
terms of adding insight.

1288
01:01:43,930 --> 01:01:47,800
What I'm going to do is, I'm going
to apply a different approach to

1289
01:01:47,800 --> 01:01:51,240
real-valued functions, which is
the bias-variance tradeoff.

1290
01:01:51,240 --> 01:01:54,560
And it's a completely different approach
from this one, that will give

1291
01:01:54,560 --> 01:01:58,320
us another angle on generalization
that is particularly suitable for

1292
01:01:58,320 --> 01:01:59,700
real-valued functions.

1293
01:01:59,700 --> 01:02:03,510
But the short answer is that, if the
function is not binary, there is

1294
01:02:03,510 --> 01:02:06,510
a counterpart to what I'm
saying that will work.

1295
01:02:06,510 --> 01:02:11,970
But it is significantly more technical
than the one I am developing.

1296
01:02:11,970 --> 01:02:14,500
MODERATOR: Just as a sanity check.

1297
01:02:14,500 --> 01:02:14,505


1298
01:02:14,505 --> 01:02:20,410
When the hypothesis set can
shatter the points, this is

1299
01:02:20,410 --> 01:02:22,650
a bad thing, right?

1300
01:02:22,650 --> 01:02:24,260
PROFESSOR: OK.

1301
01:02:24,260 --> 01:02:29,640
There is a tradeoff that will stay
with us for the entire course.

1302
01:02:29,640 --> 01:02:32,340
It's bad and good.

1303
01:02:32,340 --> 01:02:36,220
If you shatter the points, it's good
for fitting the data, because I know

1304
01:02:36,220 --> 01:02:39,270
that if you give me the data, regardless
of what your data is, I'm

1305
01:02:39,270 --> 01:02:42,040
going to be able to fit them because, I
have something that can generate

1306
01:02:42,040 --> 01:02:45,110
a hypothesis for any particular
set of combinations.

1307
01:02:45,110 --> 01:02:47,790
So if your question is,
can I fit the data?

1308
01:02:47,790 --> 01:02:49,810
Then shattering is good.

1309
01:02:49,810 --> 01:02:53,290
When you go to generalization,
shattering is bad, because basically

1310
01:02:53,290 --> 01:02:55,040
you can get anything.

1311
01:02:55,040 --> 01:02:57,280
So it doesn't mean anything
that you fit the data.

1312
01:02:57,280 --> 01:02:59,850
And therefore, you have less hope
of generalization, which will be

1313
01:02:59,850 --> 01:03:02,140
formalized through the
theoretical results.

1314
01:03:02,140 --> 01:03:08,630
And the correct answer is, what is the
good balance between the two extremes?

1315
01:03:08,630 --> 01:03:12,310
And then we'll find a value for which
we are not exactly shattering the

1316
01:03:12,310 --> 01:03:15,470
points, but we are not very restricted,
in which we are getting some

1317
01:03:15,470 --> 01:03:16,470
approximation,

1318
01:03:16,470 --> 01:03:18,110
and we're getting some generalization.

1319
01:03:18,110 --> 01:03:21,040
And that will come up.

1320
01:03:21,040 --> 01:03:24,000
MODERATOR: Is there a similar
trick to the one you used for convex

1321
01:03:24,000 --> 01:03:25,250
sets in higher dimensions?

1322
01:03:25,250 --> 01:03:29,060


1323
01:03:29,060 --> 01:03:31,640
PROFESSOR: So if you--

1324
01:03:31,640 --> 01:03:31,650


1325
01:03:31,650 --> 01:03:36,610
The principle I explained,
I explained it in terms of

1326
01:03:36,610 --> 01:03:39,040
two-dimensional and perceptrons.

1327
01:03:39,040 --> 01:03:43,285
If you look at the essence
of it, the space is X.

1328
01:03:43,285 --> 01:03:45,300
It could be anything.

1329
01:03:45,300 --> 01:03:47,810
The only restrictions I have
are binary functions.

1330
01:03:47,810 --> 01:03:49,590
So this could be a high-dimensional space.

1331
01:03:49,590 --> 01:03:52,160
And the surfaces will be very
sophisticated surfaces.

1332
01:03:52,160 --> 01:03:56,370
And all I'm reading off, as far as this
lecture is concerned, is how many

1333
01:03:56,370 --> 01:03:58,680
patterns do I get on a number
of N points.

1334
01:03:58,680 --> 01:04:02,000


1335
01:04:02,000 --> 01:04:04,700
MODERATOR: Also a question
on the complexity.

1336
01:04:04,700 --> 01:04:04,710


1337
01:04:04,710 --> 01:04:08,200
Why is usually polynomial time
considered as acceptable?

1338
01:04:08,200 --> 01:04:09,050
PROFESSOR: OK.

1339
01:04:09,050 --> 01:04:15,560
Polynomial, in this case, is polynomial
growth in the number of points N.

1340
01:04:15,560 --> 01:04:20,300
It just so happens that we are working
with the Hoeffding inequality that

1341
01:04:20,300 --> 01:04:24,580
gives us a very helpful term, which
is the negative exponential.

1342
01:04:24,580 --> 01:04:28,180
And therefore, if you get a polynomial,
as I mentioned, any

1343
01:04:28,180 --> 01:04:31,795
polynomial, you are guaranteed that for
a large enough N, the probability--

1344
01:04:31,795 --> 01:04:34,780
the right-hand side of the Hoeffding,
including the growth

1345
01:04:34,780 --> 01:04:36,150
function, will be small.

1346
01:04:36,150 --> 01:04:39,080
And therefore, the probability of
something bad happening is small.

1347
01:04:39,080 --> 01:04:39,640


1348
01:04:39,640 --> 01:04:45,730
Now obviously, there are other functions
that also will be killed by

1349
01:04:45,730 --> 01:04:47,290
the negative exponential.

1350
01:04:47,290 --> 01:04:50,070
For example, if I had a growth function
of the form, let's say, e to

1351
01:04:50,070 --> 01:04:53,310
the square root of N, that's
not a polynomial.

1352
01:04:53,310 --> 01:04:56,240
But that will also be killed by the
negative exponential, because it's

1353
01:04:56,240 --> 01:04:57,950
square root versus the other one.

1354
01:04:57,950 --> 01:05:01,170
It just so happens that we are in the
very fortunate situation that the

1355
01:05:01,170 --> 01:05:07,390
growth function is either identically
2 to the N, or else it's polynomial.

1356
01:05:07,390 --> 01:05:09,440
There is nothing in between.

1357
01:05:09,440 --> 01:05:13,110
If you draw something that is
super polynomial and sub exponential

1358
01:05:13,110 --> 01:05:16,600
and try to find the hypothesis set
for which this is a growth

1359
01:05:16,600 --> 01:05:18,450
function, you will fail.

1360
01:05:18,450 --> 01:05:20,280
So I'm getting it for free.

1361
01:05:20,280 --> 01:05:23,270
I'm just taking the simplicity
of the polynomial, because lucky for

1362
01:05:23,270 --> 01:05:25,370
me, the polynomials are the
ones that come out.

1363
01:05:25,370 --> 01:05:28,090
And they happen to serve the purpose.

1364
01:05:28,090 --> 01:05:29,150
MODERATOR: OK.

1365
01:05:29,150 --> 01:05:34,240
A few people are asking, could you
repeat the constraints of the puzzle?

1366
01:05:34,240 --> 01:05:35,530
Because they didn't get the--

1367
01:05:35,530 --> 01:05:36,780
PROFESSOR: OK.

1368
01:05:36,780 --> 01:05:39,340


1369
01:05:39,340 --> 01:05:40,830
Let's look at the puzzle.

1370
01:05:40,830 --> 01:05:41,470


1371
01:05:41,470 --> 01:05:45,130
I am putting 3 bits on every row.

1372
01:05:45,130 --> 01:05:49,300
I'm trying to get as many different rows
as possible, under the constraint

1373
01:05:49,300 --> 01:05:55,530
that if you focus on any 2 of them-- so
if I focus on x_1 and x_2 and go down

1374
01:05:55,530 --> 01:06:00,090
the columns, it must be that one
of the possible patterns for

1375
01:06:00,090 --> 01:06:01,770
x_1 and x_2 is missing.

1376
01:06:01,770 --> 01:06:05,760
Because I'm saying that 2 is
a break point, so I cannot

1377
01:06:05,760 --> 01:06:07,400
shatter any 2 points.

1378
01:06:07,400 --> 01:06:12,340
Therefore, I cannot shatter x_1 and x_2,
among others, meaning that I cannot

1379
01:06:12,340 --> 01:06:13,540
get all possible patterns.

1380
01:06:13,540 --> 01:06:17,350
There are only four possible patterns,
which is, if you take it as a binary

1381
01:06:17,350 --> 01:06:19,480
0 0, 0 1, 1 0, 1 1.

1382
01:06:19,480 --> 01:06:22,760
And I'm representing them using
the circles.

1383
01:06:22,760 --> 01:06:28,920
In this case, the x_1 and x_2
get 0 0, so to speak. If I

1384
01:06:28,920 --> 01:06:31,900
keep adding a pattern--

1385
01:06:31,900 --> 01:06:33,940
So let's look at here.

1386
01:06:33,940 --> 01:06:35,460
x_1 and x_2,

1387
01:06:35,460 --> 01:06:38,120
how many patterns do they have?

1388
01:06:38,120 --> 01:06:38,870
They have this pattern.

1389
01:06:38,870 --> 01:06:39,450
They have it again.

1390
01:06:39,450 --> 01:06:40,390
That doesn't count.

1391
01:06:40,390 --> 01:06:43,430
So there's only one pattern
here, plus one, is two.

1392
01:06:43,430 --> 01:06:46,030
So on x_1 and x_2, I have
only two patterns.

1393
01:06:46,030 --> 01:06:49,340
So I haven't violated anything, because
I will be only violating if I get all

1394
01:06:49,340 --> 01:06:50,280
four patterns.

1395
01:06:50,280 --> 01:06:52,900
So I'm OK, and similarly
for the other guys.

1396
01:06:52,900 --> 01:06:56,550
Things become interesting when you
start getting the fourth row.

1397
01:06:56,550 --> 01:07:03,000
Now again, if you look at the first 2
points, I get one pattern here and one

1398
01:07:03,000 --> 01:07:03,490
pattern here.

1399
01:07:03,490 --> 01:07:04,460
There are only two patterns.

1400
01:07:04,460 --> 01:07:07,750
Nothing is violated as far these
2 points are concerned.

1401
01:07:07,750 --> 01:07:11,610
But the constraint has to be satisfied
for any choice of 2 points.

1402
01:07:11,610 --> 01:07:15,250
So if you particularly choose x_2 and x_3,
and count the number of patterns,

1403
01:07:15,250 --> 01:07:18,130
you realize, 0 0,
0 1, 1 0, 1 1.

1404
01:07:18,130 --> 01:07:19,630
I am in trouble.

1405
01:07:19,630 --> 01:07:21,820
That's why we put it in red.

1406
01:07:21,820 --> 01:07:25,020
Because now these guys have
all possible patterns.

1407
01:07:25,020 --> 01:07:28,640
And I know, by the assumption of the
problem, that I cannot get all four

1408
01:07:28,640 --> 01:07:30,430
patterns on any 2 points.

1409
01:07:30,430 --> 01:07:31,790
So I cannot get this.

1410
01:07:31,790 --> 01:07:35,290
So I'm unable to add this row
under those constraints.

1411
01:07:35,290 --> 01:07:37,500
And therefore, I'm taking it away.

1412
01:07:37,500 --> 01:07:38,880
And I'm going through the exercise.

1413
01:07:38,880 --> 01:07:43,450
And every time I put a row, I keep
an eye on all possible combinations.

1414
01:07:43,450 --> 01:07:44,710
So here, I put--

1415
01:07:44,710 --> 01:07:46,450
let's look at x_1 and x_2.

1416
01:07:46,450 --> 01:07:49,990
1 pattern, 2, 3.

1417
01:07:49,990 --> 01:07:51,150
I'm OK.

1418
01:07:51,150 --> 01:07:54,820
x_2 and x_3, 1 pattern, which
is here and here.

1419
01:07:54,820 --> 01:07:55,940
2, 3.

1420
01:07:55,940 --> 01:07:57,220
I'm also OK.

1421
01:07:57,220 --> 01:07:59,730
And then you put x_1, x_3.

1422
01:07:59,730 --> 01:07:59,995


1423
01:07:59,995 --> 01:08:00,830
Here is a pattern.

1424
01:08:00,830 --> 01:08:02,250
It repeats here.

1425
01:08:02,250 --> 01:08:05,590
0 0 and 0 0.

1426
01:08:05,590 --> 01:08:07,000
So that's one.

1427
01:08:07,000 --> 01:08:09,340
And then I get this one
and this one, 3.

1428
01:08:09,340 --> 01:08:11,690
So this one is perfect,
everyone.

1429
01:08:11,690 --> 01:08:14,270
Not perfect in any sense, except
that I didn't violate anything.

1430
01:08:14,270 --> 01:08:16,279
So I'm allowed to put that row.

1431
01:08:16,279 --> 01:08:20,830
Now when I extend this further, and start
putting the new guys, for this

1432
01:08:20,830 --> 01:08:22,060
guy, there is a violation.

1433
01:08:22,060 --> 01:08:24,600
And you can scan your eyes and
try to find the violation.

1434
01:08:24,600 --> 01:08:26,399
And I'm highlighting it in red.

1435
01:08:26,399 --> 01:08:30,049
So I'm showing you that for x_1 and
x_3, there are the four patterns.

1436
01:08:30,049 --> 01:08:32,370
Here's one pattern, the second one.

1437
01:08:32,370 --> 01:08:34,819
I didn't count this one, just because
it's already happened.

1438
01:08:34,819 --> 01:08:39,240
So I just highlight four different ones,
and then the third one and fourth one.

1439
01:08:39,240 --> 01:08:42,689
So I cannot possibly add this row,
because it violates the constraint on

1440
01:08:42,689 --> 01:08:43,560
these 2 points.

1441
01:08:43,560 --> 01:08:45,550
So I take it out and keep adding.

1442
01:08:45,550 --> 01:08:47,859
Another attempt, this is the next guy.

1443
01:08:47,859 --> 01:08:48,960
It still violates.

1444
01:08:48,960 --> 01:08:49,819
Why does it violate?

1445
01:08:49,819 --> 01:08:50,710
For the same argument.

1446
01:08:50,710 --> 01:08:51,490
Look at the red guys.

1447
01:08:51,490 --> 01:08:52,520
You find all possible patterns.

1448
01:08:52,520 --> 01:08:54,010
So I cannot have it.

1449
01:08:54,010 --> 01:08:55,100
So we take it away.

1450
01:08:55,100 --> 01:08:57,580
And then the last one that
is remaining is this guy.

1451
01:08:57,580 --> 01:09:01,310
And that also doesn't work, because
it violates it for those guys.

1452
01:09:01,310 --> 01:09:02,609
You can look at it and verify.

1453
01:09:02,609 --> 01:09:04,880
And the conclusion here is that
I cannot add anything.

1454
01:09:04,880 --> 01:09:06,359
So that's what I'm stuck with.

1455
01:09:06,359 --> 01:09:10,310
And therefore, the number of different
rows I can get under the constraint

1456
01:09:10,310 --> 01:09:11,590
that 2 is a break point--

1457
01:09:11,590 --> 01:09:13,950
in this case, is 4.

1458
01:09:13,950 --> 01:09:18,880
Obviously, the remark I mentioned is
that maybe you can start instead of

1459
01:09:18,880 --> 01:09:22,010
gradually from 0 0 0, 0 0 1,
maybe you can start

1460
01:09:22,010 --> 01:09:23,800
more cleverly or something.

1461
01:09:23,800 --> 01:09:28,029
But however, anyway you try it, it's
sufficiently symmetric in the bits

1462
01:09:28,029 --> 01:09:29,109
that it doesn't make a difference.

1463
01:09:29,109 --> 01:09:32,506
You will be stuck with at most 4.

1464
01:09:32,506 --> 01:09:32,988


1465
01:09:32,988 --> 01:09:35,180
MODERATOR: OK.

1466
01:09:35,180 --> 01:09:39,399
In the slide with the Hoeffding
inequality, does anything change when

1467
01:09:39,399 --> 01:09:41,050
you change--

1468
01:09:41,050 --> 01:09:43,750
specifically, does a probability measure
change when you change from

1469
01:09:43,750 --> 01:09:47,649
a hypothesis to dichotomy?

1470
01:09:47,649 --> 01:09:48,430
PROFESSOR: For this one?

1471
01:09:48,430 --> 01:09:48,939
MODERATOR: Yeah.

1472
01:09:48,939 --> 01:09:49,370
PROFESSOR: Yeah.

1473
01:09:49,370 --> 01:09:51,290
The idea here,

1474
01:09:51,290 --> 01:09:55,670
M is the number of hypotheses, period.

1475
01:09:55,670 --> 01:09:57,310
So it's infinity for perceptrons.

1476
01:09:57,310 --> 01:09:59,060
We have to live with that.

1477
01:09:59,060 --> 01:10:02,620
In our attempt to replace it with the
growth function, we are going to

1478
01:10:02,620 --> 01:10:05,870
replace it by something that is not
infinite, bounded above by 2 to the N.

1479
01:10:05,870 --> 01:10:07,870
As you can see, 2 to the N is not
really helpful because I have

1480
01:10:07,870 --> 01:10:09,640
a positive exponential and
negative exponential.

1481
01:10:09,640 --> 01:10:12,140
And that's not very decisive.

1482
01:10:12,140 --> 01:10:16,220
Therefore I am trying to find if I can
put a growth function-- not only put

1483
01:10:16,220 --> 01:10:18,250
the growth function here, but also
show that the growth function is

1484
01:10:18,250 --> 01:10:20,460
polynomial for the models
of interest

1485
01:10:20,460 --> 01:10:24,755
that I have, and therefore be able to get
this to be a small quantity for

1486
01:10:24,755 --> 01:10:28,040
a real learning model, like the perceptron
or the other one, neural

1487
01:10:28,040 --> 01:10:28,550
networks, et cetera.

1488
01:10:28,550 --> 01:10:33,000
All of these will have a polynomial
growth function, as we will see.

1489
01:10:33,000 --> 01:10:37,360
So that's where the number of
hypotheses, which is M, goes to the

1490
01:10:37,360 --> 01:10:39,550
number of dichotomies, which
is the growth function.

1491
01:10:39,550 --> 01:10:41,110
Not a direct substitution,
as we will see.

1492
01:10:41,110 --> 01:10:42,800
There are some technicalities
involved.

1493
01:10:42,800 --> 01:10:47,210
But that is what gets me the right-hand
side to be a manageable right

1494
01:10:47,210 --> 01:10:51,790
hand side, and goes to 0 as N grows,
which tells me that the probability of

1495
01:10:51,790 --> 01:10:54,560
generalization will be high.

1496
01:10:54,560 --> 01:10:55,200
MODERATOR: OK.

1497
01:10:55,200 --> 01:10:59,750
Is there a systematic way
to find the break points?

1498
01:10:59,750 --> 01:11:01,390
PROFESSOR: There is.

1499
01:11:01,390 --> 01:11:06,140
It's not one size fits all.

1500
01:11:06,140 --> 01:11:06,150


1501
01:11:06,150 --> 01:11:08,870
The are arguments, for example, you
can go for neural networks.

1502
01:11:08,870 --> 01:11:13,440
And sometimes you find it by finding
a particular combination that you cannot

1503
01:11:13,440 --> 01:11:16,010
break, and argue that this
is the break point.

1504
01:11:16,010 --> 01:11:17,640
Sometimes you can argue
by--

1505
01:11:17,640 --> 01:11:20,890
Let me try to find a crude estimate
for the growth function.

1506
01:11:20,890 --> 01:11:23,730
Let's say the growth function
cannot be more than this.

1507
01:11:23,730 --> 01:11:26,000
And then as you go by, you realize
that this is not exponential.

1508
01:11:26,000 --> 01:11:27,830
So there has to be a break point
at some point.

1509
01:11:27,830 --> 01:11:31,490
This would be less than 2 to the N, and
therefore will be a break point.

1510
01:11:31,490 --> 01:11:34,080
So in that case, the estimate for the
break point will be just an estimate.

1511
01:11:34,080 --> 01:11:37,920
It will not be an exact value.

1512
01:11:37,920 --> 01:11:39,020
But it will a maximum.

1513
01:11:39,020 --> 01:11:42,280
We have a question in house.

1514
01:11:42,280 --> 01:11:43,260
STUDENT: Hi.

1515
01:11:43,260 --> 01:11:47,660
So in this slide, the top end is the
number of testing points and the lower

1516
01:11:47,660 --> 01:11:49,095
end is the number of training points.

1517
01:11:49,095 --> 01:11:49,660
PROFESSOR: Yeah.

1518
01:11:49,660 --> 01:11:51,610
N is always the size of the sample.

1519
01:11:51,610 --> 01:11:55,430
And it's a question of interpretation
between the two, whether that sample is

1520
01:11:55,430 --> 01:11:58,770
used for testing, which means that you
have already frozen your hypothesis,

1521
01:11:58,770 --> 01:12:00,960
and you are just verifying,
testing it.

1522
01:12:00,960 --> 01:12:03,340
Or in the other case, you haven't
frozen your hypothesis.

1523
01:12:03,340 --> 01:12:06,640
And you are using the same sample
to go around and find one.

1524
01:12:06,640 --> 01:12:10,570
And you are charged for the going
around aspect by M.

1525
01:12:10,570 --> 01:12:15,650
STUDENT: So let's say that our
customer gives us k sample points.

1526
01:12:15,650 --> 01:12:19,135
How do we decide how many of them do
we reserve for testing points, how

1527
01:12:19,135 --> 01:12:20,970
many for training?

1528
01:12:20,970 --> 01:12:22,470
PROFESSOR: This is
a very good point.

1529
01:12:22,470 --> 01:12:26,340
There will be a lecture down the
road called validation, in which this

1530
01:12:26,340 --> 01:12:29,060
is going to be addressed
very specifically.

1531
01:12:29,060 --> 01:12:30,250
There are rules of thumb.

1532
01:12:30,250 --> 01:12:31,550
There are some mathematical results,

1533
01:12:31,550 --> 01:12:33,660
but there is a rule of thumb.

1534
01:12:33,660 --> 01:12:37,020
There are few rules of thumb that I'm
going to say without proof, that

1535
01:12:37,020 --> 01:12:38,260
stood the test of time.

1536
01:12:38,260 --> 01:12:41,480
And one of the rules of thumb has to
apply to, how many do we reserve, in

1537
01:12:41,480 --> 01:12:45,560
order to first not to diminish the
training set very much, and still have

1538
01:12:45,560 --> 01:12:48,630
a big enough test set so that
the estimate is reliable?

1539
01:12:48,630 --> 01:12:49,600
So this will come up.

1540
01:12:49,600 --> 01:12:51,260
Thank you.

1541
01:12:51,260 --> 01:12:54,780
There is another question.

1542
01:12:54,780 --> 01:12:54,790


1543
01:12:54,790 --> 01:12:55,460
STUDENT: Hi, professor.

1544
01:12:55,460 --> 01:12:56,280
I have one question.

1545
01:12:56,280 --> 01:13:01,610
So for 2 hypotheses that have the same
dichotomy, is it true that the

1546
01:13:01,610 --> 01:13:04,776
in-sample error is the same
for the 2 hypotheses?

1547
01:13:04,776 --> 01:13:05,730
PROFESSOR: OK.

1548
01:13:05,730 --> 01:13:10,090
If it has the same dichotomy, it's even
a stronger condition than this,

1549
01:13:10,090 --> 01:13:13,090
because it returns exactly
the same values.

1550
01:13:13,090 --> 01:13:16,330
Now the in-sample error is
the fraction of errors I

1551
01:13:16,330 --> 01:13:17,330
got right and wrong.

1552
01:13:17,330 --> 01:13:18,680
The target function is fixed.

1553
01:13:18,680 --> 01:13:20,290
So that is not going to change.

1554
01:13:20,290 --> 01:13:23,400
So obviously, I'm going to get
the same pattern of errors.

1555
01:13:23,400 --> 01:13:26,320
And if I get the same pattern of errors,
then obviously I'm getting the

1556
01:13:26,320 --> 01:13:29,810
same fraction of errors,
among other things.

1557
01:13:29,810 --> 01:13:32,760
Now if you're asking, for these
2 hypotheses, what is the

1558
01:13:32,760 --> 01:13:33,690
out-of-sample error?

1559
01:13:33,690 --> 01:13:36,400
That's a different story, because for the
out-of-sample error, you take the

1560
01:13:36,400 --> 01:13:38,610
hypothesis in its entirety.

1561
01:13:38,610 --> 01:13:41,610
So in spite of the fact that it's the
same on the set of points, it may be

1562
01:13:41,610 --> 01:13:44,130
not the same on the entire input space,
which it isn't because they're

1563
01:13:44,130 --> 01:13:45,030
different hypotheses.

1564
01:13:45,030 --> 01:13:46,400
And therefore, you get
a different E_out.

1565
01:13:46,400 --> 01:13:47,230
But the answer is yes.

1566
01:13:47,230 --> 01:13:48,410
You will get the same in-sample error.

1567
01:13:48,410 --> 01:13:48,490
STUDENT: Oh, yes.

1568
01:13:48,490 --> 01:13:48,630
I see.

1569
01:13:48,630 --> 01:13:49,380
That's why I was asking.

1570
01:13:49,380 --> 01:13:54,950
Because I think that the out-of-sample
error is different for 2 hypotheses.

1571
01:13:54,950 --> 01:13:56,980
So can we replace the M with--

1572
01:13:56,980 --> 01:13:57,350
PROFESSOR: Exactly.

1573
01:13:57,350 --> 01:14:01,010
And the biggest technicality
in the proof--

1574
01:14:01,010 --> 01:14:03,800
We were saying, we're going to
replace M by the growth function.

1575
01:14:03,800 --> 01:14:05,600
That's a very helpful thing.

1576
01:14:05,600 --> 01:14:07,280
Now, there has to be a proof.

1577
01:14:07,280 --> 01:14:10,370
And I will argue for the proof, and
the overlapping aspects, and some of this.

1578
01:14:10,370 --> 01:14:13,530
The key point is, what do
I do about this fellow?

1579
01:14:13,530 --> 01:14:16,860
Because when I consider the sample, this
one is very much under control.

1580
01:14:16,860 --> 01:14:19,240
As you said, if I have 2 hypotheses
that are the same here, they are the

1581
01:14:19,240 --> 01:14:19,890
same here.

1582
01:14:19,890 --> 01:14:21,500
But they are not the same here.

1583
01:14:21,500 --> 01:14:24,350
So the statement here depends on
E_out, depends on the whole input space.

1584
01:14:24,350 --> 01:14:25,890
So how am I going to
get away with that?

1585
01:14:25,890 --> 01:14:28,750
That's really the main technical
contribution for the proof.

1586
01:14:28,750 --> 01:14:30,010
And that will come up next time.

1587
01:14:30,010 --> 01:14:31,720
STUDENT: Sure, thank you.

1588
01:14:31,720 --> 01:14:31,728


1589
01:14:31,728 --> 01:14:34,120
PROFESSOR: Sure.

1590
01:14:34,120 --> 01:14:35,180
MODERATOR: So--

1591
01:14:35,180 --> 01:14:37,300
why is it called a growth function?

1592
01:14:37,300 --> 01:14:39,820
PROFESSOR: A growth function.

1593
01:14:39,820 --> 01:14:40,410
I really--

1594
01:14:40,410 --> 01:14:40,420


1595
01:14:40,420 --> 01:14:43,270
The person who introduced this
called it a growth function.

1596
01:14:43,270 --> 01:14:47,850
I guess he called it a growth function,
because it grows, as you

1597
01:14:47,850 --> 01:14:52,800
increase N. I don't think there is any
particular merit for the name.

1598
01:14:52,800 --> 01:14:53,975
MODERATOR: Is there--

1599
01:14:53,975 --> 01:14:57,360
what is a real-life situation similar
to the one in the puzzle, where

1600
01:14:57,360 --> 01:15:01,140
you realize that this break point
may be too small?

1601
01:15:01,140 --> 01:15:02,030
PROFESSOR: OK.

1602
01:15:02,030 --> 01:15:05,620
The first order of business is to
get the break point out of the way--

1603
01:15:05,620 --> 01:15:06,570
that there is a break point,

1604
01:15:06,570 --> 01:15:07,750
we are in business.

1605
01:15:07,750 --> 01:15:11,500
Second one is, how does the value
of the break point relate to

1606
01:15:11,500 --> 01:15:12,375
the learning situation?

1607
01:15:12,375 --> 01:15:14,680
Do I need more examples when
I have a bigger break point?

1608
01:15:14,680 --> 01:15:15,760
The answer is yes.

1609
01:15:15,760 --> 01:15:16,700
What is the estimate?

1610
01:15:16,700 --> 01:15:18,820
And there's a theoretical
estimate, a bound.

1611
01:15:18,820 --> 01:15:19,880
Maybe the bound is too loose.

1612
01:15:19,880 --> 01:15:24,110
So we'll have to find practical rules
of thumb that translate the break

1613
01:15:24,110 --> 01:15:25,720
point to a number of examples.

1614
01:15:25,720 --> 01:15:26,990
All of this is coming up.

1615
01:15:26,990 --> 01:15:30,850
So the existence of the break point
means learning is feasible.

1616
01:15:30,850 --> 01:15:34,090
The value of the break point tells us
the resources needed to achieve

1617
01:15:34,090 --> 01:15:35,000
a certain performance.

1618
01:15:35,000 --> 01:15:37,252
And that will be addressed.

1619
01:15:37,252 --> 01:15:42,090
MODERATOR: Is there a probabilistic
statement for the Hoeffding inequality

1620
01:15:42,090 --> 01:15:45,410
that is an alternative to the
case-by-case discussion on M's

1621
01:15:45,410 --> 01:15:48,790
growth rate in N?

1622
01:15:48,790 --> 01:15:49,690


1623
01:15:49,690 --> 01:15:50,560


1624
01:15:50,560 --> 01:15:52,840
PROFESSOR: There are
alternatives to Hoeffding.

1625
01:15:52,840 --> 01:15:52,850


1626
01:15:52,850 --> 01:15:54,830
So there are alternatives
to Hoeffding,

1627
01:15:54,830 --> 01:15:59,960
and you can get different results
or emphasize different things.

1628
01:15:59,960 --> 01:16:01,650
I am sticking to Hoeffding.

1629
01:16:01,650 --> 01:16:05,540
And I'm not indulging too much into its
derivation, or the alternatives,

1630
01:16:05,540 --> 01:16:08,750
because this is a mathematical
tool that I'm borrowing.

1631
01:16:08,750 --> 01:16:10,680
And I'm taking it for granted.

1632
01:16:10,680 --> 01:16:13,740
And I picked the one that will help
us the most, which is this one.

1633
01:16:13,740 --> 01:16:15,630
So yes, there are variations.

1634
01:16:15,630 --> 01:16:19,400
But I am deliberately not getting
into them, in order not

1635
01:16:19,400 --> 01:16:20,750
to dilute the message.

1636
01:16:20,750 --> 01:16:26,030
I want people to become so incredibly
familiar and bored with this one, then

1637
01:16:26,030 --> 01:16:26,730
they know it cold.

1638
01:16:26,730 --> 01:16:30,145
Because when we get to modify it,
including the growth function and the

1639
01:16:30,145 --> 01:16:34,330
other technical points, I'd like the
base point to be completely clear in

1640
01:16:34,330 --> 01:16:36,870
people's mind, so that they don't get
lost with the modifications.

1641
01:16:36,870 --> 01:16:40,345
So that's why I'm sticking to this.

1642
01:16:40,345 --> 01:16:40,776


1643
01:16:40,776 --> 01:16:42,140
MODERATOR: I think that's it.

1644
01:16:42,140 --> 01:16:42,150


1645
01:16:42,150 --> 01:16:42,200


1646
01:16:42,200 --> 01:16:42,800
PROFESSOR: Very good.

1647
01:16:42,800 --> 01:16:44,050
We'll see you next time.

1648
01:16:44,050 --> 01:16:57,184

