1
00:00:00,000 --> 00:00:00,570


2
00:00:00,570 --> 00:00:03,275
ANNOUNCER: The following program
is brought to you by Caltech.

3
00:00:03,275 --> 00:00:15,800


4
00:00:15,800 --> 00:00:18,600
YASER ABU-MOSTAFA: Welcome back.

5
00:00:18,600 --> 00:00:22,290
Last time, we talked about
regularization, which is a very

6
00:00:22,290 --> 00:00:24,990
important technique in
machine learning.

7
00:00:24,990 --> 00:00:30,110
And the main analytic step that we took
is to take a constrained form of

8
00:00:30,110 --> 00:00:35,120
regularization, where you explicitly
forbid some of the hypotheses from

9
00:00:35,120 --> 00:00:39,787
being considered, and thereby reducing
the VC dimension and improving the

10
00:00:39,787 --> 00:00:45,700
generalization property, to
an unconstrained version which creates

11
00:00:45,700 --> 00:00:51,510
an augmented error in which no
particular vector of weights is

12
00:00:51,510 --> 00:00:56,620
prohibited per se, but basically you
have a preference of weights based on

13
00:00:56,620 --> 00:00:59,820
a penalty that has to do
with the constraint.

14
00:00:59,820 --> 00:01:03,700
And that equivalence will make us focus
on the augmented-error form of

15
00:01:03,700 --> 00:01:07,120
regularization, in every
practice we have.

16
00:01:07,120 --> 00:01:13,190
And the argument for it was to take the
constrained version and look at it,

17
00:01:13,190 --> 00:01:18,620
either as a Lagrangian which would be
the formal way of solving it, or as we

18
00:01:18,620 --> 00:01:22,540
did it in a geometric way, to find
a condition that corresponds to

19
00:01:22,540 --> 00:01:26,160
minimization under a constraint, and
find that this would be locally

20
00:01:26,160 --> 00:01:30,910
equivalent to minimizing this
in an unconstrained way.

21
00:01:30,910 --> 00:01:35,780
Then we went to the general form of
a regularizer, and we called it

22
00:01:35,780 --> 00:01:37,980
Omega of h.

23
00:01:37,980 --> 00:01:43,070
And it depends on small h rather than
capital H, which was the other

24
00:01:43,070 --> 00:01:46,210
Omega that we used in the VC analysis.

25
00:01:46,210 --> 00:01:49,500
And in that case, we formed the
augmented error as the in-sample

26
00:01:49,500 --> 00:01:51,700
error, plus this term.

27
00:01:51,700 --> 00:01:55,090
And the idea now is that the
augmented error will be a better thing

28
00:01:55,090 --> 00:01:58,590
to minimize, if you want to minimize the
out-of-sample error, rather than

29
00:01:58,590 --> 00:02:02,390
that-- just minimizing E_in by itself.

30
00:02:02,390 --> 00:02:03,990
And there are two choices here.

31
00:02:03,990 --> 00:02:06,170
One of them is the regularizer

32
00:02:06,170 --> 00:02:09,250
Omega, weight decay or weight
elimination, or the other

33
00:02:09,250 --> 00:02:10,780
forms you may find.

34
00:02:10,780 --> 00:02:13,850
And the other one is lambda, which is
the regularization parameter-- the

35
00:02:13,850 --> 00:02:17,050
amount of regularization
you're going to put.

36
00:02:17,050 --> 00:02:22,960
And the long and short of it is that
the choice of Omega in a practical

37
00:02:22,960 --> 00:02:28,400
situation is really a heuristic choice,
guided by theory and guided by

38
00:02:28,400 --> 00:02:33,650
certain goals, but there is no
mathematical way in a given practical

39
00:02:33,650 --> 00:02:37,580
situation to come up with
a totally principled omega.

40
00:02:37,580 --> 00:02:40,820
But we follow the guidelines,
and we do quite well.

41
00:02:40,820 --> 00:02:46,630
So we make a choice of Omega towards
smoother or simpler hypotheses.

42
00:02:46,630 --> 00:02:51,080
And then we leave the amount of
regularization to the determination of

43
00:02:51,080 --> 00:02:53,620
lambda, and lambda is a little
bit more principled.

44
00:02:53,620 --> 00:02:56,520
We'll find out that we will determine
lambda using validation, which is the

45
00:02:56,520 --> 00:02:58,020
subject of today's lecture.

46
00:02:58,020 --> 00:03:01,020
And when you do that, you will
get some benefit of Omega.

47
00:03:01,020 --> 00:03:03,570
If you choose a great Omega, you
will get a great benefit.

48
00:03:03,570 --> 00:03:06,140
If you choose an OK Omega, you
will get some benefit.

49
00:03:06,140 --> 00:03:08,820
If you choose a terrible Omega, you are
still safe, because lambda will

50
00:03:08,820 --> 00:03:10,900
tell you-- the validation
will tell you-- just

51
00:03:10,900 --> 00:03:14,430
take lambda equal to 0, and
therefore no harm done.

52
00:03:14,430 --> 00:03:18,060
And as you see, the choice of lambda is
indeed critical, because when you

53
00:03:18,060 --> 00:03:21,170
take the correct amount of lambda, which
happens to be very small in this

54
00:03:21,170 --> 00:03:25,170
case, the fit, which is the red curve,
is very close to the target,

55
00:03:25,170 --> 00:03:26,070
which is the blue.

56
00:03:26,070 --> 00:03:29,380
Whereas if you push your luck, and have
more of the regularization,

57
00:03:29,380 --> 00:03:35,080
you end up constraining the fit so much
that the red-- it wants to move

58
00:03:35,080 --> 00:03:38,020
toward the blue, but it can't because
of the penalty, and ends up being

59
00:03:38,020 --> 00:03:41,000
a poor fit for the blue curve.

60
00:03:41,000 --> 00:03:44,470
So that leads us to today's lecture,
which is about validation.

61
00:03:44,470 --> 00:03:47,830
Validation is another technique that
you will be using in almost every

62
00:03:47,830 --> 00:03:50,450
machine learning problem
you will encounter.

63
00:03:50,450 --> 00:03:53,550
And the outline is very simple.

64
00:03:53,550 --> 00:03:55,600
First, I'm going to talk about
the validation set.

65
00:03:55,600 --> 00:03:58,380
There are two aspects that
I'm going to talk about.

66
00:03:58,380 --> 00:04:01,110
The size of the validation
set is critical.

67
00:04:01,110 --> 00:04:04,980
So we'll spend some time looking at
the size of the validation set.

68
00:04:04,980 --> 00:04:07,840
And then we'll ask ourselves, why
did we call it validation

69
00:04:07,840 --> 00:04:08,700
in the first place?

70
00:04:08,700 --> 00:04:12,910
It looks exactly like the test
set that we looked at before.

71
00:04:12,910 --> 00:04:14,310
So why do we call it validation?

72
00:04:14,310 --> 00:04:17,050
And the distinction will
be pretty important.

73
00:04:17,050 --> 00:04:19,950
And then we'll go for model selection,
a very important

74
00:04:19,950 --> 00:04:21,260
subject in machine learning.

75
00:04:21,260 --> 00:04:23,510
And it is the main task of validation.

76
00:04:23,510 --> 00:04:25,410
That's what you use validation for.

77
00:04:25,410 --> 00:04:28,540
And we'll find that model selection
covers more territory than what the

78
00:04:28,540 --> 00:04:31,300
name may suggest to you.

79
00:04:31,300 --> 00:04:33,740
Finally, we will go to cross-validation,
which is a type of

80
00:04:33,740 --> 00:04:36,890
validation that is very interesting,
that allows you, if I give you

81
00:04:36,890 --> 00:04:41,970
a budget of N examples, to basically use
all of them for validation, and all of

82
00:04:41,970 --> 00:04:46,280
them for training, which looks like
cheating, because validation will look

83
00:04:46,280 --> 00:04:48,810
like a distinct activity from
training, as we will see.

84
00:04:48,810 --> 00:04:55,060
But with this trick, you will be able
to find a way to go around that.

85
00:04:55,060 --> 00:05:00,290
Now, let me contrast validation with
regularization, as far as control

86
00:05:00,290 --> 00:05:03,190
of overfitting is concerned.

87
00:05:03,190 --> 00:05:07,840
We have seen, in one form or another,
the following by-now-famous equation,

88
00:05:07,840 --> 00:05:14,670
or inequality or rule, where you have the
out-of-sample error that you want

89
00:05:14,670 --> 00:05:19,050
equal the in-sample error, or at most equal
the in-sample error, plus some penalty.

90
00:05:19,050 --> 00:05:22,970
Could be penalty for model complexity,
overfit complexity, a bunch of other

91
00:05:22,970 --> 00:05:24,910
ways of describing that.

92
00:05:24,910 --> 00:05:28,090
But basically, this tells us that
E_in is not exactly E_out.

93
00:05:28,090 --> 00:05:29,770
That, we know all too well.

94
00:05:29,770 --> 00:05:32,630
And there is a discrepancy, and the
discrepancy has to do with the

95
00:05:32,630 --> 00:05:34,940
complexity of something.

96
00:05:34,940 --> 00:05:37,850
An overfit penalty has to do with the
complexity of the model you are using

97
00:05:37,850 --> 00:05:39,890
to fit, and so on.

98
00:05:39,890 --> 00:05:43,890
So in terms of this equation, I'd like
to pose both regularization and

99
00:05:43,890 --> 00:05:48,760
validation as an activity that
deals with this equation.

100
00:05:48,760 --> 00:05:50,190
So what about regularization?

101
00:05:50,190 --> 00:05:52,030
We put the equation.

102
00:05:52,030 --> 00:05:53,610
What did regularization do?

103
00:05:53,610 --> 00:05:56,810


104
00:05:56,810 --> 00:06:01,590
It tried to estimate this penalty.

105
00:06:01,590 --> 00:06:10,200
Basically, what we did is concoct
a term that we think captures the

106
00:06:10,200 --> 00:06:11,870
overfit penalty.

107
00:06:11,870 --> 00:06:15,670
And then, instead of minimizing the
in-sample, we minimize the in-sample

108
00:06:15,670 --> 00:06:18,150
plus that, and we call that
the augmented error.

109
00:06:18,150 --> 00:06:21,490
And hopefully, the augmented error
will be a better proxy for E_out.

110
00:06:21,490 --> 00:06:23,170
That was the deal.

111
00:06:23,170 --> 00:06:29,130
And we notice that we are very,
very inaccurate in the choice here.

112
00:06:29,130 --> 00:06:33,630
We just say, smooth, pick lambda,
you can use this, you can use that.

113
00:06:33,630 --> 00:06:36,540
So obviously, we are not satisfying
any equality by any chance.

114
00:06:36,540 --> 00:06:40,010
But we are basically getting
a quantity that has a monotonic

115
00:06:40,010 --> 00:06:43,880
property, that when you minimize this,
this gets minimized, which does the

116
00:06:43,880 --> 00:06:46,540
job for us.

117
00:06:46,540 --> 00:06:50,740
Now, to contrast this, let's look at
validation, when it's dealing with the

118
00:06:50,740 --> 00:06:51,810
same equation.

119
00:06:51,810 --> 00:06:54,110
What does validation do?

120
00:06:54,110 --> 00:06:56,355
Well, validation cuts to the chase.

121
00:06:56,355 --> 00:06:59,680


122
00:06:59,680 --> 00:07:01,420
It just estimates the out-of-sample.

123
00:07:01,420 --> 00:07:04,640
Why bother with this analysis, and
overfit, and this and that?

124
00:07:04,640 --> 00:07:05,930
You want to minimize
the out-of-sample?

125
00:07:05,930 --> 00:07:08,650
Let's estimate the out-of-sample,
and minimize it.

126
00:07:08,650 --> 00:07:12,670
Obviously, it's too good to be true,
but it's not totally untrue.

127
00:07:12,670 --> 00:07:15,840
Validation does achieve something
in that direction.

128
00:07:15,840 --> 00:07:20,420
So let me spend a few slides just
describing the estimate.

129
00:07:20,420 --> 00:07:22,710
I'm trying to estimate the
out-of-sample error.

130
00:07:22,710 --> 00:07:26,740
This is not completely a foreign idea
to us, because we use a test set in

131
00:07:26,740 --> 00:07:27,910
order to do that.

132
00:07:27,910 --> 00:07:32,690
So let's focus on this, and see what
are the parameters involved in

133
00:07:32,690 --> 00:07:35,460
estimating the out-of-sample error.

134
00:07:35,460 --> 00:07:38,110
Let's look at the estimate.

135
00:07:38,110 --> 00:07:42,310
The starting point is to take
an out-of-sample point x, y.

136
00:07:42,310 --> 00:07:46,360
This is a point that was
not involved in training.

137
00:07:46,360 --> 00:07:47,840
We used to call it test point.

138
00:07:47,840 --> 00:07:50,000
Now we are going to call
it validation point.

139
00:07:50,000 --> 00:07:52,770
It's not going to become clear why we
are giving it a different name for

140
00:07:52,770 --> 00:07:56,170
a while, until we use the validation
set for something, and then the

141
00:07:56,170 --> 00:07:57,590
distinction will become clear.

142
00:07:57,590 --> 00:08:00,380
But as far as you are concerned now,
this is just a test point.

143
00:08:00,380 --> 00:08:04,560
We are estimating E_out, and we will
just read the value of E_out and be

144
00:08:04,560 --> 00:08:07,090
happy with that, and not
do anything further.

145
00:08:07,090 --> 00:08:13,090
So you take this point, and the error
on it is the difference between what

146
00:08:13,090 --> 00:08:18,020
your hypothesis does on x, and what
the target value is, which is y.

147
00:08:18,020 --> 00:08:19,330
And what is the error?

148
00:08:19,330 --> 00:08:20,940
We have seen many forms of the error.

149
00:08:20,940 --> 00:08:23,000
Let's just mention two
to make it concrete.

150
00:08:23,000 --> 00:08:24,655
This could be a simple squared error.

151
00:08:24,655 --> 00:08:26,490
We have seen that in
linear regression.

152
00:08:26,490 --> 00:08:27,630
It could be the binary error.

153
00:08:27,630 --> 00:08:28,880
We have seen that in classification.

154
00:08:28,880 --> 00:08:31,870
So nothing foreign here.

155
00:08:31,870 --> 00:08:35,958
Now, if you take this quantity,
and we are now treating it as

156
00:08:35,958 --> 00:08:37,780
an estimate for E_out,

157
00:08:37,780 --> 00:08:40,620
a poor estimate, but nonetheless
an estimate.

158
00:08:40,620 --> 00:08:44,250
We call it an estimate because, if you
take the expected value of that with

159
00:08:44,250 --> 00:08:47,750
respect to the choice of x, with the
probability distribution over the

160
00:08:47,750 --> 00:08:52,640
input space that generates x,
what will that value be?

161
00:08:52,640 --> 00:08:55,190
Well, that is simply E_out.

162
00:08:55,190 --> 00:09:00,410
So indeed, this quantity, the random
variable here, has the correct

163
00:09:00,410 --> 00:09:01,160
expected value.

164
00:09:01,160 --> 00:09:03,850
It's an unbiased estimate or E_out.

165
00:09:03,850 --> 00:09:08,110
But unbiased means that it's as likely
to be here or here, in terms of

166
00:09:08,110 --> 00:09:09,410
expected value.

167
00:09:09,410 --> 00:09:12,960
But we could be this, and this would
be a good estimate, or we could be

168
00:09:12,960 --> 00:09:14,440
this, and this would be
a terrible estimate.

169
00:09:14,440 --> 00:09:15,580
Because you are not getting
all of them.

170
00:09:15,580 --> 00:09:17,150
You are just getting one of them.

171
00:09:17,150 --> 00:09:21,030
So if this guy swings very large, and
I tell you this is an estimate of

172
00:09:21,030 --> 00:09:24,990
E_out, and you get it here, this is
what you will think E_out is.

173
00:09:24,990 --> 00:09:27,590
So there is an error, but
the error is not biased.

174
00:09:27,590 --> 00:09:30,300
That's what this equation says.

175
00:09:30,300 --> 00:09:33,770
But we have to evaluate that swing, and
the swing is obviously evaluated

176
00:09:33,770 --> 00:09:36,420
by the usual quantity, the variance.

177
00:09:36,420 --> 00:09:38,980
And let's just call the variance
sigma squared.

178
00:09:38,980 --> 00:09:41,590
It depends on a number of things,
including what is your error measure

179
00:09:41,590 --> 00:09:44,940
and whatnot, but that is what
a single point does.

180
00:09:44,940 --> 00:09:47,530
So you get an estimate, but
the estimate is poor because it's one

181
00:09:47,530 --> 00:09:50,150
point, and therefore sigma squared
is likely to be large.

182
00:09:50,150 --> 00:09:54,410
So you are unlikely to use the estimate
on one point as your guide to

183
00:09:54,410 --> 00:09:55,660
E_out.

184
00:09:55,660 --> 00:09:57,120
What do you use?

185
00:09:57,120 --> 00:10:00,390
You move from one point, to a full set.

186
00:10:00,390 --> 00:10:01,380
So you get what?

187
00:10:01,380 --> 00:10:05,820
You get a validation set that you are
going to use to estimate E_out.

188
00:10:05,820 --> 00:10:08,940
Now, the notation we are going to have
is that the number of points in the

189
00:10:08,940 --> 00:10:12,700
validation set is K. Remember that
the number of points in the

190
00:10:12,700 --> 00:10:15,700
training set was N.

191
00:10:15,700 --> 00:10:19,620
So this will be K points, also generated
according to the same rules--

192
00:10:19,620 --> 00:10:21,800
independently, according to the
probability distribution

193
00:10:21,800 --> 00:10:24,030
over the input space.

194
00:10:24,030 --> 00:10:30,495
And the error on that set we are
going to call it E_val, as

195
00:10:30,495 --> 00:10:31,540
in validation error.

196
00:10:31,540 --> 00:10:34,860
So we have E_in, and we have E_out.

197
00:10:34,860 --> 00:10:38,740
Now we are introducing another one,
E_val, the validation error.

198
00:10:38,740 --> 00:10:41,020
And the form for it is what
you expect it to be.

199
00:10:41,020 --> 00:10:44,300
You take the individual errors on the
examples, and you take the average,

200
00:10:44,300 --> 00:10:47,930
like you did with the training set, and
this one is the validation error.

201
00:10:47,930 --> 00:10:51,150
The only difference is that this
is done out-of-sample.

202
00:10:51,150 --> 00:10:54,330
These guys were not used in training,
and therefore you would expect that

203
00:10:54,330 --> 00:10:57,590
this would be a good estimate for
the out-of-sample performance.

204
00:10:57,590 --> 00:10:59,870
Let's see if it is.

205
00:10:59,870 --> 00:11:04,020
What is the expected value of
E_val, the validation error?

206
00:11:04,020 --> 00:11:07,260
Well, you take the expected
value of this fellow.

207
00:11:07,260 --> 00:11:08,960
The expectation goes inside.

208
00:11:08,960 --> 00:11:12,550
So the main component is the expected
value of this fellow, which we have

209
00:11:12,550 --> 00:11:14,520
seen before-- expected value
on a single point.

210
00:11:14,520 --> 00:11:18,420
And you just average linearly,
as you did.

211
00:11:18,420 --> 00:11:22,560
Now, this quantity happens
to be E_out.

212
00:11:22,560 --> 00:11:25,480
The expected value on
one point is E_out.

213
00:11:25,480 --> 00:11:27,700
Therefore, when you do that,
you just get E_out again.

214
00:11:27,700 --> 00:11:29,200


215
00:11:29,200 --> 00:11:35,560
So indeed, again, the validation error is
an unbiased estimate of the out-of-sample

216
00:11:35,560 --> 00:11:39,820
error, provided that all you did with
the validation set is just measure the

217
00:11:39,820 --> 00:11:40,610
out-of-sample error.

218
00:11:40,610 --> 00:11:43,040
You didn't use it in any way.

219
00:11:43,040 --> 00:11:46,720
Now, let's look at the variance, because
that was our problem with the

220
00:11:46,720 --> 00:11:50,290
single-point estimate, and let's
see if there's an improvement.

221
00:11:50,290 --> 00:11:52,870
When you get the variance, you are
going to take this formula.

222
00:11:52,870 --> 00:11:56,760
And then you are going to have a double
summation, and have all cross

223
00:11:56,760 --> 00:12:00,140
terms of e between different points.

224
00:12:00,140 --> 00:12:04,920
So you will have the covariance between
the value for k equals 1 and k

225
00:12:04,920 --> 00:12:06,990
equals 2, k equals 1 and
k equals 3, et cetera.

226
00:12:06,990 --> 00:12:10,050
And you also have that diagonal guys,
which is the variance in this case,

227
00:12:10,050 --> 00:12:15,150
with k equals 1 and 
k equals 1 again.

228
00:12:15,150 --> 00:12:20,850
The main component you are going to
get are the variance, and a bunch of

229
00:12:20,850 --> 00:12:21,400
covariances.

230
00:12:21,400 --> 00:12:23,700
Actually, there are more covariances
than variances, because the variances

231
00:12:23,700 --> 00:12:26,140
are the diagonal, the covariances
are the off-diagonal.

232
00:12:26,140 --> 00:12:28,160
There are almost K squared of them.

233
00:12:28,160 --> 00:12:31,060
The good thing about the covariance
in this case is that it will be 0,

234
00:12:31,060 --> 00:12:33,840
because we picked the points
independently.

235
00:12:33,840 --> 00:12:36,390
And therefore, the covariance between
a quantity that depends on these

236
00:12:36,390 --> 00:12:37,920
points will be 0.

237
00:12:37,920 --> 00:12:41,530
So I'm only stuck with the diagonal
elements, which happen

238
00:12:41,530 --> 00:12:42,280
to have this form.

239
00:12:42,280 --> 00:12:43,670
I have the variance here.

240
00:12:43,670 --> 00:12:46,390
And when I put the summation,
something interesting happens.

241
00:12:46,390 --> 00:12:48,900
I have the summation again,
a double summation reduced to one,

242
00:12:48,900 --> 00:12:50,930
because I'm only summing the diagonal.

243
00:12:50,930 --> 00:12:54,270
But I still have the normalizing factor
with the number of elements.

244
00:12:54,270 --> 00:12:57,800
Because I had K squared elements, the
fact that many of them dropped out is

245
00:12:57,800 --> 00:12:59,310
just to my advantage.

246
00:12:59,310 --> 00:13:03,810
I still have the 1 over K squared, and
that gives me the better variance for

247
00:13:03,810 --> 00:13:07,180
the estimate based on E_val,
than on a single point.

248
00:13:07,180 --> 00:13:12,450
This is your typical analysis
of adding a bunch

249
00:13:12,450 --> 00:13:13,740
of independent estimates.

250
00:13:13,740 --> 00:13:15,240
So you get the sigma squared.

251
00:13:15,240 --> 00:13:17,240
That was the variance on
a particular point.

252
00:13:17,240 --> 00:13:22,440
But now you divide it by K. Now we see
a hope, because even if the original

253
00:13:22,440 --> 00:13:25,730
estimate was this way, maybe we can
have K big enough that we keep

254
00:13:25,730 --> 00:13:29,640
shrinking the error bar, such that the
E_val itself as a random variable

255
00:13:29,640 --> 00:13:33,090
becomes this, which is around
E_out-- what we want.

256
00:13:33,090 --> 00:13:35,820
And therefore, it becomes
a reliable estimate.

257
00:13:35,820 --> 00:13:38,240
This looks promising.

258
00:13:38,240 --> 00:13:43,720
Now we can write the E_val, which
is a random variable, to be E_out,

259
00:13:43,720 --> 00:13:48,440
which is the value we want, plus or
minus something that averages to 0, and

260
00:13:48,440 --> 00:13:52,700
happens to be the order of approximately
1 over square root of K.

261
00:13:52,700 --> 00:13:55,410
If the variance is 1 over K, then
the standard deviation is 1 over

262
00:13:55,410 --> 00:13:56,390
square root of K.

263
00:13:56,390 --> 00:13:59,300
I'm assuming here that sigma
squared is constant in the

264
00:13:59,300 --> 00:14:01,210
range that I'm using.

265
00:14:01,210 --> 00:14:04,440
And therefore, the dependency
on K only comes from here.

266
00:14:04,440 --> 00:14:07,790
Therefore, I have this quantity
that tells me this is what I'm

267
00:14:07,790 --> 00:14:11,380
estimating, and this is the error I'm
committing, and this is how the error

268
00:14:11,380 --> 00:14:15,550
is behaving as I increase
the number K.

269
00:14:15,550 --> 00:14:19,100
The interesting point now
is that K is not free.

270
00:14:19,100 --> 00:14:22,670
It's not like I tell you, it
looks like if I increase K,

271
00:14:22,670 --> 00:14:23,900
this is a good situation.

272
00:14:23,900 --> 00:14:26,770
So why don't we use more and
more validation points?

273
00:14:26,770 --> 00:14:31,390
Because the reality is, K not given to
you on top of your training set.

274
00:14:31,390 --> 00:14:36,910
What I give you is a data set, N points,
and it's up to you to use how

275
00:14:36,910 --> 00:14:39,250
many to train, and how
many to validate.

276
00:14:39,250 --> 00:14:43,200
I'm not going to give you more, just
because you want to validate.

277
00:14:43,200 --> 00:14:46,580
So every time you take a point for
validation, you are taking it away

278
00:14:46,580 --> 00:14:48,400
from training, so to speak.

279
00:14:48,400 --> 00:14:53,120
Let's see the ramifications
of this regime.

280
00:14:53,120 --> 00:14:55,590
K is taken out of N. So let's
now have the notation.

281
00:14:55,590 --> 00:15:00,560
We are given a data set D, as
we always called it, and it has

282
00:15:00,560 --> 00:15:02,540
N points.

283
00:15:02,540 --> 00:15:06,430
What do we do with it?

284
00:15:06,430 --> 00:15:09,720
We are going to take K points,
and use them for validation.

285
00:15:09,720 --> 00:15:13,600
And you can take any K points, as long
as you don't look at the particular

286
00:15:13,600 --> 00:15:14,600
input and output.

287
00:15:14,600 --> 00:15:17,870
Let's say you pick K points at
random, from the N points.

288
00:15:17,870 --> 00:15:22,900
That will be a valid set
of validation for you.

289
00:15:22,900 --> 00:15:31,020
So I have the K points, and therefore
I'm left with N minus K for training.

290
00:15:31,020 --> 00:15:35,860
The ones I left for training,
I'm going

291
00:15:35,860 --> 00:15:38,140
to call D_train.

292
00:15:38,140 --> 00:15:40,560
I didn't have to use that when I
didn't have validation, because

293
00:15:40,560 --> 00:15:44,660
D all went to training,
so I didn't need to have the

294
00:15:44,660 --> 00:15:45,470
distinction.

295
00:15:45,470 --> 00:15:48,260
Now, because I have two utilities, I'm
going to take the guys that go into

296
00:15:48,260 --> 00:15:51,970
training and call that subset
D_train.

297
00:15:51,970 --> 00:15:54,160
And the guys that I hold
for validation I'm going to

298
00:15:54,160 --> 00:15:56,680
call it D_val.

299
00:15:56,680 --> 00:16:01,900
The union of them is D.
That's the setup.

300
00:16:01,900 --> 00:16:07,320
Now, we looked in the previous slide
at the reliability of the estimate of

301
00:16:07,320 --> 00:16:08,320
the validation set.

302
00:16:08,320 --> 00:16:11,810
And we found that this reliability, if we
measure it by the error bar on the

303
00:16:11,810 --> 00:16:17,020
fluctuation, it will be the order of
1 over square root of K, the

304
00:16:17,020 --> 00:16:19,070
number of validation points.

305
00:16:19,070 --> 00:16:24,950
Then our conclusion is that if you use
small K, you have a bad estimate.

306
00:16:24,950 --> 00:16:28,180
And the whole role we have for
validation so far is estimate, so we

307
00:16:28,180 --> 00:16:29,780
are not doing a good job.

308
00:16:29,780 --> 00:16:31,540
So we need to increase K.

309
00:16:31,540 --> 00:16:35,115
It looks like a good idea, just from
that point of view, to take

310
00:16:35,115 --> 00:16:36,960
large K.

311
00:16:36,960 --> 00:16:40,790
But there are ramifications for taking
large K, so we have a question mark.

312
00:16:40,790 --> 00:16:44,300
And let's try to be
quantitative about it.

313
00:16:44,300 --> 00:16:46,790
Remember this fellow?

314
00:16:46,790 --> 00:16:48,280
That was the learning curve.

315
00:16:48,280 --> 00:16:49,980
What did it do?

316
00:16:49,980 --> 00:16:55,660
It told us, as you increase the number
of training points, what is the

317
00:16:55,660 --> 00:16:59,890
expected value of E_out and what is the
expected value of E_in, for a given

318
00:16:59,890 --> 00:17:05,270
model, the model that I'm plotting
the learning curves of. Right?

319
00:17:05,270 --> 00:17:11,530
Now, the number of data points used to
be N. Here I'm writing it as N minus

320
00:17:11,530 --> 00:17:13,180
K. Why am I doing that?

321
00:17:13,180 --> 00:17:15,560
Because under the regime of
validation, this is what

322
00:17:15,560 --> 00:17:17,560
I'm using for training.

323
00:17:17,560 --> 00:17:23,490
Therefore if you increase K, you are
moving in this direction, right?

324
00:17:23,490 --> 00:17:28,220
I used to be here, and I used
to expect that level of E_out.

325
00:17:28,220 --> 00:17:31,720
Now I am here, and I'm expecting
that level of E_out.

326
00:17:31,720 --> 00:17:34,480
That doesn't look very promising.

327
00:17:34,480 --> 00:17:38,900
I may get a reliable estimate, because
I'm using bigger K, but I'm getting

328
00:17:38,900 --> 00:17:42,750
a reliable estimate of a worse quantity.

329
00:17:42,750 --> 00:17:46,980
If you want to take an extreme case, you
are going to take this estimate and

330
00:17:46,980 --> 00:17:50,990
go to your customer, and tell them what
you expect for the performance to be.

331
00:17:50,990 --> 00:17:53,950
So you don't only deliver
the final hypothesis.

332
00:17:53,950 --> 00:17:58,390
You deliver the final hypothesis, with
an estimate for how it will do when

333
00:17:58,390 --> 00:18:02,170
they test it on a point that
you haven't seen before.

334
00:18:02,170 --> 00:18:04,860
Now, we want the estimate to be very
reliable, and you forget about the

335
00:18:04,860 --> 00:18:06,420
quality of the hypothesis.

336
00:18:06,420 --> 00:18:10,260
So you keep increasing K, keep
increasing K, keep increasing K.

337
00:18:10,260 --> 00:18:13,920
You end up with a very, very
reliable estimate.

338
00:18:13,920 --> 00:18:17,830
The problem is that it's an estimate of
a very, very poor quantity, because

339
00:18:17,830 --> 00:18:22,150
you used 2 examples to train, and
you are basically in the noise.

340
00:18:22,150 --> 00:18:27,200
So the statement you are going to make
to your customer in this case is that,

341
00:18:27,200 --> 00:18:29,600
here is a system.

342
00:18:29,600 --> 00:18:34,480
I am very sure that it's terrible!

343
00:18:34,480 --> 00:18:38,250
That is unlikely to please a customer.

344
00:18:38,250 --> 00:18:42,850
So now, we realize that there is a price
to be paid for K. It turns out

345
00:18:42,850 --> 00:18:47,970
that we are going to have a trick that
will make us not pay that price.

346
00:18:47,970 --> 00:18:52,130
But still, the question of what happens
when K is big is a question

347
00:18:52,130 --> 00:18:53,450
mark in our mind.

348
00:18:53,450 --> 00:18:57,310
What I'm going to do now, I'm going
to tell you, you used K to

349
00:18:57,310 --> 00:18:59,420
estimate the error.

350
00:18:59,420 --> 00:19:04,460
Now, what I'm going to tell you, why
don't you restore the data set after

351
00:19:04,460 --> 00:19:07,970
you have the estimate? Because the
estimate now is in your pocket, train

352
00:19:07,970 --> 00:19:12,410
on the full set, so that you
get the better guy.

353
00:19:12,410 --> 00:19:14,580
Well, I estimated for the smaller guy.

354
00:19:14,580 --> 00:19:15,610
What are we doing here?

355
00:19:15,610 --> 00:19:18,330
Let's just do this systematically.

356
00:19:18,330 --> 00:19:21,680
Let's put K back into the pot.

357
00:19:21,680 --> 00:19:23,250
So here is the regime.

358
00:19:23,250 --> 00:19:25,860


359
00:19:25,860 --> 00:19:30,860
I'm going to describe this figure,
but let's talk it piece by piece.

360
00:19:30,860 --> 00:19:33,710
We have the data set D, right?

361
00:19:33,710 --> 00:19:40,450
We separated it into
D_train and D_val.

362
00:19:40,450 --> 00:19:43,000
D itself has N points.

363
00:19:43,000 --> 00:19:47,460
We took N minus K to train,
K to validate.

364
00:19:47,460 --> 00:19:49,680
That's the game.

365
00:19:49,680 --> 00:19:51,790
What happened?

366
00:19:51,790 --> 00:19:57,030
If we used the full training set to
train, we would get a final hypothesis

367
00:19:57,030 --> 00:19:59,020
that we called g.

368
00:19:59,020 --> 00:20:01,930
This is just a matter of notation.

369
00:20:01,930 --> 00:20:05,750
But under the regime of validation,
you took out some guys.

370
00:20:05,750 --> 00:20:10,630
And therefore, you are using
only D_train to train.

371
00:20:10,630 --> 00:20:13,430
And this has N minus K. Doesn't
have all the examples.

372
00:20:13,430 --> 00:20:18,600
Therefore, I am going to generically
label the final hypothesis that I get

373
00:20:18,600 --> 00:20:23,200
from training on a reduced set, D_train,
I am going to call it g minus.

374
00:20:23,200 --> 00:20:28,090
Just to remind ourselves that it's
not on the full training set.

375
00:20:28,090 --> 00:20:31,780
So now, here is the idea, if
you look at the figure.

376
00:20:31,780 --> 00:20:34,860


377
00:20:34,860 --> 00:20:41,680
I have the D. Let me get it a bit
smaller so that we can get the output.

378
00:20:41,680 --> 00:20:44,710


379
00:20:44,710 --> 00:20:49,530
If I use the training set by
itself, I would get g.

380
00:20:49,530 --> 00:20:54,530
What I am doing now is that I am going
to take D_train, which has fewer

381
00:20:54,530 --> 00:20:58,150
examples, and the rest
go to validation.

382
00:20:58,150 --> 00:21:04,300
I use D_train to get g minus, and then
I take g minus and evaluate it on

383
00:21:04,300 --> 00:21:08,410
D_val, the validation set, in
order to get an estimate.

384
00:21:08,410 --> 00:21:11,850
So the trick now is that instead of
reporting g minus as my final

385
00:21:11,850 --> 00:21:17,675
hypothesis, I know if I added the other
data points here to the pot, I

386
00:21:17,675 --> 00:21:19,630
am going to get a better
out-of-sample.

387
00:21:19,630 --> 00:21:20,700
I don't know what it is.

388
00:21:20,700 --> 00:21:22,130
I don't have an estimate for it.

389
00:21:22,130 --> 00:21:25,070
But I know it's going to be better
than the one for g minus, simply

390
00:21:25,070 --> 00:21:26,090
because of the learning curve.

391
00:21:26,090 --> 00:21:30,510
On average, I get more examples, I
get better out-of-sample error.

392
00:21:30,510 --> 00:21:32,850
So I put it back and then report g.

393
00:21:32,850 --> 00:21:33,830
So it's a funny situation.

394
00:21:33,830 --> 00:21:37,700
I'm giving you g, and I'm giving you
the validation estimate on g minus.

395
00:21:37,700 --> 00:21:38,640
Why?

396
00:21:38,640 --> 00:21:39,930
Because that's the only
estimate I have.

397
00:21:39,930 --> 00:21:43,090
I cannot give you the estimate on g,
because now if I get g, I don't have

398
00:21:43,090 --> 00:21:45,230
any guys to validate on.

399
00:21:45,230 --> 00:21:47,970
So you can see now the compromise.

400
00:21:47,970 --> 00:21:52,410
Under this scenario, I'm not really
losing in performance by taking

401
00:21:52,410 --> 00:21:55,840
a bigger validation set, because I'm going
to put them back when I get

402
00:21:55,840 --> 00:21:56,870
the final hypothesis.

403
00:21:56,870 --> 00:22:01,570
What I am losing here is that, the
validation error I'm reporting, is

404
00:22:01,570 --> 00:22:03,900
a validation error on a different
hypothesis than the

405
00:22:03,900 --> 00:22:05,650
one I am giving you.

406
00:22:05,650 --> 00:22:09,700
And if the difference is big, then
my estimate is bad, because I'm

407
00:22:09,700 --> 00:22:12,380
estimating on something other
than what I am giving you.

408
00:22:12,380 --> 00:22:15,500
And that's what happens
when you have large K.

409
00:22:15,500 --> 00:22:19,840
When you have large K, the discrepancy
between g minus and g is bigger.

410
00:22:19,840 --> 00:22:22,020
And I am giving you the
estimate on g minus.

411
00:22:22,020 --> 00:22:24,090
So that estimate is poor.

412
00:22:24,090 --> 00:22:27,490
And therefore, I get
a bad estimate again.

413
00:22:27,490 --> 00:22:29,770
Now, you see the subtlety here.

414
00:22:29,770 --> 00:22:32,680
This is the regime that is used
in validation, universally.

415
00:22:32,680 --> 00:22:36,330
After you do your thing, and you do your
estimates, and, as you will see

416
00:22:36,330 --> 00:22:40,450
further, you do your choices, you go and
put all the examples to train on,

417
00:22:40,450 --> 00:22:43,910
because this is your best bet of
getting a good hypothesis.

418
00:22:43,910 --> 00:22:48,260
If your K is small, the validation
error is not reliable.

419
00:22:48,260 --> 00:22:52,770
It's a bad estimate, just because
the variance of it is big.

420
00:22:52,770 --> 00:22:56,260
I have small K, it's 1 over square
root of K, so I'm doing this.

421
00:22:56,260 --> 00:22:59,990
If you get big K, the problem is not
the reliability of the estimate.

422
00:22:59,990 --> 00:23:03,260
The problem is that the thing you are
estimating is getting further and

423
00:23:03,260 --> 00:23:06,450
further away from the thing
you are reporting.

424
00:23:06,450 --> 00:23:07,910
So now we have a compromise.

425
00:23:07,910 --> 00:23:11,100
We don't want K to be too small, in
order not to have fluctuations.

426
00:23:11,100 --> 00:23:14,740
We don't want K to be too big, in order
not to be too far from

427
00:23:14,740 --> 00:23:16,460
what we are reporting.

428
00:23:16,460 --> 00:23:21,240
And as usual in machine learning,
there is a rule of thumb.

429
00:23:21,240 --> 00:23:22,970
And the rule of thumb
is pretty simple.

430
00:23:22,970 --> 00:23:24,640
That's why it's a rule of thumb.

431
00:23:24,640 --> 00:23:29,580
It says, take one fifth
for validation.

432
00:23:29,580 --> 00:23:32,600
That usually gives you the
best of both worlds.

433
00:23:32,600 --> 00:23:33,880
Nothing proved.

434
00:23:33,880 --> 00:23:35,350
You can find counterexamples.

435
00:23:35,350 --> 00:23:36,430
I'm not going to argue with that.

436
00:23:36,430 --> 00:23:37,870
It's a rule of thumb.

437
00:23:37,870 --> 00:23:42,970
Use it in practice, and actually you
will be quite successful here.

438
00:23:42,970 --> 00:23:44,890
There's an argument with some
people, whether it should be N

439
00:23:44,890 --> 00:23:46,450
over 5 or N over 6.

440
00:23:46,450 --> 00:23:48,000
I'm going not going
to fret over that.

441
00:23:48,000 --> 00:23:49,670
It's a rule of thumb, after
all, for crying out loud!

442
00:23:49,670 --> 00:23:50,920
We'll just leave it at that.

443
00:23:50,920 --> 00:23:53,520


444
00:23:53,520 --> 00:23:55,560
So we now have that.

445
00:23:55,560 --> 00:23:57,160
Let's go to the other aspect.

446
00:23:57,160 --> 00:24:01,060
We know what validation is, and we
understand how critical it is to

447
00:24:01,060 --> 00:24:02,810
choose the number, and we
have a rule of thumb.

448
00:24:02,810 --> 00:24:06,310
Now let's ask the question, why
are we calling this validation

449
00:24:06,310 --> 00:24:07,470
in the first place?

450
00:24:07,470 --> 00:24:09,570
So far, it's purely a test.

451
00:24:09,570 --> 00:24:11,010
We get an out-of-sample point.

452
00:24:11,010 --> 00:24:11,870
The estimate is unbiased.

453
00:24:11,870 --> 00:24:13,280
What is the deal?

454
00:24:13,280 --> 00:24:16,630
We call it validation, because
we use it to make choices.

455
00:24:16,630 --> 00:24:21,080
And this is a very important point,
so let's talk about it in detail.

456
00:24:21,080 --> 00:24:29,270
Once I make my estimate affect the
learning process, the set I am using

457
00:24:29,270 --> 00:24:30,920
is going to change nature.

458
00:24:30,920 --> 00:24:33,680
So let's look at a situation
that we have seen before.

459
00:24:33,680 --> 00:24:35,200
Remember this fellow?

460
00:24:35,200 --> 00:24:38,730
Yeah, this was early stopping
in neural networks.

461
00:24:38,730 --> 00:24:43,300
And let me magnify it for you
to see the green curve.

462
00:24:43,300 --> 00:24:45,680
Do you see the green curve now?

463
00:24:45,680 --> 00:24:46,820
OK, so there is a green curve.

464
00:24:46,820 --> 00:24:48,070
Now we'll scoot it back.

465
00:24:48,070 --> 00:24:50,490


466
00:24:50,490 --> 00:24:52,690
So the in-sample goes down.

467
00:24:52,690 --> 00:24:53,590
Out-of-sample--

468
00:24:53,590 --> 00:24:56,780
let's say that I have a general estimate
for the out-of-sample--

469
00:24:56,780 --> 00:25:00,010
goes down with it until such a point
that it goes up, and we have the

470
00:25:00,010 --> 00:25:01,430
overfitting, and we talked about it.

471
00:25:01,430 --> 00:25:04,800
And in this case, it's a good
idea to have early stopping.

472
00:25:04,800 --> 00:25:08,660
Now, let's say that you are using K
points, that you did not use for

473
00:25:08,660 --> 00:25:12,240
training, in order to estimate E_out.

474
00:25:12,240 --> 00:25:17,010
That would be E_test, the test error, if
all you are doing is just plotting

475
00:25:17,010 --> 00:25:19,560
the red in order to look
at it and admire it.

476
00:25:19,560 --> 00:25:20,650
Oh, that's a nice curve.

477
00:25:20,650 --> 00:25:21,580
Oh, it's going up.

478
00:25:21,580 --> 00:25:25,220
But you're not going to take
any actions based on it.

479
00:25:25,220 --> 00:25:29,510
Now, if you decide that,
this is going up,

480
00:25:29,510 --> 00:25:32,940
I had better stop here.

481
00:25:32,940 --> 00:25:35,710
That changes the game dramatically.

482
00:25:35,710 --> 00:25:38,620
All of a sudden, this is
no longer a test error.

483
00:25:38,620 --> 00:25:40,970
Now it's a validation error.

484
00:25:40,970 --> 00:25:43,040
So you ask yourself, what the heck?

485
00:25:43,040 --> 00:25:44,930
It's just semantics.

486
00:25:44,930 --> 00:25:46,790
It's the same curve.

487
00:25:46,790 --> 00:25:48,910
Why am I calling it a different name?

488
00:25:48,910 --> 00:25:52,660
I'm calling it a different name, because
it used to be unbiased.

489
00:25:52,660 --> 00:25:57,200
That is, if this is an estimate of E_out,
not the actual E_out, there will be

490
00:25:57,200 --> 00:25:59,440
an error bar in estimating E_out.

491
00:25:59,440 --> 00:26:04,460
But it is as likely to be optimistic,
as pessimistic.

492
00:26:04,460 --> 00:26:07,920
Now, when you do early stopping, you
say, I'm going to stop here and I'm

493
00:26:07,920 --> 00:26:12,720
going to use this value as my estimate
for what you are getting.

494
00:26:12,720 --> 00:26:17,260
I claim that your
estimate is now biased.

495
00:26:17,260 --> 00:26:18,510
It's the same point.

496
00:26:18,510 --> 00:26:20,870
You told us it was unbiased before.

497
00:26:20,870 --> 00:26:22,230
What is the deal?

498
00:26:22,230 --> 00:26:25,250
Let's look at a very specific
simple case, in order to

499
00:26:25,250 --> 00:26:26,500
understand what happens.

500
00:26:26,500 --> 00:26:29,750


501
00:26:29,750 --> 00:26:32,060
This is no longer a test set.

502
00:26:32,060 --> 00:26:35,940
It becomes, in red, a validation set.

503
00:26:35,940 --> 00:26:36,920
Fine, fine.

504
00:26:36,920 --> 00:26:38,620
Now convince us of the
substance of it.

505
00:26:38,620 --> 00:26:40,580
We know the name.

506
00:26:40,580 --> 00:26:44,290
So let's look at the difference, when
you actually make a choice.

507
00:26:44,290 --> 00:26:47,020
Very simple thing
that you can reason.

508
00:26:47,020 --> 00:26:50,540
Let's say I have the test set, which is
unbiased, and I'm claiming that the

509
00:26:50,540 --> 00:26:52,450
validation set has an optimistic bias.

510
00:26:52,450 --> 00:26:54,970


511
00:26:54,970 --> 00:26:56,080
Optimism is good.

512
00:26:56,080 --> 00:26:58,530
But here, it's optimism followed
by disappointment.

513
00:26:58,530 --> 00:27:00,000
It's deception.

514
00:27:00,000 --> 00:27:02,530
We are just calling it optimistic, to
understand that it's always in the

515
00:27:02,530 --> 00:27:05,360
direction of thinking that the error
will be smaller than it will, actually,

516
00:27:05,360 --> 00:27:06,610
turn out to be.

517
00:27:06,610 --> 00:27:08,550


518
00:27:08,550 --> 00:27:11,040
So let's say we have two hypotheses.

519
00:27:11,040 --> 00:27:15,620
And for simplicity, let's have them
both have the same E_out.

520
00:27:15,620 --> 00:27:16,480
So I have two hypotheses.

521
00:27:16,480 --> 00:27:20,210
Each of them has out-of-sample
error 0.5.

522
00:27:20,210 --> 00:27:24,420
Now, I'm using a point to
estimate that error.

523
00:27:24,420 --> 00:27:26,140
And I have two estimates,

524
00:27:26,140 --> 00:27:31,720
e_1 for the hypothesis 1, and
e_2 for the hypothesis 2.

525
00:27:31,720 --> 00:27:34,900
I'm going to use-- because the
estimate has fluctuations in it, just

526
00:27:34,900 --> 00:27:39,460
again for simplicity, I'm going to
assume that both e_1 and e_2 are uniform

527
00:27:39,460 --> 00:27:41,270
between 0 and 1.

528
00:27:41,270 --> 00:27:44,480
So indeed, the expected value is half,
which is the expected value I want,

529
00:27:44,480 --> 00:27:46,860
which is the out-of-sample error.

530
00:27:46,860 --> 00:27:50,250
Now, I'm not going to assume strictly
that e_1 and e_2 are independent, but

531
00:27:50,250 --> 00:27:53,170
you can assume they are independent
for the sake of argument.

532
00:27:53,170 --> 00:27:55,740
But they can have some level of
correlation, and you'll still get the

533
00:27:55,740 --> 00:27:56,400
same effect.

534
00:27:56,400 --> 00:28:01,480
Let's think now that they are
independent variables, e_1 and e_2.

535
00:28:01,480 --> 00:28:05,700
Now, e_1 is an unbiased estimate of
its out-of-sample error, right?

536
00:28:05,700 --> 00:28:06,760
Right.

537
00:28:06,760 --> 00:28:08,600
e_2 is the same, right?

538
00:28:08,600 --> 00:28:09,380
Right.

539
00:28:09,380 --> 00:28:13,130
Unbiased means the expected value
is what it should be.

540
00:28:13,130 --> 00:28:15,080
And the expected value,
indeed in this case,

541
00:28:15,080 --> 00:28:17,390
is what it should be, 0.5.

542
00:28:17,390 --> 00:28:21,170
Now, let's take the game, where we
pick one of the hypotheses,

543
00:28:21,170 --> 00:28:23,320
either h_1 or h_2.

544
00:28:23,320 --> 00:28:24,990
How are we going to pick it?

545
00:28:24,990 --> 00:28:29,090
We are going to pick it according
to the value of the error.

546
00:28:29,090 --> 00:28:35,180
So now, the measurement we have
is applying to that choice.

547
00:28:35,180 --> 00:28:37,020
What I'm going to do,
I'm going to pick the

548
00:28:37,020 --> 00:28:39,230
smaller of e_1 and e_2.

549
00:28:39,230 --> 00:28:42,300
And whichever that one is, I'm going
to pick the hypothesis that

550
00:28:42,300 --> 00:28:43,840
corresponds to it.

551
00:28:43,840 --> 00:28:45,320
So this is mini learning.

552
00:28:45,320 --> 00:28:46,440
The error--

553
00:28:46,440 --> 00:28:48,420
just pick one, and this is the one.

554
00:28:48,420 --> 00:28:51,570
My question to you is very simple.

555
00:28:51,570 --> 00:28:55,380
What is the expected value of e?

556
00:28:55,380 --> 00:28:59,270
A naive thought would say, you told us
the expected value of e_1 is 1/2.

557
00:28:59,270 --> 00:29:01,930
You told us the expected
value of e_2 is 1/2.

558
00:29:01,930 --> 00:29:04,030
e has to be either e_1 or e_2.

559
00:29:04,030 --> 00:29:07,180
So the expected value should be 1/2?

560
00:29:07,180 --> 00:29:09,910
Of course not, because now the rules
of the game-- the probabilities that

561
00:29:09,910 --> 00:29:12,660
you're applying-- have changed, because
you are deliberately picking the

562
00:29:12,660 --> 00:29:15,180
minimum of the realization.

563
00:29:15,180 --> 00:29:20,000
And it's very easy to see that the
expected value of e is less than 0.5.

564
00:29:20,000 --> 00:29:24,360
The easiest thing to say is that if
I have two variables like that, the

565
00:29:24,360 --> 00:29:31,650
probability that the minimum will be
less than 1/2 is 75%, because all you

566
00:29:31,650 --> 00:29:34,390
need to do is one of them
being less than 1/2.

567
00:29:34,390 --> 00:29:37,340
If the probability of being less
than 1/2 is 75%, you expect the

568
00:29:37,340 --> 00:29:39,170
expected value to be less than 1/2.

569
00:29:39,170 --> 00:29:39,910
It's mostly there.

570
00:29:39,910 --> 00:29:42,230
The mass is mostly below.

571
00:29:42,230 --> 00:29:44,190
So now you realize this is what?

572
00:29:44,190 --> 00:29:46,500
This is an optimistic bias.

573
00:29:46,500 --> 00:29:49,770
And that is exactly the same as what
happened with the early stopping.

574
00:29:49,770 --> 00:29:54,390
We picked the point because it's minimum
on the realization, and that is what

575
00:29:54,390 --> 00:29:54,880
we reported.

576
00:29:54,880 --> 00:29:57,260
Because of that-- the thing
used to be this,

577
00:29:57,260 --> 00:29:57,950
but we wait.

578
00:29:57,950 --> 00:29:59,440
When it's there, we ignore it.

579
00:29:59,440 --> 00:30:01,090
When it's here, we take it.

580
00:30:01,090 --> 00:30:04,550
So now that introduces a bias,
and that bias is optimistic.

581
00:30:04,550 --> 00:30:06,690
And that will be true for
the validation set.

582
00:30:06,690 --> 00:30:11,180
Our discussion so far is based
on just looking at the E_out.

583
00:30:11,180 --> 00:30:14,630
Now we're going to use it, and we're
going to introduce a bias.

584
00:30:14,630 --> 00:30:19,810
Fortunately for us, the utility of
validation in machine learning is so

585
00:30:19,810 --> 00:30:23,740
light, that we are going
to swallow the bias.

586
00:30:23,740 --> 00:30:24,760
Bias is minor.

587
00:30:24,760 --> 00:30:26,120
We are not going to push our luck.

588
00:30:26,120 --> 00:30:30,550
We are not going to estimate tons of
stuff, and keep adding bias until the

589
00:30:30,550 --> 00:30:33,380
validation error basically becomes
training error in disguise.

590
00:30:33,380 --> 00:30:36,130
We're just going to-- let's choose
a parameter, choose between models,

591
00:30:36,130 --> 00:30:36,860
and whatnot.

592
00:30:36,860 --> 00:30:40,660
And by and large, if you do that, and
you have a respectable-size validation

593
00:30:40,660 --> 00:30:44,092
set, you get a pretty reliable
estimate for the E_out,

594
00:30:44,092 --> 00:30:47,310
conceding that it's biased, but the bias
is not going to hurt us too much.

595
00:30:47,310 --> 00:30:50,150
So this is the general philosophy.

596
00:30:50,150 --> 00:30:53,440
Now with this understanding, let's
use validation set for model

597
00:30:53,440 --> 00:30:55,840
selection, which is what
validation sets do.

598
00:30:55,840 --> 00:30:59,700
That is the main use
of validation sets.

599
00:30:59,700 --> 00:31:03,740
And the choice of lambda, in the
case we saw, happens to be

600
00:31:03,740 --> 00:31:05,390
a manifestation of this.

601
00:31:05,390 --> 00:31:07,630
So let's talk about it.

602
00:31:07,630 --> 00:31:11,930
Basically, we are going to use the
validation set more than once.

603
00:31:11,930 --> 00:31:13,410
That's how we are going
to make the choice.

604
00:31:13,410 --> 00:31:15,060
So let's look.

605
00:31:15,060 --> 00:31:16,140
This is a diagram.

606
00:31:16,140 --> 00:31:17,640
I'm going to build it up.

607
00:31:17,640 --> 00:31:21,040
Let's build it up, and then I'll
focus on it, and look at how the

608
00:31:21,040 --> 00:31:24,140
diagram reflects the logic.

609
00:31:24,140 --> 00:31:28,560
We have M models that
we're going to choose from.

610
00:31:28,560 --> 00:31:32,980
When I say model, you are thinking of
one model versus another, but this is

611
00:31:32,980 --> 00:31:35,090
really talking more generally.

612
00:31:35,090 --> 00:31:40,660
I could be talking about models as in,
should I use linear models or neural

613
00:31:40,660 --> 00:31:43,200
networks or support vector machines?

614
00:31:43,200 --> 00:31:45,250
These are models.

615
00:31:45,250 --> 00:31:48,810
I could be using only
polynomial models.

616
00:31:48,810 --> 00:31:52,070
And I'm asking myself, should
I go for 2nd order, 5th

617
00:31:52,070 --> 00:31:54,020
order, or 10th order?

618
00:31:54,020 --> 00:31:56,540
That's a choice between models.

619
00:31:56,540 --> 00:32:00,930
I could be using 5th-order polynomials
throughout, and the only

620
00:32:00,930 --> 00:32:02,110
thing I'm choosing,

621
00:32:02,110 --> 00:32:09,550
should I choose lambda of the
regularization to be 0.01, 0.1, or 1?

622
00:32:09,550 --> 00:32:11,820
All of this lies under
model selection.

623
00:32:11,820 --> 00:32:15,270
There's a choice to be made, and I want
to make it in a principled way,

624
00:32:15,270 --> 00:32:17,820
based on the out-of-sample error,
because that's the bottom line.

625
00:32:17,820 --> 00:32:20,050
And I'm going to use the validation
set to do that.

626
00:32:20,050 --> 00:32:21,300
This is the game.

627
00:32:21,300 --> 00:32:23,290


628
00:32:23,290 --> 00:32:28,230
So we'll call them, since they're
models, I have H_1 up to H_M.

629
00:32:28,230 --> 00:32:34,000
And we are going to use D to train,
and I am going to get

630
00:32:34,000 --> 00:32:35,250
as a result of that--

631
00:32:35,250 --> 00:32:37,950


632
00:32:37,950 --> 00:32:42,280
it's not the whole set, as usual,
so I left some for validation.

633
00:32:42,280 --> 00:32:43,640
And I'm going to get g minus.

634
00:32:43,640 --> 00:32:46,260
That is our convention for whenever
we train on something less

635
00:32:46,260 --> 00:32:47,520
than the full set.

636
00:32:47,520 --> 00:32:51,700
But because I'm getting a hypothesis
from each model, I am labeling it

637
00:32:51,700 --> 00:32:58,080
by the subscript m. So there is g_1 up
to g_M, with a minus, because they

638
00:32:58,080 --> 00:33:01,450
used D_train to train.

639
00:33:01,450 --> 00:33:03,850
So I get one for each model.

640
00:33:03,850 --> 00:33:08,340
And then I'm going to evaluate that
fellow, using the validation set.

641
00:33:08,340 --> 00:33:12,440
The validation set are the examples that
were left out from D, when I took

642
00:33:12,440 --> 00:33:14,140
the D_train.

643
00:33:14,140 --> 00:33:15,060
So now I'm going to do this.

644
00:33:15,060 --> 00:33:18,490
All I'm doing is exactly what I did
before, except I'm doing it M

645
00:33:18,490 --> 00:33:22,660
times, and introducing the notation
that goes with that.

646
00:33:22,660 --> 00:33:26,260
Let's look at the figure
now a little bit.

647
00:33:26,260 --> 00:33:28,310
Here is the situation.

648
00:33:28,310 --> 00:33:30,010
I have the data set.

649
00:33:30,010 --> 00:33:30,920
What do I do with it?

650
00:33:30,920 --> 00:33:32,410
I break it into two,

651
00:33:32,410 --> 00:33:35,800
validation and training.

652
00:33:35,800 --> 00:33:40,960
I use the training to apply to
each of these

653
00:33:40,960 --> 00:33:43,740
hypothesis sets, H_1 up to H_M.

654
00:33:43,740 --> 00:33:47,350
And when I train, I end up
with a final hypothesis.

655
00:33:47,350 --> 00:33:51,740
It is with a minus, a small
minus in this case, because I'm

656
00:33:51,740 --> 00:33:53,130
training on D_train.

657
00:33:53,130 --> 00:33:59,430
And they correspond to the hypotheses
they came from, so g_1, g_2, up to g_M.

658
00:33:59,430 --> 00:34:02,510
These are done without any
validation, just training

659
00:34:02,510 --> 00:34:03,990
on a reduced set.

660
00:34:03,990 --> 00:34:07,930
Once I get them, I'm going to
evaluate their performance.

661
00:34:07,930 --> 00:34:11,540
I'm going to evaluate their performance
using the validation set.

662
00:34:11,540 --> 00:34:13,960
So I take the validation
set and run it here.

663
00:34:13,960 --> 00:34:16,600
It's out-of-sample as far they're
concerned, because it's

664
00:34:16,600 --> 00:34:17,929
not part of D_train.

665
00:34:17,929 --> 00:34:20,780
And therefore, I'm going to get
estimates-- these are the validation errors.

666
00:34:20,780 --> 00:34:26,580
I'm just giving them a simple notation
as E_1, E_2, up to E_M.

667
00:34:26,580 --> 00:34:31,620
Now, your model selection is to look
at these errors, which supposedly

668
00:34:31,620 --> 00:34:37,440
reflect the out-of-sample performance if
you use this as your final product,

669
00:34:37,440 --> 00:34:39,870
and you pick the best.

670
00:34:39,870 --> 00:34:43,500
Now that you are picking one of them,
you immediately have alarm bells--

671
00:34:43,500 --> 00:34:44,790
bias, bias, bias.

672
00:34:44,790 --> 00:34:47,810
Something is happening now, because
now we are going to be biased.

673
00:34:47,810 --> 00:34:51,650
Each of these guys was an unbiased
estimate of the out-of-sample error of

674
00:34:51,650 --> 00:34:53,179
the corresponding hypothesis.

675
00:34:53,179 --> 00:34:56,159
You pick the smallest of them,
and now you have a bias.

676
00:34:56,159 --> 00:34:59,970
So the smallest of them will give the
index m star, whichever that might be.

677
00:34:59,970 --> 00:35:05,340
So E_m star is the validation error on
the model we selected, and now we

678
00:35:05,340 --> 00:35:07,650
realize it has an optimistic bias.

679
00:35:07,650 --> 00:35:11,280
And we are not going to take g_m
star minus, which is the one that

680
00:35:11,280 --> 00:35:12,310
gave rise to this.

681
00:35:12,310 --> 00:35:14,630
We are now going to go back
to the full data set, as

682
00:35:14,630 --> 00:35:15,890
we said in our regime.

683
00:35:15,890 --> 00:35:17,580
We are going to train with it.

684
00:35:17,580 --> 00:35:22,080
And from that training, which is
training now on the model we chose, we

685
00:35:22,080 --> 00:35:27,170
are going to get the final hypothesis,
which is g_m star.

686
00:35:27,170 --> 00:35:32,090
So again, we are reporting the
validation error on a reduced

687
00:35:32,090 --> 00:35:35,950
hypothesis, if you will, but reporting
the hypothesis-- the best we can do,

688
00:35:35,950 --> 00:35:37,860
because we know that we get
better out-of-sample

689
00:35:37,860 --> 00:35:39,060
when we add the examples.

690
00:35:39,060 --> 00:35:41,630
So this is the regime.

691
00:35:41,630 --> 00:35:44,680
Let's complete the slide.

692
00:35:44,680 --> 00:35:49,055
E_m, that we introduced, happens to be
the value of the validation error on

693
00:35:49,055 --> 00:35:51,570
the reduced, as we discussed.

694
00:35:51,570 --> 00:35:53,400
And this is true for all of them.

695
00:35:53,400 --> 00:35:58,790
And then you pick the model m star, that
happens to have the smallest E_m.

696
00:35:58,790 --> 00:36:02,520
And that is the one that you are going
to report, and you are going to

697
00:36:02,520 --> 00:36:06,250
restore your D, as we did before,
and this is what you have.

698
00:36:06,250 --> 00:36:10,890
This is the algorithm
for model selection.

699
00:36:10,890 --> 00:36:12,426
Now, let's look at the bias.

700
00:36:12,426 --> 00:36:15,020
I'm going to run an experiment
to show you the bias.

701
00:36:15,020 --> 00:36:18,280
So let me put it here and
just build towards it.

702
00:36:18,280 --> 00:36:19,680
What is the bias now?

703
00:36:19,680 --> 00:36:23,380
We know we selected a particular
model, and we selected

704
00:36:23,380 --> 00:36:25,580
it based on D_val.

705
00:36:25,580 --> 00:36:27,040
That's the killer.

706
00:36:27,040 --> 00:36:30,520
When you use the estimate to choose,
the estimate is no longer

707
00:36:30,520 --> 00:36:34,680
reliable, because you particularly
chose it, so now it looks

708
00:36:34,680 --> 00:36:35,110
optimistic.

709
00:36:35,110 --> 00:36:39,150
Because by choice, it has a good
performance, not because it has

710
00:36:39,150 --> 00:36:41,190
an inherently good performance. Because
you looked for the one with

711
00:36:41,190 --> 00:36:44,210
the good performance.

712
00:36:44,210 --> 00:36:50,950
So the expected value of this fellow
is now a biased estimate of the

713
00:36:50,950 --> 00:36:53,940
ultimate quantity we want, which
is the out-of-sample error.

714
00:36:53,940 --> 00:36:58,300
So E_val, the sample thing,
is biased from that.

715
00:36:58,300 --> 00:37:00,500
And we would like to evaluate that.

716
00:37:00,500 --> 00:37:03,250
Here is the illustration on the
curve, and I'm going to ask you

717
00:37:03,250 --> 00:37:05,750
a question about it, so you have to pay
attention in order to be able to

718
00:37:05,750 --> 00:37:07,230
answer the question.

719
00:37:07,230 --> 00:37:08,330
Here is the experiment.

720
00:37:08,330 --> 00:37:10,140
I have a very simple situation.

721
00:37:10,140 --> 00:37:12,650
I have only two models
to choose between.

722
00:37:12,650 --> 00:37:16,020
One of them is 2nd-order polynomials,
and the other one is

723
00:37:16,020 --> 00:37:18,130
5th-order polynomials.

724
00:37:18,130 --> 00:37:21,990
I'm generating a bunch of problems, and
in each of them, I make a choice

725
00:37:21,990 --> 00:37:24,010
based on validation set.

726
00:37:24,010 --> 00:37:28,160
And after that, I look at the
actual out-of-sample error.

727
00:37:28,160 --> 00:37:31,900
And I'm trying to find out whether there
is a systematic bias in the one

728
00:37:31,900 --> 00:37:34,830
I choose, with respect to
its out-of-sample error.

729
00:37:34,830 --> 00:37:37,580
So it's not clear which--

730
00:37:37,580 --> 00:37:39,460
I'm saying that I chose H_2 or H_5.

731
00:37:39,460 --> 00:37:43,840
In each run, I may choose H_2 sometimes
and H_5 sometimes, whichever gave me

732
00:37:43,840 --> 00:37:45,650
the smaller E_val.

733
00:37:45,650 --> 00:37:48,680
And I'm taking the average over
an incredible number of runs.

734
00:37:48,680 --> 00:37:50,670
That's why you have a smooth curve.

735
00:37:50,670 --> 00:37:54,160
So this will give me an indication of
the typical bias you get when you make

736
00:37:54,160 --> 00:37:58,120
a choice between two models, under the
circumstances of the experiment.

737
00:37:58,120 --> 00:38:01,180
Now the experiment is done carefully,
with few examples.

738
00:38:01,180 --> 00:38:03,240
The total is 30-some examples.

739
00:38:03,240 --> 00:38:07,440
And I'm taking a validation set,
which is 5 examples, 15

740
00:38:07,440 --> 00:38:09,610
examples, up to 25 examples.

741
00:38:09,610 --> 00:38:12,010
So at this point really, the
number of examples left for

742
00:38:12,010 --> 00:38:14,440
training is very small.

743
00:38:14,440 --> 00:38:21,520
And I'm plotting this, so this is what I
get for the average over the runs, of

744
00:38:21,520 --> 00:38:25,550
the validation error on the model I
chose-- the final hypothesis of the

745
00:38:25,550 --> 00:38:26,460
model I chose.

746
00:38:26,460 --> 00:38:30,420
And this is the out-of-sample
error of that guy.

747
00:38:30,420 --> 00:38:33,950
Now, I'd like to ask
you two questions.

748
00:38:33,950 --> 00:38:36,610
Think about them, and also
for the online audience,

749
00:38:36,610 --> 00:38:38,370
please think about them.

750
00:38:38,370 --> 00:38:40,330
First question--

751
00:38:40,330 --> 00:38:42,030
why are the curves going up?

752
00:38:42,030 --> 00:38:54,550


753
00:38:54,550 --> 00:38:57,620
This is K, the size of
the validation set.

754
00:38:57,620 --> 00:38:59,110
I'm evaluating it.

755
00:38:59,110 --> 00:39:02,330
It's not because I'm evaluating
on more points that the

756
00:39:02,330 --> 00:39:03,690
curves are going up.

757
00:39:03,690 --> 00:39:08,750
It's because when I use more for
validation, I'm inherently using less

758
00:39:08,750 --> 00:39:10,280
for training.

759
00:39:10,280 --> 00:39:13,160
So there's an N minus K that is
going the other direction.

760
00:39:13,160 --> 00:39:18,450
And what we are seeing here really
is the learning curve, backwards.

761
00:39:18,450 --> 00:39:19,810
This is E_out.

762
00:39:19,810 --> 00:39:23,250
I have more and more examples to train
as I go here, so the out-of-sample

763
00:39:23,250 --> 00:39:24,460
error goes down.

764
00:39:24,460 --> 00:39:27,340
So in the other direction, it goes up.

765
00:39:27,340 --> 00:39:29,880
And this, being an estimate for
it, goes up with it.

766
00:39:29,880 --> 00:39:31,740
So that makes sense.

767
00:39:31,740 --> 00:39:34,200
Second question--

768
00:39:34,200 --> 00:39:36,325
why are the two curves getting
closer together?

769
00:39:36,325 --> 00:39:38,870


770
00:39:38,870 --> 00:39:41,390
Whether they're going up or down, that's
not my concern at this point,

771
00:39:41,390 --> 00:39:43,230
just the fact that they are
converging to each other.

772
00:39:43,230 --> 00:39:48,060


773
00:39:48,060 --> 00:39:50,440
Now, that has to do with
K proper, directly.

774
00:39:50,440 --> 00:39:53,750
The other had to do with K indirectly,
because I'm left with N minus K.

775
00:39:53,750 --> 00:39:57,700
But now, when I have bigger K, the
estimate is more and more reliable,

776
00:39:57,700 --> 00:40:00,560
and therefore I get closer
to what I'm estimating.

777
00:40:00,560 --> 00:40:01,790
So we understand this.

778
00:40:01,790 --> 00:40:03,270
This is the definitely evidence,

779
00:40:03,270 --> 00:40:05,620
and in every situation you will
have, there will be a bias.

780
00:40:05,620 --> 00:40:08,915
How much bias depends on a number of
factors, but the bias is there.

781
00:40:08,915 --> 00:40:18,070


782
00:40:18,070 --> 00:40:22,210
Let's try to find, analytically,
a guideline for the type of bias.

783
00:40:22,210 --> 00:40:22,910
Why is that?

784
00:40:22,910 --> 00:40:26,420
Because I'm using the validation set to
estimate the out-of-sample error,

785
00:40:26,420 --> 00:40:29,170
and I'm really claiming that it's close
to the out-of-sample error.

786
00:40:29,170 --> 00:40:33,210
And we realize that, if I don't
use it too much, I'll be OK.

787
00:40:33,210 --> 00:40:34,470
But what is too much?

788
00:40:34,470 --> 00:40:38,370
I want to be a little bit quantitative
about it, at least as a guideline.

789
00:40:38,370 --> 00:40:43,870
So I have M models, and you can see
that the M is in red.

790
00:40:43,870 --> 00:40:47,810
That should remind you when we had
M in red very early in the

791
00:40:47,810 --> 00:40:52,060
course, because M used
to make things worse.

792
00:40:52,060 --> 00:40:55,040
It was the number of hypotheses, when we
were talking about generalization.

793
00:40:55,040 --> 00:40:59,530
And it was really that, when you have
bigger M, you are in bigger trouble.

794
00:40:59,530 --> 00:41:02,530
So it seems like we are also going to
be in bigger trouble here, but the

795
00:41:02,530 --> 00:41:04,080
manifestation is different.

796
00:41:04,080 --> 00:41:07,360
We have now M models we
are choosing from-- models

797
00:41:07,360 --> 00:41:08,140
in the general sense.

798
00:41:08,140 --> 00:41:13,130
This could be M values of the
regularization parameter lambda in

799
00:41:13,130 --> 00:41:16,850
a fixed situation, but we are still
making one of M choices.

800
00:41:16,850 --> 00:41:20,180


801
00:41:20,180 --> 00:41:25,810
Now, the way to look at it is to think
that the validation set is actually

802
00:41:25,810 --> 00:41:34,130
used for training, but training on
a very special hypothesis set, the

803
00:41:34,130 --> 00:41:36,980
hypothesis set of the finalists.

804
00:41:36,980 --> 00:41:38,780
What does that mean?

805
00:41:38,780 --> 00:41:40,720
So I have H_1 up to H_M.

806
00:41:40,720 --> 00:41:44,720
I'm going to run a full training
algorithm on each of them, in order to

807
00:41:44,720 --> 00:41:50,700
find a final hypothesis from
each, using D_train.

808
00:41:50,700 --> 00:41:53,770
Now, after I'm done, I am only
left with the finalists,

809
00:41:53,770 --> 00:41:58,230
g_1 up to g_M, with a minus sign because
they are trained on the

810
00:41:58,230 --> 00:41:59,940
reduced set.

811
00:41:59,940 --> 00:42:09,010
So the hypothesis set that I am training
on now is just those guys.

812
00:42:09,010 --> 00:42:11,130
As far as the validation set is
concerned, it didn't know what

813
00:42:11,130 --> 00:42:12,100
happened before.

814
00:42:12,100 --> 00:42:13,470
It doesn't relate to D_train.

815
00:42:13,470 --> 00:42:17,520
All you did, you gave it this hypothesis
set, which is the final

816
00:42:17,520 --> 00:42:22,060
hypotheses from your previous guy,
and you are asking it to choose.

817
00:42:22,060 --> 00:42:23,160
And what are you going to choose?

818
00:42:23,160 --> 00:42:24,670
You are going to choose
the minimum error.

819
00:42:24,670 --> 00:42:26,710
Well, that is simply training.

820
00:42:26,710 --> 00:42:31,090
If I just told you that this is your
hypothesis set, and that D_val is your

821
00:42:31,090 --> 00:42:32,750
training, what would you do?

822
00:42:32,750 --> 00:42:35,310
You will look for the hypothesis
with the smallest error.

823
00:42:35,310 --> 00:42:36,860
That's what you are doing here.

824
00:42:36,860 --> 00:42:42,850
So we can think of it now as if we are
actually training on this set.

825
00:42:42,850 --> 00:42:47,560
And this tells us, oh, we need to
estimate the discrepancy or the bias

826
00:42:47,560 --> 00:42:48,500
between this and that.

827
00:42:48,500 --> 00:42:51,680
Now it's between the validation error
and the out-of-sample error.

828
00:42:51,680 --> 00:42:55,980
But the validation error is really the
training error on this special set.

829
00:42:55,980 --> 00:43:00,520
So we can go back to our good old
Hoeffding and VC, and say that the

830
00:43:00,520 --> 00:43:02,060
out-of-sample error,

831
00:43:02,060 --> 00:43:05,090
in this case, given from those--
and now you can see that the

832
00:43:05,090 --> 00:43:06,350
choice here is star.

833
00:43:06,350 --> 00:43:08,250
So I'm actually choosing
one of those guys.

834
00:43:08,250 --> 00:43:14,510
This is my training, and the final,
final hypothesis is this guy-- is less

835
00:43:14,510 --> 00:43:17,900
than or equal to the out-of-sample
error, plus a penalty for the model

836
00:43:17,900 --> 00:43:18,850
complexity.

837
00:43:18,850 --> 00:43:21,400
And the penalty, if you use even
the simple union bound,

838
00:43:21,400 --> 00:43:22,840
will have that form.

839
00:43:22,840 --> 00:43:26,710
You still have the 1 over square root
of K, so you can always make it

840
00:43:26,710 --> 00:43:28,630
better by having more examples.

841
00:43:28,630 --> 00:43:31,490
But then you have a contribution because
of the number of guys you are

842
00:43:31,490 --> 00:43:32,650
choosing from.

843
00:43:32,650 --> 00:43:35,070
If you are choosing between
10 guys, that's one thing.

844
00:43:35,070 --> 00:43:37,080
If you are choosing between
100 guys, that's another.

845
00:43:37,080 --> 00:43:38,480
It's worse.

846
00:43:38,480 --> 00:43:41,240
Well, benignly worse, because
it's logarithmic, but

847
00:43:41,240 --> 00:43:43,820
nonetheless, worse.

848
00:43:43,820 --> 00:43:49,570
And if you are choosing between
an infinite number of guys, we know

849
00:43:49,570 --> 00:43:51,280
better than to dismiss
the case off-hand.

850
00:43:51,280 --> 00:43:53,720
You say, infinite number
of guys, we can't do that.

851
00:43:53,720 --> 00:43:56,950
No, no, no. Because once you
go to the infinite choices,

852
00:43:56,950 --> 00:43:58,490
you don't count anymore.

853
00:43:58,490 --> 00:44:01,310
You go for a VC dimension
of what you are doing.

854
00:44:01,310 --> 00:44:04,680
That's what the effective
complexity goes with.

855
00:44:04,680 --> 00:44:09,490
And indeed, if you're looking for choice of
one parameter, let's say I'm picking the

856
00:44:09,490 --> 00:44:11,260
regularization parameter.

857
00:44:11,260 --> 00:44:13,370
When you are actually picking the
regularization parameter, and you

858
00:44:13,370 --> 00:44:16,652
haven't put a grid-- you don't
say, I'm choosing between 1, 0.1,

859
00:44:16,652 --> 00:44:18,820
and 0.01, et cetera--

860
00:44:18,820 --> 00:44:19,820
a finite number.

861
00:44:19,820 --> 00:44:22,770
I'm actually choosing the numerical
value of lambda, whatever it would be.

862
00:44:22,770 --> 00:44:27,850
So I could end up with
lambda equal 0.127543.

863
00:44:27,850 --> 00:44:30,530
You are making a choice between
an infinite number of guys, but you don't

864
00:44:30,530 --> 00:44:32,660
look at it as an infinite
number of guys.

865
00:44:32,660 --> 00:44:35,160
You look at it as a single parameter.

866
00:44:35,160 --> 00:44:38,530
And we know a single parameter
goes with a VC dimension 1.

867
00:44:38,530 --> 00:44:40,940
That doesn't faze us.

868
00:44:40,940 --> 00:44:43,190
We dealt with VC dimensions
much bigger than that.

869
00:44:43,190 --> 00:44:46,440
And we know that if we have one
parameter, or maybe two parameters,

870
00:44:46,440 --> 00:44:49,750
and the VC dimension maybe is 2,
if you have a decent set--

871
00:44:49,750 --> 00:44:52,760
in this case decent K, not decent N,
because that's the size of the set you

872
00:44:52,760 --> 00:44:53,580
are talking about--

873
00:44:53,580 --> 00:44:57,870
then your estimate will not
be that far from E_out.

874
00:44:57,870 --> 00:44:58,980
This is the idea.

875
00:44:58,980 --> 00:45:00,870
So now you can apply this with the VC

876
00:45:00,870 --> 00:45:04,890
analysis. Instead of just going for
the number, which is the union bound,

877
00:45:04,890 --> 00:45:08,550
you go for the VC version, and
now apply it to this fellow.

878
00:45:08,550 --> 00:45:10,480
And you can ask yourself, if
I have a regularization

879
00:45:10,480 --> 00:45:12,580
parameter, what do I need?

880
00:45:12,580 --> 00:45:14,670
Or if I have another thing, which
is the early stopping.

881
00:45:14,670 --> 00:45:16,500
What is early stopping?

882
00:45:16,500 --> 00:45:19,200
I'm choosing how many
epochs to choose.

883
00:45:19,200 --> 00:45:22,830
Epochs is integer, but there is
a continuity to it, so I'm

884
00:45:22,830 --> 00:45:23,960
choosing where to stop.

885
00:45:23,960 --> 00:45:26,930
All of those choices, where one parameter
is being chosen one way or

886
00:45:26,930 --> 00:45:30,500
the other, correspond
to one degree of freedom.

887
00:45:30,500 --> 00:45:33,940
So if I tell you the rule of thumb is
that, when you are using the

888
00:45:33,940 --> 00:45:37,650
validation set, if it's a reasonable-size
set, let's say 100 points, and you

889
00:45:37,650 --> 00:45:42,710
use those 100 points to choose a couple
of parameters, you are OK.

890
00:45:42,710 --> 00:45:44,140
You already can relate to that.

891
00:45:44,140 --> 00:45:45,580
You don't need me to tell you that.

892
00:45:45,580 --> 00:45:47,600
Because, 100 points,
VC dimension 2,

893
00:45:47,600 --> 00:45:49,250
yeah, I can get something.

894
00:45:49,250 --> 00:45:52,420
Now, if I give you the 100 points
and tell you you are choosing 20

895
00:45:52,420 --> 00:45:55,560
parameters, you immediately
say, this is crazy.

896
00:45:55,560 --> 00:45:58,670
Your estimate will be completely
ruined, because you are now

897
00:45:58,670 --> 00:45:59,720
contaminating the thing.

898
00:45:59,720 --> 00:46:02,940
This is now genuinely training, because
the choice of the value of

899
00:46:02,940 --> 00:46:03,740
a parameter is what?

900
00:46:03,740 --> 00:46:05,100
Well, that's what training did.

901
00:46:05,100 --> 00:46:09,340
The training of a neural network tried to
choose the weights of the network,

902
00:46:09,340 --> 00:46:10,140
the parameters.

903
00:46:10,140 --> 00:46:12,500
There were just so many of them,
that we called it training.

904
00:46:12,500 --> 00:46:16,000
Now, when it's only one parameter or two,
we call it choice of a parameter by

905
00:46:16,000 --> 00:46:17,250
validation.

906
00:46:17,250 --> 00:46:19,820
So it's a gray area.

907
00:46:19,820 --> 00:46:23,620
If you push your luck in that direction,
the validation estimate

908
00:46:23,620 --> 00:46:27,640
will lose its main attraction, which
is the fact that it's a reasonable

909
00:46:27,640 --> 00:46:30,320
estimate of the out-of-sample,
that we can rely on.

910
00:46:30,320 --> 00:46:31,730
The reliability goes down.

911
00:46:31,730 --> 00:46:32,980
So there is this tradeoff.

912
00:46:32,980 --> 00:46:36,030


913
00:46:36,030 --> 00:46:39,790
So with the data contamination, let me
summarize it as follows.

914
00:46:39,790 --> 00:46:40,910
We have error estimates.

915
00:46:40,910 --> 00:46:42,240
We have seen some of them.

916
00:46:42,240 --> 00:46:49,750
We looked at the in-sample error, the
out-of-sample error, or E_test, and

917
00:46:49,750 --> 00:46:52,700
then we have E_val, the
validation error.

918
00:46:52,700 --> 00:46:57,240
I'd like to describe those as data
contamination, that if you use the

919
00:46:57,240 --> 00:47:01,250
data to make choices, you are
contaminating it as far as its ability

920
00:47:01,250 --> 00:47:03,010
to estimate the real performance.

921
00:47:03,010 --> 00:47:04,180
That's the idea.

922
00:47:04,180 --> 00:47:07,560
So you can look at what
is contamination.

923
00:47:07,560 --> 00:47:11,960
It's the built-in optimistic,
better described as deceptive

924
00:47:11,960 --> 00:47:13,720
because it's bad--

925
00:47:13,720 --> 00:47:17,150
you are going to go to the
bank and tell them, I can

926
00:47:17,150 --> 00:47:18,340
forecast the stock market.

927
00:47:18,340 --> 00:47:19,230
No, you can't.

928
00:47:19,230 --> 00:47:20,250
So that's bad.

929
00:47:20,250 --> 00:47:21,960
You were optimistic before
you went there.

930
00:47:21,960 --> 00:47:24,420
After that, you are in trouble.

931
00:47:24,420 --> 00:47:28,040
You are trying to get the bias in
estimating E_out, and you are trying to

932
00:47:28,040 --> 00:47:30,170
measure what is the level
of contamination.

933
00:47:30,170 --> 00:47:33,110
So let's look at the
three sets we used.

934
00:47:33,110 --> 00:47:35,770
We have the training set.

935
00:47:35,770 --> 00:47:37,390
This is just totally contaminated.

936
00:47:37,390 --> 00:47:38,860
Forget it.

937
00:47:38,860 --> 00:47:42,510
We took a neural network with 70 parameters,
and we did backpropagation, and

938
00:47:42,510 --> 00:47:45,600
we went back and forth, and we ended up
with something, and we have a great

939
00:47:45,600 --> 00:47:48,740
E_in, and we know that E_in
is no indication of E_out.

940
00:47:48,740 --> 00:47:50,860
This has been contaminated to death.

941
00:47:50,860 --> 00:47:55,680
So you cannot really rely on E_in,
as an estimate for E_out.

942
00:47:55,680 --> 00:48:00,720
When you go to the test set,
this is totally clean.

943
00:48:00,720 --> 00:48:02,770
It wasn't used in any decisions.

944
00:48:02,770 --> 00:48:04,320
It will give you an estimate.

945
00:48:04,320 --> 00:48:06,750
The estimate is unbiased.

946
00:48:06,750 --> 00:48:10,440
When you give that as your estimate,
your customer is as likely to be

947
00:48:10,440 --> 00:48:14,340
pleasantly surprised, as
unpleasantly surprised.

948
00:48:14,340 --> 00:48:18,560
And if your test set is big, they are
likely not to be surprised at all.

949
00:48:18,560 --> 00:48:20,560
It'll be very close to your estimate.

950
00:48:20,560 --> 00:48:23,090
So there is no bias there.

951
00:48:23,090 --> 00:48:26,620
Now, the validation set is in between.

952
00:48:26,620 --> 00:48:30,380
It's slightly contaminated, because
it made few choices.

953
00:48:30,380 --> 00:48:32,030
And the wisdom here,

954
00:48:32,030 --> 00:48:36,260
please keep it slightly contaminated.

955
00:48:36,260 --> 00:48:37,520
Don't get carried away.

956
00:48:37,520 --> 00:48:40,140
Sometimes when you are in the middle
of a big problem, with lots of

957
00:48:40,140 --> 00:48:42,180
data, you choose this parameter.

958
00:48:42,180 --> 00:48:44,640
Then, oh, there's another parameter I
want to choose, so you use the same

959
00:48:44,640 --> 00:48:46,300
validation set--

960
00:48:46,300 --> 00:48:48,890
alarm bells, alarm bells--
and you keep doing it.

961
00:48:48,890 --> 00:48:52,200
So you should have a regime to begin
with, that you should have not only one

962
00:48:52,200 --> 00:48:52,860
validation set.

963
00:48:52,860 --> 00:48:56,240
You could have a number of them, such
that when one of them gets dirty,

964
00:48:56,240 --> 00:48:59,560
contaminated, you move on to the other
one which hasn't been used for

965
00:48:59,560 --> 00:49:01,650
decisions, and therefore the
estimates will be reliable.

966
00:49:01,650 --> 00:49:04,610


967
00:49:04,610 --> 00:49:06,310
Now we go to cross-validation.

968
00:49:06,310 --> 00:49:14,100
Very sweet regime, and it has to do with
the dilemma about K. So now we're

969
00:49:14,100 --> 00:49:16,130
not talking about biased versus
unbiased, because this is

970
00:49:16,130 --> 00:49:17,070
already behind us.

971
00:49:17,070 --> 00:49:19,590
Now we're looking at an estimate
and a variation of the

972
00:49:19,590 --> 00:49:20,930
estimate, as we did before.

973
00:49:20,930 --> 00:49:24,950
And we have the discipline to make
sure that we don't mess it up by

974
00:49:24,950 --> 00:49:26,260
making it biased.

975
00:49:26,260 --> 00:49:27,690
So that is taken for granted.

976
00:49:27,690 --> 00:49:31,380
Now I'm just looking at a regime of
validation as we described it, versus

977
00:49:31,380 --> 00:49:36,200
another regime which will get us
a better estimate, in terms of the error

978
00:49:36,200 --> 00:49:39,170
bar, the fluctuation around
the estimate we want.

979
00:49:39,170 --> 00:49:42,640
So we had the following
chain of reasoning.

980
00:49:42,640 --> 00:49:47,400
E_out of g, the hypothesis we are
actually going to report, is what we

981
00:49:47,400 --> 00:49:48,370
would like to know.

982
00:49:48,370 --> 00:49:51,010
If we know that, we are set.

983
00:49:51,010 --> 00:49:53,610
We don't have that, but that
is approximately the same

984
00:49:53,610 --> 00:49:58,660
as E_out of g minus.

985
00:49:58,660 --> 00:50:02,730
This is the out-of-sample error, the
proper out-of-sample error, but on the

986
00:50:02,730 --> 00:50:05,670
hypothesis that was trained
on a reduced set.

987
00:50:05,670 --> 00:50:07,030
Correct?

988
00:50:07,030 --> 00:50:10,660
And if I didn't take too
many examples, they are

989
00:50:10,660 --> 00:50:11,910
close to each other.

990
00:50:11,910 --> 00:50:13,950


991
00:50:13,950 --> 00:50:18,620
This one happens to be close to
the validation estimate of it.

992
00:50:18,620 --> 00:50:25,650
So here, it is because it's a different
set that I'm training on.

993
00:50:25,650 --> 00:50:28,250
Here, it's because I am
making a finite-sample

994
00:50:28,250 --> 00:50:29,650
estimate of the quantity.

995
00:50:29,650 --> 00:50:32,570
Here, I could go up
and down from this.

996
00:50:32,570 --> 00:50:36,060
I'm looking at this chain. This
is really what I want, and this

997
00:50:36,060 --> 00:50:36,930
is what I'm working with.

998
00:50:36,930 --> 00:50:38,880
This is unknown to me.

999
00:50:38,880 --> 00:50:41,670
In order to get from here to
here, I need the following.

1000
00:50:41,670 --> 00:50:44,940


1001
00:50:44,940 --> 00:50:50,400
I need K to be small, so that g
minus is fairly close to g.

1002
00:50:50,400 --> 00:50:53,160
And therefore I can claim that their
out-of-sample error is close, because

1003
00:50:53,160 --> 00:50:57,690
the bigger K is, the bigger the
discrepancy between the training set

1004
00:50:57,690 --> 00:51:00,890
and the full set, and therefore the
bigger the discrepancy between the

1005
00:51:00,890 --> 00:51:03,040
hypothesis I get here, and the
hypothesis I get here.

1006
00:51:03,040 --> 00:51:05,490
So I'd like K to be small.

1007
00:51:05,490 --> 00:51:10,910
But also, I'd like K to be large,
because the bigger K is, the more

1008
00:51:10,910 --> 00:51:13,910
reliable this estimate is, for that.

1009
00:51:13,910 --> 00:51:16,520
So I want K to have two conditions.

1010
00:51:16,520 --> 00:51:20,760
It has to be small, and
it has to be large.

1011
00:51:20,760 --> 00:51:23,460
We will achieve both.

1012
00:51:23,460 --> 00:51:24,390
You'll see in a moment.

1013
00:51:24,390 --> 00:51:27,860
New mathematics is going
to be introduced!

1014
00:51:27,860 --> 00:51:30,690
So here is the dilemma.

1015
00:51:30,690 --> 00:51:36,450
Can we have K to be both
small and large?

1016
00:51:36,450 --> 00:51:40,530
The method looks like complete cheating,
when you look at it first,

1017
00:51:40,530 --> 00:51:42,600
and then you realize,
this is actually valid.

1018
00:51:42,600 --> 00:51:44,600
So what do we do?

1019
00:51:44,600 --> 00:51:47,620
I'm going to describe one form of
cross-validation, which is the simplest

1020
00:51:47,620 --> 00:51:50,950
to describe, which is called "leave
one out". Other methods will be

1021
00:51:50,950 --> 00:51:53,210
"leave more out", that's all.

1022
00:51:53,210 --> 00:51:55,290
But let's focus on "leave one out".

1023
00:51:55,290 --> 00:51:58,590
Here is the idea.

1024
00:51:58,590 --> 00:52:01,790
You give me a data set of N.
I am going to use N minus

1025
00:52:01,790 --> 00:52:04,220
1 of them for training.

1026
00:52:04,220 --> 00:52:08,800
That's good, because now I am very close
to N, so the hypothesis g minus

1027
00:52:08,800 --> 00:52:11,520
will be awfully close to g.

1028
00:52:11,520 --> 00:52:16,150
That's great, wonderful,
except for one problem.

1029
00:52:16,150 --> 00:52:18,820
You have one point to validate on.

1030
00:52:18,820 --> 00:52:22,580
Your estimate will be completely
laughable, right?

1031
00:52:22,580 --> 00:52:23,830
Not so fast.

1032
00:52:23,830 --> 00:52:27,240


1033
00:52:27,240 --> 00:52:31,730
In terms of a notation, I'm going to
create a reduced data set from

1034
00:52:31,730 --> 00:52:35,180
D, call it D_n, because I'm actually
going to repeat this

1035
00:52:35,180 --> 00:52:38,780
exercise for different
indices n.

1036
00:52:38,780 --> 00:52:39,800
What do I do?

1037
00:52:39,800 --> 00:52:43,930
I take the full data set, and then take
one of the points, that happens to

1038
00:52:43,930 --> 00:52:46,610
be n, and take it out.

1039
00:52:46,610 --> 00:52:49,060
This will be the one that
is used for validation.

1040
00:52:49,060 --> 00:52:52,200
And the rest of the guys are going
to be used for training.

1041
00:52:52,200 --> 00:52:57,020
Nothing different, except that
it's a very small validation set.

1042
00:52:57,020 --> 00:52:59,580
That's what is different.

1043
00:52:59,580 --> 00:53:05,550
Now the final hypothesis, that we learn
from this particular set, we have

1044
00:53:05,550 --> 00:53:08,000
to call g minus because it's
not on the full set.

1045
00:53:08,000 --> 00:53:11,890
But now, because it depends on which guy
we left out, we give it the label

1046
00:53:11,890 --> 00:53:12,950
of the guy we left out.

1047
00:53:12,950 --> 00:53:16,970
So we know that this one is trained
on all the examples but n.

1048
00:53:16,970 --> 00:53:21,540


1049
00:53:21,540 --> 00:53:26,930
Let's look at the validation error,
which has to be one point.

1050
00:53:26,930 --> 00:53:28,720
This would be what?

1051
00:53:28,720 --> 00:53:31,800
This would be E validation-- big
symbol of this and that--

1052
00:53:31,800 --> 00:53:35,920
but in reality, the validation set is
one point, so this is simply just the

1053
00:53:35,920 --> 00:53:39,150
error on the point I left out.

1054
00:53:39,150 --> 00:53:44,190
g did not involve the n-th example.

1055
00:53:44,190 --> 00:53:45,460
It was taken out.

1056
00:53:45,460 --> 00:53:48,585
And now that we froze it, we are going
to evaluate it on that example,

1057
00:53:48,585 --> 00:53:51,820
so that example is indeed
out-of-sample for it.

1058
00:53:51,820 --> 00:53:53,490
So I get this fellow.

1059
00:53:53,490 --> 00:53:57,260
Now, I know that this guy is an unbiased
estimate, and I know that

1060
00:53:57,260 --> 00:53:59,570
it's a crummy estimate.

1061
00:53:59,570 --> 00:54:00,270


1062
00:54:00,270 --> 00:54:02,130
That much, I know.

1063
00:54:02,130 --> 00:54:03,760
Now, here is the idea.

1064
00:54:03,760 --> 00:54:07,890
What happens if I repeat this exercise
for different n?

1065
00:54:07,890 --> 00:54:13,720
So I generate D_1, do all of this,
and end up with this estimate.

1066
00:54:13,720 --> 00:54:18,380
Do D_2, all of this-- end up
with another estimate.

1067
00:54:18,380 --> 00:54:23,050
Each estimate is out-of-sample with
respect to the hypothesis that it's

1068
00:54:23,050 --> 00:54:25,460
used to evaluate.

1069
00:54:25,460 --> 00:54:28,740
Now, the hypotheses are different.

1070
00:54:28,740 --> 00:54:33,640
So I'm not really getting the
performance of a particular hypothesis.

1071
00:54:33,640 --> 00:54:33,650


1072
00:54:33,650 --> 00:54:35,330
For this hypothesis, this
is the estimate.

1073
00:54:35,330 --> 00:54:35,840
It's off.

1074
00:54:35,840 --> 00:54:37,110
For this hypothesis, this
is the estimate.

1075
00:54:37,110 --> 00:54:37,470
It's off.

1076
00:54:37,470 --> 00:54:39,360
For this hypothesis,
this is the estimate.

1077
00:54:39,360 --> 00:54:43,290
The common thread between all the
hypotheses is that they are hypotheses

1078
00:54:43,290 --> 00:54:49,500
that were obtained by training on
N minus 1 data points.

1079
00:54:49,500 --> 00:54:51,620
That is common between all of them.

1080
00:54:51,620 --> 00:54:55,560
It's different N minus 1
data points, but nonetheless,

1081
00:54:55,560 --> 00:54:57,840
it's N minus 1.

1082
00:54:57,840 --> 00:55:00,350
Because of the learning curve,
I know there is a tendency.

1083
00:55:00,350 --> 00:55:04,130
If I told you this is the number of
examples, you can tell me what is the

1084
00:55:04,130 --> 00:55:06,200
expected out-of-sample error.

1085
00:55:06,200 --> 00:55:09,480
So in spite of the fact that these are
different hypotheses, the fact that

1086
00:55:09,480 --> 00:55:11,620
they come from the same
number of points,

1087
00:55:11,620 --> 00:55:12,720
N minus 1,

1088
00:55:12,720 --> 00:55:16,780
tells me that they are all realizations
of something that is the

1089
00:55:16,780 --> 00:55:19,340
expected value of all of them.

1090
00:55:19,340 --> 00:55:24,030
So the small errors estimate
the error on these guys,

1091
00:55:24,030 --> 00:55:29,380
and these guys estimate the error of
the expected value on N minus 1

1092
00:55:29,380 --> 00:55:32,460
examples, regardless of the
identity of the examples.

1093
00:55:32,460 --> 00:55:34,340
So there is something common
between these guys.

1094
00:55:34,340 --> 00:55:36,940
They are trying to estimate something.

1095
00:55:36,940 --> 00:55:41,020
So now what I'm going to do, I am going
to define the cross-validation

1096
00:55:41,020 --> 00:55:44,280
error to be--

1097
00:55:44,280 --> 00:55:47,500
E cross-validation, E_cv,

1098
00:55:47,500 --> 00:55:50,860
to be the average of those guys.

1099
00:55:50,860 --> 00:55:53,020
It's a funny situation now.

1100
00:55:53,020 --> 00:55:58,200
These came from N full training
sessions, each of them

1101
00:55:58,200 --> 00:56:02,250
followed by a single evaluation on
a point, and I get a number.

1102
00:56:02,250 --> 00:56:06,760
And after I'm done with all of this, I
take these numbers and average them.

1103
00:56:06,760 --> 00:56:09,700
Now, if you think of it as a validation
set, now all of a sudden

1104
00:56:09,700 --> 00:56:11,820
the validation set is
very respectable.

1105
00:56:11,820 --> 00:56:15,450
It has N points.

1106
00:56:15,450 --> 00:56:18,150
Never mind the fact that each of them
is evaluated on a different

1107
00:56:18,150 --> 00:56:19,690
hypothesis.

1108
00:56:19,690 --> 00:56:25,270
I was able to use N minus 1
points to train, and that will give me

1109
00:56:25,270 --> 00:56:27,580
something very close to what
happens with N. And I'm

1110
00:56:27,580 --> 00:56:30,570
using N points to validate.

1111
00:56:30,570 --> 00:56:31,820
The catch, obviously--

1112
00:56:31,820 --> 00:56:31,830


1113
00:56:31,830 --> 00:56:38,720
these are not independent, because the
examples were used to create the

1114
00:56:38,720 --> 00:56:42,230
hypotheses, and some example
was used to evaluate them.

1115
00:56:42,230 --> 00:56:48,070
And you will see that each of them is
affected by the other, because the

1116
00:56:48,070 --> 00:56:53,100
hypothesis either has the point you left
out, or you are evaluating on that.

1117
00:56:53,100 --> 00:56:56,430
Let's say, e_1 and e_3.

1118
00:56:56,430 --> 00:57:01,930
e_1 was used to evaluate the error on
a hypothesis that involved the third

1119
00:57:01,930 --> 00:57:06,340
example, because the third example
was in, when I talk about e_1.

1120
00:57:06,340 --> 00:57:11,380
Then e_3 was used to evaluate on the
third example, but on a hypothesis

1121
00:57:11,380 --> 00:57:12,760
that involved e_1.

1122
00:57:12,760 --> 00:57:14,710
So you can see where
the correlation is.

1123
00:57:14,710 --> 00:57:20,340
Surprisingly, the effective number, if
you use this, is very close to N. It's

1124
00:57:20,340 --> 00:57:22,840
as if they were independent.

1125
00:57:22,840 --> 00:57:26,800
If you do the variance analysis,
you will be using

1126
00:57:26,800 --> 00:57:30,130
out of 100 examples, it's probably
as if you were using 95 examples.

1127
00:57:30,130 --> 00:57:35,110
So it's remarkably efficient,
in terms of getting that.

1128
00:57:35,110 --> 00:57:36,830
So this is the algorithm.

1129
00:57:36,830 --> 00:57:38,990
Now, let's illustrate it.

1130
00:57:38,990 --> 00:57:41,660
If you understand this, you
understand cross-validation.

1131
00:57:41,660 --> 00:57:45,390
I'm illustrating it for
the "leave one out".

1132
00:57:45,390 --> 00:57:46,150
I have a case.

1133
00:57:46,150 --> 00:57:48,020
I am trying to estimate a function.

1134
00:57:48,020 --> 00:57:50,350
I actually generated this function
using a particular target.

1135
00:57:50,350 --> 00:57:53,720
I'm not going to tell you yet what
it is. Added some noise.

1136
00:57:53,720 --> 00:57:58,240
And I am trying to use cross-validation,
in order to choose a model,

1137
00:57:58,240 --> 00:58:00,630
or to just evaluate the
out-of-sample error.

1138
00:58:00,630 --> 00:58:04,160
So let's evaluate the out-of-sample
error using the cross-validation

1139
00:58:04,160 --> 00:58:06,420
method, for a linear model.

1140
00:58:06,420 --> 00:58:07,730
So what do you do?

1141
00:58:07,730 --> 00:58:12,660
First order of business, take a point
that you will leave out. Right?

1142
00:58:12,660 --> 00:58:18,750
So now, this guy is the training set,
and this guy is the validation set.

1143
00:58:18,750 --> 00:58:21,190
It's one point.

1144
00:58:21,190 --> 00:58:24,380
Then you train.

1145
00:58:24,380 --> 00:58:26,570
And you get a good fit.

1146
00:58:26,570 --> 00:58:31,550
Then, you evaluate the validation
error on the point you left out.

1147
00:58:31,550 --> 00:58:34,230
That will be that.

1148
00:58:34,230 --> 00:58:35,880
That's one session.

1149
00:58:35,880 --> 00:58:38,580
We are going to repeat this three times,
because we have three points.

1150
00:58:38,580 --> 00:58:41,450
So this is the second time we do it.

1151
00:58:41,450 --> 00:58:43,750
This time, this point was left out.

1152
00:58:43,750 --> 00:58:45,700
These guys were the training.

1153
00:58:45,700 --> 00:58:49,110
I connected them and
computed the error.

1154
00:58:49,110 --> 00:58:54,560
Third one. You can see the pattern.

1155
00:58:54,560 --> 00:58:59,670
After I am done, I'm going to compute
the cross-validation error to

1156
00:58:59,670 --> 00:59:02,990
be simply the average
of the three errors.

1157
00:59:02,990 --> 00:59:05,110
So let's say we are using
squared errors.

1158
00:59:05,110 --> 00:59:08,920
e_1 is the squared of this
distance, et cetera, and you are

1159
00:59:08,920 --> 00:59:10,140
adding them up, one third.

1160
00:59:10,140 --> 00:59:12,010
This will be the cross-validation error.

1161
00:59:12,010 --> 00:59:15,430
What I am saying now is that you
are going to take this as

1162
00:59:15,430 --> 00:59:22,790
an indication for how well the linear
model fits the data, out-of-sample.

1163
00:59:22,790 --> 00:59:25,400
If you look in-sample, obviously
it fits the data perfectly.

1164
00:59:25,400 --> 00:59:29,140
And if you use the three points, the
line will be something like that.

1165
00:59:29,140 --> 00:59:31,010
It will fit it pretty decently.

1166
00:59:31,010 --> 00:59:34,990
But you have no way to tell how you are
going to perform out-of-sample.

1167
00:59:34,990 --> 00:59:39,450
Here, we created a mini out-of-sample, in
each case, and we took the average

1168
00:59:39,450 --> 00:59:42,860
performance of those as an indication
of what will happen out-of-sample.

1169
00:59:42,860 --> 00:59:46,100
Mind you, we are using
only 2 points here.

1170
00:59:46,100 --> 00:59:48,600
And when we are done, we are going
to use it on 3 points.

1171
00:59:48,600 --> 00:59:50,410
That's g minus versus g.

1172
00:59:50,410 --> 00:59:53,400
It's a little bit dramatic here,
because 2 and 3--

1173
00:59:53,400 --> 00:59:55,210
the difference is 1, but
the ratio is huge.

1174
00:59:55,210 --> 00:59:57,130
But think of 99 versus 100.

1175
00:59:57,130 --> 00:59:57,810
Who cares?

1176
00:59:57,810 --> 00:59:58,900
It's close enough.

1177
00:59:58,900 --> 01:00:00,920
This is just for illustration.

1178
01:00:00,920 --> 01:00:03,670
So let's use this for model selection.

1179
01:00:03,670 --> 01:00:09,560
We did the linear model,
and we call it linear.

1180
01:00:09,560 --> 01:00:16,710
So now let's go for the usual suspect,
the constant model, exactly with the

1181
01:00:16,710 --> 01:00:17,970
same data set.

1182
01:00:17,970 --> 01:00:20,130
Let's look at the first guy.

1183
01:00:20,130 --> 01:00:23,120
These are the two points left out, the
two points left out, and this is the

1184
01:00:23,120 --> 01:00:24,870
one for validation.

1185
01:00:24,870 --> 01:00:25,990
You train on those.

1186
01:00:25,990 --> 01:00:27,020
Here, you connected. Here,

1187
01:00:27,020 --> 01:00:28,200
you have the middle number--

1188
01:00:28,200 --> 01:00:29,810
it's a constant number.

1189
01:00:29,810 --> 01:00:33,220
And this would be you error
here. Right?

1190
01:00:33,220 --> 01:00:40,570
Second guy, you get the
idea? Third guy.

1191
01:00:40,570 --> 01:00:46,270
Now, if your question is:
is the linear model better

1192
01:00:46,270 --> 01:00:49,680
than the constant model in this case?

1193
01:00:49,680 --> 01:00:56,310
the only thing you look in all of this
is the cross-validation error.

1194
01:00:56,310 --> 01:01:01,750
So this guy, this guy, this guy,
averaged, is the grade--

1195
01:01:01,750 --> 01:01:04,620
negative grade, because it's error--
for the linear model.

1196
01:01:04,620 --> 01:01:08,920
This guy, this guy, this guy, averaged,
is that grade for the constant model.

1197
01:01:08,920 --> 01:01:11,990
And as you see, the constant
model wins.

1198
01:01:11,990 --> 01:01:15,030
And it's a matter of record that these
three points were actually generated

1199
01:01:15,030 --> 01:01:17,640
by a constant model.

1200
01:01:17,640 --> 01:01:19,590
Of course, they could have been
generated by anything.

1201
01:01:19,590 --> 01:01:23,510
But on average, they will give
you the correct decision.

1202
01:01:23,510 --> 01:01:26,400
And they avoid a lot of funny heuristics
that you can apply.

1203
01:01:26,400 --> 01:01:30,290
You can say-- wait a minute,
linear model, OK.

1204
01:01:30,290 --> 01:01:34,110
Any two points I pick, the
slope here is positive.

1205
01:01:34,110 --> 01:01:37,320
So there is a very strong indication
that there is a positive slope

1206
01:01:37,320 --> 01:01:41,260
involved, and maybe it's a linear
model with a positive slope.

1207
01:01:41,260 --> 01:01:42,590
Don't go there.

1208
01:01:42,590 --> 01:01:45,470
You can fool yourself into
any pattern you want.

1209
01:01:45,470 --> 01:01:47,560
Go about it in a systematic way.

1210
01:01:47,560 --> 01:01:50,050
This is a quantity we know,
the cross-validation error.

1211
01:01:50,050 --> 01:01:51,430
This is the way to compute it.

1212
01:01:51,430 --> 01:01:55,040
We are going to take it as the
indication, notwithstanding that there

1213
01:01:55,040 --> 01:01:59,080
is an error bar because it's a small
sample, in this case 3,

1214
01:01:59,080 --> 01:02:03,180
and also because we are making the
decision for 2 points, and we are

1215
01:02:03,180 --> 01:02:05,000
using it for 3 points.

1216
01:02:05,000 --> 01:02:08,020
These are obviously inherent, but
at least it gives you something

1217
01:02:08,020 --> 01:02:08,790
systematic.

1218
01:02:08,790 --> 01:02:13,470
And indeed, it gives you the correct
choice in this case.

1219
01:02:13,470 --> 01:02:18,400
So let's look at
cross-validation in action.

1220
01:02:18,400 --> 01:02:21,970
I'm going to go with
a familiar case.

1221
01:02:21,970 --> 01:02:23,580
You remember this one?

1222
01:02:23,580 --> 01:02:26,350
Oh, these were the handwritten
digits, and we

1223
01:02:26,350 --> 01:02:27,450
extracted two features,

1224
01:02:27,450 --> 01:02:30,250
symmetry and intensity.

1225
01:02:30,250 --> 01:02:34,340
And we are plotting the different guys,
and we would like to find

1226
01:02:34,340 --> 01:02:36,450
a separating surface.

1227
01:02:36,450 --> 01:02:40,040
We are going to use a nonlinear
transform, as we always do.

1228
01:02:40,040 --> 01:02:43,580
And in this case, what I'm going to
do, I'm going to sample 500 points

1229
01:02:43,580 --> 01:02:49,050
from this set at random for training,
and use the rest for testing the

1230
01:02:49,050 --> 01:02:51,050
hypothesis.

1231
01:02:51,050 --> 01:02:52,840
What is the nonlinear transformation?

1232
01:02:52,840 --> 01:02:53,990
It's huge--

1233
01:02:53,990 --> 01:02:55,930
5th order.

1234
01:02:55,930 --> 01:02:59,620
So I am going to take all
20 features, or 21

1235
01:02:59,620 --> 01:03:01,600
including the constant.

1236
01:03:01,600 --> 01:03:03,550
And what am I going to
use validation for?

1237
01:03:03,550 --> 01:03:05,200
This is the interesting part.

1238
01:03:05,200 --> 01:03:09,700
What I'm going to use validation
for is, where do I cut off?

1239
01:03:09,700 --> 01:03:12,180
So I'm comparing 20 models.

1240
01:03:12,180 --> 01:03:14,420
The first model is,
just take this guy.

1241
01:03:14,420 --> 01:03:16,000
Second model is, take x_1 and x_2.

1242
01:03:16,000 --> 01:03:18,640
Third model, take x_1, x_2, and
x_1 squared, et cetera.

1243
01:03:18,640 --> 01:03:19,800
Each of them is a model.

1244
01:03:19,800 --> 01:03:22,270
I can definitely train on
it and see what happens.

1245
01:03:22,270 --> 01:03:25,690
And I'm going to use cross-validation
"leave one out", in order to choose

1246
01:03:25,690 --> 01:03:28,550
where to stop.

1247
01:03:28,550 --> 01:03:32,040
So if I have 500 examples, realize that
every time I do this, I have to

1248
01:03:32,040 --> 01:03:36,060
have 500 training sessions.

1249
01:03:36,060 --> 01:03:39,200
Each training session has 499 points.

1250
01:03:39,200 --> 01:03:41,000
It's quite an elaborate thing.

1251
01:03:41,000 --> 01:03:43,980
But when you do this, this
is the curve you get.

1252
01:03:43,980 --> 01:03:45,690
You get different errors.

1253
01:03:45,690 --> 01:03:47,110
Let me magnify it.

1254
01:03:47,110 --> 01:03:51,010


1255
01:03:51,010 --> 01:03:53,530
This is the number
of features used.

1256
01:03:53,530 --> 01:03:54,730
This is the cutoff I talked about.

1257
01:03:54,730 --> 01:03:57,820
You can go all the way
up to 20 features.

1258
01:03:57,820 --> 01:04:01,180
When you look at the training error, not
surprisingly, the training error

1259
01:04:01,180 --> 01:04:02,570
always goes down.

1260
01:04:02,570 --> 01:04:03,230
What else is new?

1261
01:04:03,230 --> 01:04:06,190
You have more, you fit better.

1262
01:04:06,190 --> 01:04:09,080
The out-of-sample error, which I'm
evaluating on the points that were not

1263
01:04:09,080 --> 01:04:12,630
involved at all in this process,
cross-validation or otherwise, just out of

1264
01:04:12,630 --> 01:04:16,140
sample totally, I get this fellow.

1265
01:04:16,140 --> 01:04:19,810
And the cross-validation error, which
I get from the 500 examples by

1266
01:04:19,810 --> 01:04:23,690
excluding one point at a time and taking
the average, is remarkably

1267
01:04:23,690 --> 01:04:25,115
similar to E_out.

1268
01:04:25,115 --> 01:04:27,620
It tracks it very nicely.

1269
01:04:27,620 --> 01:04:32,000
And if I use it as a criterion for model
choice, the minima are here.

1270
01:04:32,000 --> 01:04:34,370
So if I take between 5 and 7,
let's say I take 6.

1271
01:04:34,370 --> 01:04:37,040


1272
01:04:37,040 --> 01:04:40,310
I would say, let me cut off at 6
and see what the performance is like.

1273
01:04:40,310 --> 01:04:43,460
Let's look at the result of that,

1274
01:04:43,460 --> 01:04:44,710
without validation, and with validation.

1275
01:04:44,710 --> 01:04:48,670


1276
01:04:48,670 --> 01:04:51,600
Without validation, I'm using
the full model, all 20.

1277
01:04:51,600 --> 01:04:54,910
And you can see, we have seen
this before-- overfitting.

1278
01:04:54,910 --> 01:04:58,300
I'm sweating bullets to include this
single point in the middle, and after

1279
01:04:58,300 --> 01:04:59,710
I included it, guess what?

1280
01:04:59,710 --> 01:05:01,490
None of the out-of-sample points
was red here.

1281
01:05:01,490 --> 01:05:02,520
This was just an anomaly.

1282
01:05:02,520 --> 01:05:04,380
So I didn't get anything for it.
1283
01:05:04,380 --&gt; 01:05:05,870t
This is a typical thing.

1283
01:05:05,870 --> 01:05:07,940
It's unregularized.

1284
01:05:07,940 --> 01:05:10,860
Now, when you use the validation, and
you stop at the 6th because the

1285
01:05:10,860 --> 01:05:14,200
cross-validation error told you so,
it's a nice, smooth surface.

1286
01:05:14,200 --> 01:05:19,600
It's not perfect error, but it didn't
put an effort where it didn't belong.

1287
01:05:19,600 --> 01:05:24,200
And when you look at the bottom line,
what is the in-sample error here?

1288
01:05:24,200 --> 01:05:24,810
0%.

1289
01:05:24,810 --> 01:05:26,080
You got it perfect.

1290
01:05:26,080 --> 01:05:27,970
We know that.

1291
01:05:27,970 --> 01:05:30,190
And the out-of-sample sample error?

1292
01:05:30,190 --> 01:05:31,280
2.5%.

1293
01:05:31,280 --> 01:05:33,106
For digits, that's OK,.

1294
01:05:33,106 --> 01:05:36,650
OK, but not great.

1295
01:05:36,650 --> 01:05:37,440
Here, we went.

1296
01:05:37,440 --> 01:05:39,580
And now the in-sample error is 0.8%.

1297
01:05:39,580 --> 01:05:40,610
But we know better.

1298
01:05:40,610 --> 01:05:42,610
We don't care about the in-sample
error going to 0.

1299
01:05:42,610 --> 01:05:44,600
That's actually harmful in some cases.

1300
01:05:44,600 --> 01:05:48,810
The out-of-sample error is 1.5%.

1301
01:05:48,810 --> 01:05:52,000
Now, if you are in the range--
2.5% means that you are

1302
01:05:52,000 --> 01:05:54,080
performing 97.5%.

1303
01:05:54,080 --> 01:05:56,620
Here, you are performing 98.5%.

1304
01:05:56,620 --> 01:06:00,570
40% improvement in that
range is a lot.

1305
01:06:00,570 --> 01:06:03,400
There is a limit here that
you cannot exceed.

1306
01:06:03,400 --> 01:06:07,730
So here, you are really doing great
by just doing that simple thing.

1307
01:06:07,730 --> 01:06:11,700
Now you can see why validation is
considered, in this context, as

1308
01:06:11,700 --> 01:06:12,920
similar to regularization.

1309
01:06:12,920 --> 01:06:14,220
It does the same thing.

1310
01:06:14,220 --> 01:06:18,040
It prevented overfitting, but it
prevented overfitting by estimating

1311
01:06:18,040 --> 01:06:21,230
the out-of-sample error, rather than
estimating something else.

1312
01:06:21,230 --> 01:06:25,310


1313
01:06:25,310 --> 01:06:31,910
Now, let me go and very quickly--

1314
01:06:31,910 --> 01:06:33,520
and I will close the lecture with it--

1315
01:06:33,520 --> 01:06:34,650
give you the more general form.

1316
01:06:34,650 --> 01:06:38,230
We talked about "leave one out".
Seldom you use "leave one out"

1317
01:06:38,230 --> 01:06:39,930
in real problems, and you
can think of why.

1318
01:06:39,930 --> 01:06:44,120
Because if I give you 100,000 data
points, and you want to leave one out,

1319
01:06:44,120 --> 01:06:50,330
you are going to have 100,000 sessions
training on 99,999 for each, and you

1320
01:06:50,330 --> 01:06:53,220
will be an old person before
the results are out.

1321
01:06:53,220 --> 01:06:58,520
So when you have "leave one out",
you have N training sessions

1322
01:06:58,520 --> 01:07:02,990
using N minus 1 points each, right?

1323
01:07:02,990 --> 01:07:03,000


1324
01:07:03,000 --> 01:07:07,070
Now, let's consider to take
more points for validation.

1325
01:07:07,070 --> 01:07:10,070
1 point makes it great, because
N minus 1 is so close to N,

1326
01:07:10,070 --> 01:07:12,130
that my g minus will
be so close to g.

1327
01:07:12,130 --> 01:07:16,200
But hey, 100,000, if you decided
to take 100,000 minus 1,000,

1328
01:07:16,200 --> 01:07:17,600
that's still 99,000.

1329
01:07:17,600 --> 01:07:18,720
That's fairly close to 100,000.

1330
01:07:18,720 --> 01:07:21,030
You don't have to make
it difference 1.

1331
01:07:21,030 --> 01:07:26,740
So what you do is, you take your
data set, and you break it into

1332
01:07:26,740 --> 01:07:27,630
a number of folds.

1333
01:07:27,630 --> 01:07:29,110
Let's say 10-fold.

1334
01:07:29,110 --> 01:07:31,830
So this will be 10-fold
cross-validation.

1335
01:07:31,830 --> 01:07:36,860
And each time, you take one of the guys
here, that is, 1/10 in this

1336
01:07:36,860 --> 01:07:41,360
case, use it for validation, and the
9/10, you use them for training.

1337
01:07:41,360 --> 01:07:45,190
And you change, from one run to another,
which one you take for validation.

1338
01:07:45,190 --> 01:07:48,900
So "leave one out" is exactly the
same, except that here, the 10,

1339
01:07:48,900 --> 01:07:52,850
replace it by N. I break the thing
into 1 example at a time, and then I

1340
01:07:52,850 --> 01:07:54,120
validate on 1 example.

1341
01:07:54,120 --> 01:07:57,150
Here, I'm taking a chunk.

1342
01:07:57,150 --> 01:08:00,530
And therefore, you have fewer
training sessions,

1343
01:08:00,530 --> 01:08:03,100
in this case 10 training sessions,

1344
01:08:03,100 --> 01:08:07,730
with not that much of a difference, in
terms of the number of examples.

1345
01:08:07,730 --> 01:08:12,340
If N is big, instead of taking 1,
you take a few more.

1346
01:08:12,340 --> 01:08:15,100
Now, the reason I introduced this is
because this is what I actually

1347
01:08:15,100 --> 01:08:16,689
recommend to you.

1348
01:08:16,689 --> 01:08:20,590
Very specifically, 10-fold
cross-validation works

1349
01:08:20,590 --> 01:08:24,069
very nicely in practice.

1350
01:08:24,069 --> 01:08:27,540
So the rule is, you take the total
number of examples, divide them by 10,

1351
01:08:27,540 --> 01:08:29,090
and that is the size of
your validation set.

1352
01:08:29,090 --> 01:08:33,020
You repeat it 10 times, and you get
an estimate, and you are ready to go.

1353
01:08:33,020 --> 01:08:34,609
That's it.

1354
01:08:34,609 --> 01:08:37,571
I will stop here, and we'll take
questions after a short break.

1355
01:08:37,571 --> 01:08:40,520


1356
01:08:40,520 --> 01:08:46,430
Let's start the Q&amp;A. And we
have an in-house question.

1357
01:08:46,430 --> 01:08:51,779
STUDENT: You told about
validation, and you told that we

1358
01:08:51,779 --> 01:08:57,340
should restrict ourselves in amount of
parameters we should estimate.

1359
01:08:57,340 --> 01:09:02,660
Do we have a rule of thumb about
the number of these parameters?

1360
01:09:02,660 --> 01:09:09,470
So is, say, K over 20 parameters
reasonable for the maximum number?

1361
01:09:09,470 --> 01:09:12,300
PROFESSOR: It obviously depends
on the number of data points.

1362
01:09:12,300 --> 01:09:16,180
So the reason why I didn't give a rule
of thumb in this case, because it goes

1363
01:09:16,180 --> 01:09:17,840
with the number of points.

1364
01:09:17,840 --> 01:09:21,729
But let's say that if I have 100 points
for validation, so it's a small

1365
01:09:21,729 --> 01:09:24,770
data set, I would say that a couple
of parameters would be fine.

1366
01:09:24,770 --> 01:09:27,319
At least, that's my own experience.

1367
01:09:27,319 --> 01:09:30,260
And you can afford more,
when you have more.

1368
01:09:30,260 --> 01:09:33,410
And when you have more, you can even
afford more than one validation set,

1369
01:09:33,410 --> 01:09:37,180
in which case, you use each of them
for a different estimate.

1370
01:09:37,180 --> 01:09:41,500
But the simplest thing, I would say,
a couple of parameters for 100 points

1371
01:09:41,500 --> 01:09:42,750
would be OK.

1372
01:09:42,750 --> 01:09:48,200


1373
01:09:48,200 --> 01:09:54,800
MODERATOR: Can you clarify why model
choice by validation doesn't count as

1374
01:09:54,800 --> 01:09:56,050
data snooping?

1375
01:09:56,050 --> 01:10:00,640


1376
01:10:00,640 --> 01:10:04,870
PROFESSOR: For the same reason
that the answer is usually given for

1377
01:10:04,870 --> 01:10:08,870
a question like that, because
it is accounted for.

1378
01:10:08,870 --> 01:10:14,880
I took the validation set, the
validation set are patently out of

1379
01:10:14,880 --> 01:10:19,490
sample, and I used them
to make a choice.

1380
01:10:19,490 --> 01:10:27,870
And when I did that choice, I made
sure that the discrepancy between

1381
01:10:27,870 --> 01:10:31,110
in-sample and out-of-sample on the
validation set is very little.

1382
01:10:31,110 --> 01:10:34,300
So we had this discussion of how much
bias there is, and we want to make

1383
01:10:34,300 --> 01:10:37,120
sure that the discrepancy
is very little.

1384
01:10:37,120 --> 01:10:40,860
So because I have already done the
accounting, I can take it as

1385
01:10:40,860 --> 01:10:44,170
a reasonable estimate for the
out-of-sample.

1386
01:10:44,170 --> 01:10:46,540
That is why.

1387
01:10:46,540 --> 01:10:50,320
In the other case, the problem with the
data snooping that I gave is that

1388
01:10:50,320 --> 01:10:54,420
you use the data in order
to make choices, and in

1389
01:10:54,420 --> 01:10:55,530
that case, huge choices.

1390
01:10:55,530 --> 01:10:58,270
You looked at the data and you chose
between different models, and you

1391
01:10:58,270 --> 01:10:59,240
didn't pay for it.

1392
01:10:59,240 --> 01:11:01,480
You didn't account for it.

1393
01:11:01,480 --> 01:11:02,730
That's where the problem was.

1394
01:11:02,730 --> 01:11:06,250


1395
01:11:06,250 --> 01:11:14,130
MODERATOR: Some people recommend
using cross-validation 10 times.

1396
01:11:14,130 --> 01:11:15,380
What does that add?

1397
01:11:15,380 --> 01:11:18,590


1398
01:11:18,590 --> 01:11:25,320
PROFESSOR: The regime I
described, I only need to tell you

1399
01:11:25,320 --> 01:11:30,510
10-fold, 12-fold, 50-fold, and
then the rest is fixed.

1400
01:11:30,510 --> 01:11:33,230
So if I use 10-fold, then
by definition I'm going

1401
01:11:33,230 --> 01:11:34,250
to do this 10 times.

1402
01:11:34,250 --> 01:11:38,740
It's not a choice, given the regime
that I described. In each

1403
01:11:38,740 --> 01:11:43,590
run, I am choosing one of the 10 to
be my validation, and the rest for

1404
01:11:43,590 --> 01:11:46,720
training, and taking the average.

1405
01:11:46,720 --> 01:11:51,200
So the question is asking,
do I do this 10 times?

1406
01:11:51,200 --> 01:11:54,170
Inherently, built in the method
is that you use it 10 times,

1407
01:11:54,170 --> 01:11:55,920
if that's the question.

1408
01:11:55,920 --> 01:12:02,270
MODERATOR: I think the question goes to,
since you chose your 10 data sets

1409
01:12:02,270 --> 01:12:06,780
inside, then you'd run
cross-validation.

1410
01:12:06,780 --> 01:12:11,270
What if you do it again choosing 10
subsets, and you repeat that process?

1411
01:12:11,270 --> 01:12:12,100
PROFESSOR: There are variations.

1412
01:12:12,100 --> 01:12:16,560
For example, even, let's say, with the
"leave one out", maybe I can take

1413
01:12:16,560 --> 01:12:19,590
a point at random, and not necessarily
insist on going through all the

1414
01:12:19,590 --> 01:12:22,400
examples-- do it like 50 times,
and take the average.

1415
01:12:22,400 --> 01:12:26,400
Or I can take subsets, like in the
10-fold, but I take random subsets

1416
01:12:26,400 --> 01:12:27,390
and stop at some point.

1417
01:12:27,390 --> 01:12:29,070
So there are variations of those.

1418
01:12:29,070 --> 01:12:32,650
The ones I described are
the most standard ones.

1419
01:12:32,650 --> 01:12:34,270
But there are obviously
variations.

1420
01:12:34,270 --> 01:12:35,920
And one can do an analysis
for them as well.

1421
01:12:35,920 --> 01:12:38,860


1422
01:12:38,860 --> 01:12:41,910
MODERATOR: Is there any rule for
separating data among training,

1423
01:12:41,910 --> 01:12:43,790
validation, and test?

1424
01:12:43,790 --> 01:12:47,010
PROFESSOR: Random is
the only trustworthy thing.

1425
01:12:47,010 --> 01:12:51,740
Because if you use your judgment
somehow, you may introduce a sampling

1426
01:12:51,740 --> 01:12:54,180
bias, which we'll talk about
in a later lecture.

1427
01:12:54,180 --> 01:12:58,970
And the best way to avoid that for sure,
if you sort of flip coins to

1428
01:12:58,970 --> 01:13:00,740
choose your examples, then you
know that you are safe.

1429
01:13:00,740 --> 01:13:04,390


1430
01:13:04,390 --> 01:13:07,190
MODERATOR: What's the computational
complexity of adding

1431
01:13:07,190 --> 01:13:08,840
a cross-validation?

1432
01:13:08,840 --> 01:13:10,770


1433
01:13:10,770 --> 01:13:13,450
PROFESSOR: I didn't give
the formula for it.

1434
01:13:13,450 --> 01:13:18,590
Basically, for "leave one out", you
are doing N times as much

1435
01:13:18,590 --> 01:13:20,110
training as you did before.

1436
01:13:20,110 --> 01:13:23,370
The evaluation is trivial. Most of
the time goes for the training.

1437
01:13:23,370 --> 01:13:27,440
So you can ask yourself, how many
training sessions do I have to do now

1438
01:13:27,440 --> 01:13:30,305
that I'm using cross-validation, versus
what I had to do before?

1439
01:13:30,305 --> 01:13:32,080
Before, you had to do one session.

1440
01:13:32,080 --> 01:13:35,200
Here, you have to do as many sessions
as there are folds.

1441
01:13:35,200 --> 01:13:37,520
So 10-fold will be 10 times.

1442
01:13:37,520 --> 01:13:40,480
"Leave one out" would be N,
because it's really N-fold, if

1443
01:13:40,480 --> 01:13:41,730
you want, and so on.

1444
01:13:41,730 --> 01:13:44,850


1445
01:13:44,850 --> 01:13:45,870
MODERATOR: A clarification--

1446
01:13:45,870 --> 01:13:48,650
can you use both regularization
and cross-validation?

1447
01:13:48,650 --> 01:13:50,110
PROFESSOR: Absolutely.

1448
01:13:50,110 --> 01:13:54,530
In fact, one of the biggest utilities
for validation is to choose

1449
01:13:54,530 --> 01:13:56,570
the regularization parameter.

1450
01:13:56,570 --> 01:13:58,220
So inherently in those
cases, you do it.

1451
01:13:58,220 --> 01:14:00,510
You can use it to choose the
regularization parameter.

1452
01:14:00,510 --> 01:14:03,350
And then you can also use it on
the side, to do something else.

1453
01:14:03,350 --> 01:14:05,760
So both of them are active
in the same problem.

1454
01:14:05,760 --> 01:14:10,390
And in most of the practical cases you
will encounter, you will actually be

1455
01:14:10,390 --> 01:14:11,810
using both.

1456
01:14:11,810 --> 01:14:15,700
Very seldom can you get away without
regularization, and very seldom can

1457
01:14:15,700 --> 01:14:17,070
you get away without validation.

1458
01:14:17,070 --> 01:14:20,250


1459
01:14:20,250 --> 01:14:23,960
MODERATOR: Someone is asking that, this
seems to be a brute force method for

1460
01:14:23,960 --> 01:14:25,200
model selection.

1461
01:14:25,200 --> 01:14:31,480
Is there a way to branch and bound
how many hypotheses to consider?

1462
01:14:31,480 --> 01:14:35,630
PROFESSOR: There are lots
of methods for model selection.

1463
01:14:35,630 --> 01:14:41,620
This is the only one, at least among the
major ones, which does not require

1464
01:14:41,620 --> 01:14:43,290
assumptions.

1465
01:14:43,290 --> 01:14:47,610
I can do model selection based on,
I know my target function is

1466
01:14:47,610 --> 01:14:50,260
symmetric, so I'm going to
choose a symmetric model.

1467
01:14:50,260 --> 01:14:52,290
That can be considered
model selection.

1468
01:14:52,290 --> 01:14:57,030
And there are a bunch of other logical
methods to choose the model.

1469
01:14:57,030 --> 01:15:00,140
The great thing about validation is
that there are no assumptions

1470
01:15:00,140 --> 01:15:01,400
whatsoever.

1471
01:15:01,400 --> 01:15:02,550
You have M models.

1472
01:15:02,550 --> 01:15:05,210
What are the models?
What assumptions do they have?

1473
01:15:05,210 --> 01:15:07,630
How close they are, or not close
to the target function--

1474
01:15:07,630 --> 01:15:08,760
Who cares?

1475
01:15:08,760 --> 01:15:10,320
They are M models.

1476
01:15:10,320 --> 01:15:13,460
I am going to take a validation set, and
I'm going to find this objective

1477
01:15:13,460 --> 01:15:16,470
criterion, which is a validation or
cross-validation error, and I'm going

1478
01:15:16,470 --> 01:15:17,670
to use it to choose.

1479
01:15:17,670 --> 01:15:21,530
So it's extremely simple to implement,
and very immune to assumptions.

1480
01:15:21,530 --> 01:15:24,260
Obviously, if you make assumptions and
you know that the assumption are

1481
01:15:24,260 --> 01:15:27,600
valid, then you would be doing
better than I am doing.

1482
01:15:27,600 --> 01:15:29,550
But then you know that the
assumptions are valid.

1483
01:15:29,550 --> 01:15:32,660
I'm taking a case where I don't want to
make assumptions, that I don't know

1484
01:15:32,660 --> 01:15:37,740
hold, and still make the
model selection.

1485
01:15:37,740 --> 01:15:41,570
MODERATOR: In the case where the data
depends on time evolution,

1486
01:15:41,570 --> 01:15:45,440
how can validation update the model?

1487
01:15:45,440 --> 01:15:46,690
Is it used for that, or not?

1488
01:15:46,690 --> 01:15:49,330


1489
01:15:49,330 --> 01:15:54,570
PROFESSOR: Validation makes
a principled choice, regardless of the

1490
01:15:54,570 --> 01:15:57,760
nature of that choice.

1491
01:15:57,760 --> 01:16:03,130
Let's say that I have a time series, and
one of the things in time series--

1492
01:16:03,130 --> 01:16:04,660
let's say they're for financial
forecasting--

1493
01:16:04,660 --> 01:16:09,920
is that, you can train, and then you
get a system, and then the world

1494
01:16:09,920 --> 01:16:11,110
is not stationary.

1495
01:16:11,110 --> 01:16:14,490
So a system that used to work,
doesn't work anymore.

1496
01:16:14,490 --> 01:16:17,680
You can make choices about, let's
say I have a bunch of models, and I

1497
01:16:17,680 --> 01:16:22,220
want to know which one of them works
at a particular time, given some

1498
01:16:22,220 --> 01:16:23,120
conditions.

1499
01:16:23,120 --> 01:16:25,960
You can make the model selection
based on validation, and then you take

1500
01:16:25,960 --> 01:16:31,420
that model and apply it to the real
data, or there are a bunch of things

1501
01:16:31,420 --> 01:16:32,110
you can do.

1502
01:16:32,110 --> 01:16:36,730
But in terms of tracking the evolution
of systems, again, if you translate

1503
01:16:36,730 --> 01:16:40,200
the problem into making a choice, then
you are ready to go with validation.

1504
01:16:40,200 --> 01:16:41,850
So the answer is yes.

1505
01:16:41,850 --> 01:16:46,950
And the method is to make it
spelled out as a choice.

1506
01:16:46,950 --> 01:16:48,040
MODERATOR: Another clarification--

1507
01:16:48,040 --> 01:16:53,090
so with cross-validation,
there's still some bias.

1508
01:16:53,090 --> 01:17:01,190
can you quantify why is it better
than just regular validation?

1509
01:17:01,190 --> 01:17:03,830
PROFESSOR: Both validation and
cross-validation will have bias for

1510
01:17:03,830 --> 01:17:05,860
the same reasons.

1511
01:17:05,860 --> 01:17:10,900
The only question is the reliability
of the estimate.

1512
01:17:10,900 --> 01:17:16,391
Let's say that I use "leave
one out", so here's E_out.

1513
01:17:16,391 --> 01:17:22,520
And the bias aside, if I use
"leave one out", I'm using all N of

1514
01:17:22,520 --> 01:17:24,750
the examples eventually,
when I average them.

1515
01:17:24,750 --> 01:17:26,510
So the error bar is small.

1516
01:17:26,510 --> 01:17:31,690
Granted, it's not as small as it would
be if the N errors were independent of

1517
01:17:31,690 --> 01:17:32,420
each other.

1518
01:17:32,420 --> 01:17:35,300
But it's fairly close to being
as if they were independent.

1519
01:17:35,300 --> 01:17:37,320
So I get that estimate.

1520
01:17:37,320 --> 01:17:41,890
Therefore, anytime you have this
estimate, it becomes less

1521
01:17:41,890 --> 01:17:43,130
vulnerable to bias.

1522
01:17:43,130 --> 01:17:47,320
Because if I have this play, and I'm
pulling down, I'm not going to pull

1523
01:17:47,320 --> 01:17:50,380
down too far, because I'm
still within here.

1524
01:17:50,380 --> 01:17:53,150
If I have the other guy which is
completely swinging, it's very easy to

1525
01:17:53,150 --> 01:17:55,660
pull it down and I get worse
effect of the bias.

1526
01:17:55,660 --> 01:17:59,805
So whenever you minimize the error bar,
you minimize the vulnerability to

1527
01:17:59,805 --> 01:18:00,920
bias as well.

1528
01:18:00,920 --> 01:18:03,420
That's the only thing that
cross-validation does.

1529
01:18:03,420 --> 01:18:07,430
It allows you to use a lot of examples
to validate, while using a lot of

1530
01:18:07,430 --> 01:18:08,690
examples to train.

1531
01:18:08,690 --> 01:18:09,940
That's the key.

1532
01:18:09,940 --> 01:18:13,250


1533
01:18:13,250 --> 01:18:17,120
MODERATOR: Going back to the previous
lecture, a question on that.

1534
01:18:17,120 --> 01:18:24,790
Can you see the augmented error as
conceptually the same as a low-pass

1535
01:18:24,790 --> 01:18:29,090
filtered version of the initial
error, or not?

1536
01:18:29,090 --> 01:18:31,690
PROFESSOR: It can be translated
to that under the condition

1537
01:18:31,690 --> 01:18:35,100
that the regularizer is a smoothness
regularizer, because that's what

1538
01:18:35,100 --> 01:18:36,590
low-pass filters do.

1539
01:18:36,590 --> 01:18:41,080
So as an intuition, it's not
a bad thing to consider

1540
01:18:41,080 --> 01:18:45,240
in the case of something like weight
decay. It's not going to be strictly

1541
01:18:45,240 --> 01:18:48,940
low-pass as in working in the Fourier
domain and cutting off, et cetera.

1542
01:18:48,940 --> 01:18:51,590
But it will have the same
effect of being smooth.

1543
01:18:51,590 --> 01:18:52,670
If you have a question,

1544
01:18:52,670 --> 01:18:57,240
please step to the microphone,
and you can ask it.

1545
01:18:57,240 --> 01:19:00,310
So there's a question in house.

1546
01:19:00,310 --> 01:19:01,560
STUDENT: Yes.

1547
01:19:01,560 --> 01:19:04,450


1548
01:19:04,450 --> 01:19:09,140
It seems that cross-validation is
a method to deal with limited size of

1549
01:19:09,140 --> 01:19:10,060
the data set.

1550
01:19:10,060 --> 01:19:14,040
So is it possible in practice that
we have a data set so large that

1551
01:19:14,040 --> 01:19:17,540
cross-validation is not needed or not
beneficial, or do people do it all the

1552
01:19:17,540 --> 01:19:18,550
time in principle?

1553
01:19:18,550 --> 01:19:23,610
PROFESSOR: It is possible, and
one of the cases is the Netflix case,

1554
01:19:23,610 --> 01:19:25,390
where you had 100 million points.

1555
01:19:25,390 --> 01:19:28,610
So you think at this point, nobody will
care about cross-validation.

1556
01:19:28,610 --> 01:19:32,590
But it turned out that even in this
case, the 100 million points only had

1557
01:19:32,590 --> 01:19:37,510
a very small subset which come from the
same distribution as the output.

1558
01:19:37,510 --> 01:19:38,580
So the 100 million--

1559
01:19:38,580 --> 01:19:41,320
again, it's the same question
as the time evolution.

1560
01:19:41,320 --> 01:19:44,470
You have people making ratings,
and different people making different

1561
01:19:44,470 --> 01:19:48,640
number of ratings, and this
changes for a number of reasons.

1562
01:19:48,640 --> 01:19:49,920
Even the same user,

1563
01:19:49,920 --> 01:19:54,385
after you rate for a while, you tend
to change from the initial rating.

1564
01:19:54,385 --> 01:19:55,850
Maybe you are initially
excited or something.

1565
01:19:55,850 --> 01:19:57,860
So there are lots of
considerations like that.

1566
01:19:57,860 --> 01:20:03,010
So eventually, the number of points
that were patently coming from the

1567
01:20:03,010 --> 01:20:07,110
same distribution as the out-of-sample
was much smaller than 100 million.

1568
01:20:07,110 --> 01:20:09,380
And these are the ones that were
used to make big decisions,

1569
01:20:09,380 --> 01:20:10,980
like validation decisions.

1570
01:20:10,980 --> 01:20:14,480
And in that case, even if we started
with 100 million, it might be a good

1571
01:20:14,480 --> 01:20:16,340
idea to use cross-validation
at the end.

1572
01:20:16,340 --> 01:20:20,440
And if you use something like 10-fold
cross-validation, then it's not that

1573
01:20:20,440 --> 01:20:23,390
big a deal, because you are just
multiplying the effort by 10, which

1574
01:20:23,390 --> 01:20:26,860
is, given what is involved,
not that big a deal.

1575
01:20:26,860 --> 01:20:29,210
And you really get a dividend
in performance.

1576
01:20:29,210 --> 01:20:32,380
And if you insist on performance,
then it becomes indicated.

1577
01:20:32,380 --> 01:20:38,280
So the answer is yes, because it doesn't
cost that much, and because

1578
01:20:38,280 --> 01:20:42,870
sometimes in a big data set, the
relevant part, or the most relevant

1579
01:20:42,870 --> 01:20:44,190
part, is smaller than the whole set.

1580
01:20:44,190 --> 01:20:46,930


1581
01:20:46,930 --> 01:20:50,160
MODERATOR: Say there's a scenario where
you find your model through

1582
01:20:50,160 --> 01:20:53,660
cross-validation, and then you
test the out-of-sample error.

1583
01:20:53,660 --> 01:20:57,010
But somehow you test a different model,
and it gives you a smaller

1584
01:20:57,010 --> 01:20:57,800
out-of-sample error.

1585
01:20:57,800 --> 01:21:01,590
Should you still keep the one you
found through cross-validation?

1586
01:21:01,590 --> 01:21:05,060
PROFESSOR: So I went
through this learning and

1587
01:21:05,060 --> 01:21:06,380
came up with a model.

1588
01:21:06,380 --> 01:21:10,710
Someone else went through whatever
exercise they have and came up with

1589
01:21:10,710 --> 01:21:12,780
a final hypothesis in this case.

1590
01:21:12,780 --> 01:21:16,240
And I am declaring mine the winner
because of cross-validation, and now

1591
01:21:16,240 --> 01:21:18,360
we are saying that there's further
statistical evidence.

1592
01:21:18,360 --> 01:21:22,780
We get an out-of-sample error that tells
me that mine is not as good as

1593
01:21:22,780 --> 01:21:24,110
the other one.

1594
01:21:24,110 --> 01:21:28,540
Then it really is the question of,
I have two samples, and I'm doing

1595
01:21:28,540 --> 01:21:29,850
an evaluation.

1596
01:21:29,850 --> 01:21:33,410
And one of them tells me something, and
the other one tells me the other.

1597
01:21:33,410 --> 01:21:36,665
So I need to consider first the size
of them. That will give me the

1598
01:21:36,665 --> 01:21:40,210
relative size of the error bar.
And correlations, if any. And bias, which

1599
01:21:40,210 --> 01:21:43,040
cross-validation may have, whereas the
other one, if it's truly out of

1600
01:21:43,040 --> 01:21:44,010
sample, does not.

1601
01:21:44,010 --> 01:21:47,310
If I go through the math, and maybe
the math won't go through--

1602
01:21:47,310 --> 01:21:48,790
it's not always the case--

1603
01:21:48,790 --> 01:21:52,170
I will get an indication about
which one I would favor.

1604
01:21:52,170 --> 01:21:58,910
But basically, it's purely a statistical
question at this point.

1605
01:21:58,910 --> 01:22:02,190
MODERATOR: When there are few points,
and cross-validation is going to be

1606
01:22:02,190 --> 01:22:06,330
done, is it a good idea to re-sample
to enlarge the current

1607
01:22:06,330 --> 01:22:08,420
sample, or not really?

1608
01:22:08,420 --> 01:22:10,290
PROFESSOR: So I have a small data set.

1609
01:22:10,290 --> 01:22:11,580
That's the premise?

1610
01:22:11,580 --> 01:22:12,860
And I'm doing cross-validation.

1611
01:22:12,860 --> 01:22:13,850
So what is the--

1612
01:22:13,850 --> 01:22:17,520
MODERATOR: So the problem is,
since you have few samples,

1613
01:22:17,520 --> 01:22:19,890
do you want to re-sample?

1614
01:22:19,890 --> 01:22:23,210
PROFESSOR: So instead of
breaking them into chunks,

1615
01:22:23,210 --> 01:22:24,940
keep taking at random?

1616
01:22:24,940 --> 01:22:33,430
Well, I don't have from my experience
something that would indicate that one

1617
01:22:33,430 --> 01:22:35,320
would win over the other.

1618
01:22:35,320 --> 01:22:40,300
And I suspect that if you are close to
10-fold, you probably are close to the

1619
01:22:40,300 --> 01:22:44,240
best performance you can get with
variations of these methods.

1620
01:22:44,240 --> 01:22:47,820
And the problem is that all of these
things are not completely pinned down

1621
01:22:47,820 --> 01:22:48,460
mathematically.

1622
01:22:48,460 --> 01:22:51,250
There is a heuristic part of it, because
even cross-validation, we

1623
01:22:51,250 --> 01:22:53,340
don't know what the correlation
is, et cetera.

1624
01:22:53,340 --> 01:22:56,810
So we cannot definitively answer the
question of which one is better.

1625
01:22:56,810 --> 01:22:59,560
It's a question of trying in a number
of problems, after getting the

1626
01:22:59,560 --> 01:23:02,440
theoretical guidelines, and
then choosing something.

1627
01:23:02,440 --> 01:23:06,520
What is being reported here is that
the 10-fold cross-validation stood the

1628
01:23:06,520 --> 01:23:07,550
test of time.

1629
01:23:07,550 --> 01:23:10,000
That's the statement.

1630
01:23:10,000 --> 01:23:14,820
MODERATOR: When there is a big class
size imbalance, does cross-validation

1631
01:23:14,820 --> 01:23:16,070
become a problem?

1632
01:23:16,070 --> 01:23:18,105


1633
01:23:18,105 --> 01:23:20,690
PROFESSOR: When there is an imbalance
between the classes, that is

1634
01:23:20,690 --> 01:23:24,810
a bunch of +1's and fewer -1's,
there are certain things that need

1635
01:23:24,810 --> 01:23:27,350
to be taken into consideration, in order
to make learning go through

1636
01:23:27,350 --> 01:23:31,500
well-- in order to basically avoid the
learning algorithm going for the

1637
01:23:31,500 --> 01:23:34,760
all +1 solution, because it's
a very attractive one.

1638
01:23:34,760 --> 01:23:37,270
So there are a bunch of things that can
be taken into consideration, and I

1639
01:23:37,270 --> 01:23:39,600
can see a possible role
for cross-validation.

1640
01:23:39,600 --> 01:23:48,660
But it's not a strong component
as far as I can see.

1641
01:23:48,660 --> 01:23:52,170
The question of balancing them, making
sure that you avoid the all-constant,

1642
01:23:52,170 --> 01:23:54,333
and stuff like that will probably
play a bigger role.

1643
01:23:54,333 --> 01:23:59,430


1644
01:23:59,430 --> 01:24:05,790
MODERATOR: How does the bias behave when
we increase the number of points

1645
01:24:05,790 --> 01:24:07,620
that we leave out?

1646
01:24:07,620 --> 01:24:09,820
The size of t if we leave t out.

1647
01:24:09,820 --> 01:24:16,380
PROFESSOR: The points we
leave out are the validation points.

1648
01:24:16,380 --> 01:24:21,080
And if we are using the 10-fold or
12-fold, et cetera, the total number

1649
01:24:21,080 --> 01:24:24,665
that go into the summation will be
constant, because in spite of the fact

1650
01:24:24,665 --> 01:24:26,870
that we're taking different numbers,
we go through all of them,

1651
01:24:26,870 --> 01:24:28,410
and we add them up.

1652
01:24:28,410 --> 01:24:30,400
So that number doesn't change.

1653
01:24:30,400 --> 01:24:34,770


1654
01:24:34,770 --> 01:24:38,650
MODERATOR: So how does it change,
if instead of doing

1655
01:24:38,650 --> 01:24:40,930
10-fold, you use 20-fold?

1656
01:24:40,930 --> 01:24:43,040
How does that--

1657
01:24:43,040 --> 01:24:43,640
PROFESSOR: How does it change?

1658
01:24:43,640 --> 01:24:47,180
It doesn't change the number of total
points going into the estimate

1659
01:24:47,180 --> 01:24:48,220
of cross-validation.

1660
01:24:48,220 --> 01:24:49,590
But what was the original question?

1661
01:24:49,590 --> 01:24:51,480
MODERATOR: So how does
the bias behave?

1662
01:24:51,480 --> 01:24:53,120
PROFESSOR: Oh.

1663
01:24:53,120 --> 01:24:59,040
Well, given that the total number will
give you the error bar, and given that

1664
01:24:59,040 --> 01:25:02,010
the bias is really a function of how
you use it, rather than something

1665
01:25:02,010 --> 01:25:07,500
inherent in the estimate, the error bar
will give you an indication of how

1666
01:25:07,500 --> 01:25:09,140
vulnerable you are to bias.

1667
01:25:09,140 --> 01:25:12,300
Say that, if you take two scenarios
where the error bar is comparable, you

1668
01:25:12,300 --> 01:25:15,190
have no reason to think that one of them
will be more vulnerable to bias

1669
01:25:15,190 --> 01:25:16,500
or another.

1670
01:25:16,500 --> 01:25:19,740
Now, you need a very detailed analysis
to see the difference between taking

1671
01:25:19,740 --> 01:25:23,720
one at a time coming from N minus 1,
et cetera, and to consider the

1672
01:25:23,720 --> 01:25:28,740
correlations, and then taking 1/10 at
the time and adding them up, to find

1673
01:25:28,740 --> 01:25:31,710
out what is the correlation and what is
the effective number of examples,

1674
01:25:31,710 --> 01:25:33,360
and therefore what is the error bar.

1675
01:25:33,360 --> 01:25:38,230
In any given situation, that would
be a pretty heavy task to do.

1676
01:25:38,230 --> 01:25:44,500
So basically, that answer is that as
long as you do a number of

1677
01:25:44,500 --> 01:25:47,470
folds, and you take every example to
appear in the cross-validation

1678
01:25:47,470 --> 01:25:52,500
estimate exactly once, then there is no
preference between them as far as

1679
01:25:52,500 --> 01:25:54,262
the bias is concerned.

1680
01:25:54,262 --> 01:25:55,730
MODERATOR: I think that's it.

1681
01:25:55,730 --> 01:25:56,340
PROFESSOR: Very good.

1682
01:25:56,340 --> 01:25:57,840
We'll see you on Thursday.

1683
01:25:57,840 --> 01:26:11,271

