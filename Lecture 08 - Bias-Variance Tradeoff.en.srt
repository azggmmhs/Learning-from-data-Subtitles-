1
00:00:00,000 --> 00:00:00,570


2
00:00:00,570 --> 00:00:03,270
ANNOUNCER: The following program
is brought to you by Caltech.

3
00:00:03,270 --> 00:00:15,410


4
00:00:15,410 --> 00:00:18,410
YASER ABU-MOSTAFA: Welcome back.

5
00:00:18,410 --> 00:00:22,370
Last time, we finished the VC analysis.

6
00:00:22,370 --> 00:00:25,950
And that took us three full lectures.

7
00:00:25,950 --> 00:00:31,800
The end result was the definition of the
VC dimension of a hypothesis set.

8
00:00:31,800 --> 00:00:38,820
It was defined as the most points that
the hypothesis set can shatter.

9
00:00:38,820 --> 00:00:43,060
And we used the VC dimension in
establishing that learning is

10
00:00:43,060 --> 00:00:47,540
feasible, on one hand, and then in
estimating the example resources that

11
00:00:47,540 --> 00:00:50,540
are needed in order to learn.

12
00:00:50,540 --> 00:00:55,430
One of the important aspects of
the VC analysis is the scope.

13
00:00:55,430 --> 00:00:59,690
The VC inequality, and the generalization
bound that corresponds

14
00:00:59,690 --> 00:01:05,379
to it, describe the generalization
ability of the final hypothesis you

15
00:01:05,379 --> 00:01:07,320
are going to pick.

16
00:01:07,320 --> 00:01:14,290
It describes that in terms of the VC
dimension of the hypothesis set, and

17
00:01:14,290 --> 00:01:19,560
makes a statement that is true for
all but delta of the data sets

18
00:01:19,560 --> 00:01:20,820
that you might get.

19
00:01:20,820 --> 00:01:23,340
So this is where it applies.

20
00:01:23,340 --> 00:01:27,910
And the most important part of the
application are the disappearing

21
00:01:27,910 --> 00:01:32,840
blocks, because it gives the generality
that the VC inequality has.

22
00:01:32,840 --> 00:01:39,040
So the VC bound is valid for any
learning algorithm, for any input

23
00:01:39,040 --> 00:01:44,960
distribution that may take place, and
also for any target function that you

24
00:01:44,960 --> 00:01:47,530
may be able to learn.

25
00:01:47,530 --> 00:01:49,120
So this is the most theoretical part.

26
00:01:49,120 --> 00:01:52,920
And then we went into a little bit of
a practical part, where we are asking

27
00:01:52,920 --> 00:01:55,850
about the utility of the VC
dimension in practice.

28
00:01:55,850 --> 00:01:59,420
You have a learning problem-- someone
comes with a problem, and you would

29
00:01:59,420 --> 00:02:03,360
like to know how many examples. What is
the size of the data set you need,

30
00:02:03,360 --> 00:02:06,840
in order to be able to achieve
a certain level of performance.

31
00:02:06,840 --> 00:02:11,980
The way we did this analysis is by
plotting the core aspect of the delta,

32
00:02:11,980 --> 00:02:15,060
the probability of error
in the VC bound.

33
00:02:15,060 --> 00:02:17,790
And we found that it's
behaving regularly.

34
00:02:17,790 --> 00:02:21,860
We focused on a certain aspect of these
curves, which correspond to

35
00:02:21,860 --> 00:02:23,960
different VC dimensions.

36
00:02:23,960 --> 00:02:26,720
And the main aspect is
below this line.

37
00:02:26,720 --> 00:02:29,340
This line designates
the probability 1.

38
00:02:29,340 --> 00:02:33,080
We want the probability of the bad event
to be small, so we are working

39
00:02:33,080 --> 00:02:34,570
in this region.

40
00:02:34,570 --> 00:02:39,320
And the x-axis here is the number
of examples-- the size

41
00:02:39,320 --> 00:02:40,930
of your data set.

42
00:02:40,930 --> 00:02:43,770
And we don't particularly care about
the shape of these guys.

43
00:02:43,770 --> 00:02:46,280
They could be a little bit
nonlinear, et cetera.

44
00:02:46,280 --> 00:02:54,440
But the quantity we are looking for is,
if we cut through this way, what is

45
00:02:54,440 --> 00:02:59,280
the behavior of the x-axis, the number
of examples, in terms of the VC

46
00:02:59,280 --> 00:03:02,640
dimension, which is the label
for the colored curves?

47
00:03:02,640 --> 00:03:07,780
And we realized that, given this analysis,
it is very much proportional.

48
00:03:07,780 --> 00:03:11,620
And we were able to say that,
theoretically, the bound will give us

49
00:03:11,620 --> 00:03:14,000
that the number of examples needed
would be proportional to the VC

50
00:03:14,000 --> 00:03:16,380
dimension, more or less.

51
00:03:16,380 --> 00:03:21,450
And although the constant of
proportionality, if you go for the

52
00:03:21,450 --> 00:03:24,480
bound, will be horrifically
pessimistic--

53
00:03:24,480 --> 00:03:27,660
you will end up requiring tens of
thousands of examples for something

54
00:03:27,660 --> 00:03:30,890
for which you really need
only maybe 50 examples--

55
00:03:30,890 --> 00:03:33,890
the good news is that the actual
quantity behaves in the

56
00:03:33,890 --> 00:03:35,330
same way as the bound.

57
00:03:35,330 --> 00:03:38,440
So the number of examples needed
is, in practice, as a practical

58
00:03:38,440 --> 00:03:41,950
observation, indeed proportional
to the VC dimension.

59
00:03:41,950 --> 00:03:45,670
And furthermore, as a rule of thumb,
in order to get to the interesting

60
00:03:45,670 --> 00:03:49,660
part, or interesting delta and epsilon,
you need the number of

61
00:03:49,660 --> 00:03:52,870
examples to be 10 times
the VC dimension.

62
00:03:52,870 --> 00:03:54,110
More will be better.

63
00:03:54,110 --> 00:03:55,240
Less might work.

64
00:03:55,240 --> 00:03:58,990
But the ballpark of it is that you have
a factor of 10, in order to start

65
00:03:58,990 --> 00:04:03,050
getting interesting generalization
properties.

66
00:04:03,050 --> 00:04:07,020
We ended with summarizing the entire
theoretical analysis into a very

67
00:04:07,020 --> 00:04:11,110
simple bound, which we are referring
to as the generalization bound, that

68
00:04:11,110 --> 00:04:14,510
tells us a bound on the out-of-sample
performance given the in-sample

69
00:04:14,510 --> 00:04:16,060
performance.

70
00:04:16,060 --> 00:04:20,930
And that involved adding
a term, capital Omega.

71
00:04:20,930 --> 00:04:24,210
And Omega captures all the
theoretical analysis we had.

72
00:04:24,210 --> 00:04:26,920
It's a function of N, function of the
hypothesis set through the VC

73
00:04:26,920 --> 00:04:33,640
dimension, function of your tolerance
for probability of error, which is delta.

74
00:04:33,640 --> 00:04:38,860
And although this is a bound, we keep
saying that, in reality, E_out will

75
00:04:38,860 --> 00:04:42,240
be equal to E_in plus something
that behaves like Omega.

76
00:04:42,240 --> 00:04:44,750
And we will take advantage of that,
when we get a technique like

77
00:04:44,750 --> 00:04:46,090
regularization.

78
00:04:46,090 --> 00:04:50,060
So that's the end of the VC analysis,
which is the biggest part of the

79
00:04:50,060 --> 00:04:51,600
theory here.

80
00:04:51,600 --> 00:04:55,110
And we are going to switch today to
another approach, which is the

81
00:04:55,110 --> 00:04:57,060
bias-variance tradeoff.

82
00:04:57,060 --> 00:04:59,320
It's a stand-alone theory.

83
00:04:59,320 --> 00:05:02,340
It gives us a different angle
on generalization.

84
00:05:02,340 --> 00:05:06,540
And I am going to cover it, beginning
to end, during this lecture.

85
00:05:06,540 --> 00:05:09,060
This is the plan.

86
00:05:09,060 --> 00:05:10,540
The outline is very simple.

87
00:05:10,540 --> 00:05:13,610
We are going to talk about the bias
and variance, define them, see the

88
00:05:13,610 --> 00:05:18,530
tradeoff, take a very detailed example--
one particular example-- in

89
00:05:18,530 --> 00:05:21,950
order to demonstrate what the
bias and variance are.

90
00:05:21,950 --> 00:05:26,310
And then we are going to introduce
a very interesting tool for illustrating

91
00:05:26,310 --> 00:05:28,360
learning, which are called
learning curves.

92
00:05:28,360 --> 00:05:31,920
And we are going to contrast the
bias-variance analysis versus the VC

93
00:05:31,920 --> 00:05:35,270
analysis on these learning curves, and
then apply them to the linear

94
00:05:35,270 --> 00:05:37,460
regression case that we
are familiar with.

95
00:05:37,460 --> 00:05:38,486
So this is the plan.

96
00:05:38,486 --> 00:05:42,690
The first part is the
bias and variance.

97
00:05:42,690 --> 00:05:47,790
In the big picture, we have been trying
to characterize a tradeoff.

98
00:05:47,790 --> 00:05:52,740
And roughly speaking, the tradeoff
is between approximation and

99
00:05:52,740 --> 00:05:53,620
generalization.

100
00:05:53,620 --> 00:05:58,320
So let me discuss this for a moment,
before we put bias and variance into

101
00:05:58,320 --> 00:05:59,940
the picture.

102
00:05:59,940 --> 00:06:02,640
We would like to get to small E_out.

103
00:06:02,640 --> 00:06:03,960
That's the purpose of learning.

104
00:06:03,960 --> 00:06:05,970
If E_out is small, then
you have learned.

105
00:06:05,970 --> 00:06:10,060
You have a hypothesis that approximates
the target function well.

106
00:06:10,060 --> 00:06:14,340
There are two components to this, and
we are very familiar with them now.

107
00:06:14,340 --> 00:06:17,600
We are looking for a good
approximation of f.

108
00:06:17,600 --> 00:06:19,440
That's the approximation part.

109
00:06:19,440 --> 00:06:24,960
But we would like that approximation
to hold out-of-sample.

110
00:06:24,960 --> 00:06:29,910
We are not going to be happy if we
approximate f well in-sample, and we

111
00:06:29,910 --> 00:06:31,510
behave badly out-of-sample.

112
00:06:31,510 --> 00:06:34,870
These are the two components.

113
00:06:34,870 --> 00:06:41,550
In the case of a more complex
hypothesis set, you are going to have

114
00:06:41,550 --> 00:06:44,830
a better chance of approximating
f, obviously.

115
00:06:44,830 --> 00:06:46,400
I have more hypotheses to run around.

116
00:06:46,400 --> 00:06:49,220
I'll be able to find one of
them that is closer to the

117
00:06:49,220 --> 00:06:50,970
target function I want.

118
00:06:50,970 --> 00:06:57,020
The problem is that, if you have the
bigger hypothesis set, you are going

119
00:06:57,020 --> 00:07:01,620
to have a problem identifying
the good hypothesis.

120
00:07:01,620 --> 00:07:06,130
That is, if you have fewer hypotheses,
you have a better chance of

121
00:07:06,130 --> 00:07:07,160
generalization.

122
00:07:07,160 --> 00:07:10,010
And one way to look at it is
that, I'm trying to

123
00:07:10,010 --> 00:07:11,190
approximate the target function.

124
00:07:11,190 --> 00:07:13,120
You give me a hypothesis set.

125
00:07:13,120 --> 00:07:16,470
Now, if I tell you I have
good news for you.

126
00:07:16,470 --> 00:07:20,340
The target function is actually
in the hypothesis set.

127
00:07:20,340 --> 00:07:24,870
You have the perfect approximation
under your control.

128
00:07:24,870 --> 00:07:29,280
Well, it's under your hand, but not
necessarily under your control.

129
00:07:29,280 --> 00:07:32,670
Because you still have to navigate
through the hypothesis set in order to

130
00:07:32,670 --> 00:07:34,390
find the good candidate.

131
00:07:34,390 --> 00:07:37,650
And the way you navigate is
through the data set.

132
00:07:37,650 --> 00:07:42,360
That is your only resource for finding
one hypothesis versus the other.

133
00:07:42,360 --> 00:07:46,650
So the target function could be
sitting there calling for you.

134
00:07:46,650 --> 00:07:48,360
Please, I am the target
function, come.

135
00:07:48,360 --> 00:07:49,560
But you can't see it.

136
00:07:49,560 --> 00:07:52,770
You're just navigating with the training
set, you have very limited resources,

137
00:07:52,770 --> 00:07:54,880
and you end up with something
that is really bad.

138
00:07:54,880 --> 00:07:59,140


139
00:07:59,140 --> 00:08:01,850
Having f in the hypothesis set
is great for approximation.

140
00:08:01,850 --> 00:08:05,500
But having a big hypothesis set, that is
big enough to include that, may be

141
00:08:05,500 --> 00:08:09,080
bad news, because you will
not be able to get it.

142
00:08:09,080 --> 00:08:14,030
Now if you think about it, what is the
ideal hypothesis set for learning?

143
00:08:14,030 --> 00:08:18,100
If I only had a hypothesis set that has
a singleton hypothesis, which

144
00:08:18,100 --> 00:08:21,020
happens to be the target function, then
I have the best of both worlds.

145
00:08:21,020 --> 00:08:22,450
The perfect approximation.

146
00:08:22,450 --> 00:08:25,170
I will zoom in directly, because
it is only one.

147
00:08:25,170 --> 00:08:29,920
Well, you might as well go
and buy a lottery ticket.

148
00:08:29,920 --> 00:08:32,090
That's the equivalent.

149
00:08:32,090 --> 00:08:35,558
We don't know the target function, so
we will have to make the hypothesis set

150
00:08:35,558 --> 00:08:37,020
big enough to stand a chance.

151
00:08:37,020 --> 00:08:40,270
And once we do that, then the question
of generalization kicks in.

152
00:08:40,270 --> 00:08:41,340
This is this big picture.

153
00:08:41,340 --> 00:08:46,000
So let's try to fit the VC analysis in
it, and then fit the bias-variance

154
00:08:46,000 --> 00:08:49,730
analysis in it, before we even know what
the bias-variance analysis is, in

155
00:08:49,730 --> 00:08:52,740
order to see where we
are going with this.

156
00:08:52,740 --> 00:08:55,060
So we are quantifying this tradeoff.

157
00:08:55,060 --> 00:08:59,320
And the quantification, in the case
of the VC analysis, was what?

158
00:08:59,320 --> 00:09:02,760
Was the generalization bound.

159
00:09:02,760 --> 00:09:04,560


160
00:09:04,560 --> 00:09:05,990
E_in is approximation.

161
00:09:05,990 --> 00:09:09,850
Because I am actually trying to fit the
target function-- I am just fitting

162
00:09:09,850 --> 00:09:11,310
them on the sample.

163
00:09:11,310 --> 00:09:13,030
That's the restriction here.

164
00:09:13,030 --> 00:09:15,980
So if I get this well, then I'm
approximating f well, at

165
00:09:15,980 --> 00:09:18,130
least on some points.

166
00:09:18,130 --> 00:09:20,520
This guy is purely generalization.

167
00:09:20,520 --> 00:09:24,750
The question is, how do you generalize
from in-sample to out-of-sample?

168
00:09:24,750 --> 00:09:27,530
So this is a way of quantifying it.

169
00:09:27,530 --> 00:09:30,950
Now the bias-variance analysis
has another approach.

170
00:09:30,950 --> 00:09:36,850
It also decomposes E_out, as you did
in the generalization bound.

171
00:09:36,850 --> 00:09:41,730
But it does decompose it into
two different entities.

172
00:09:41,730 --> 00:09:46,590
The first one is an approximation
entity, how well H can approximate f.

173
00:09:46,590 --> 00:09:48,580
Well, what is the difference then?

174
00:09:48,580 --> 00:09:51,680
The difference is that the bias-variance
asks the question, how

175
00:09:51,680 --> 00:09:55,170
can H approximate f, overall?

176
00:09:55,170 --> 00:10:00,600
Not on your sample. In reality. As if
you had access to the target function,

177
00:10:00,600 --> 00:10:04,800
and you are stuck with this hypothesis
set, and you are eagerly looking for

178
00:10:04,800 --> 00:10:08,240
which hypothesis best describes
the target function.

179
00:10:08,240 --> 00:10:13,370
And then you quantify how well that best
hypothesis performs, and that is

180
00:10:13,370 --> 00:10:17,270
your measure of the approximation
ability.

181
00:10:17,270 --> 00:10:19,650
Then what is the other component?

182
00:10:19,650 --> 00:10:22,490
The other component is exactly
what I alluded to.

183
00:10:22,490 --> 00:10:25,660
Can you zoom in on it?

184
00:10:25,660 --> 00:10:30,130
So this is the best hypothesis, and it
has a certain approximation ability.

185
00:10:30,130 --> 00:10:34,230
Now I need to pick it, so I have to use
the examples in order to zoom in

186
00:10:34,230 --> 00:10:37,080
into the hypothesis set, and
pick this particular one.

187
00:10:37,080 --> 00:10:38,610
Can I zoom in on it?

188
00:10:38,610 --> 00:10:44,180
Or do I get something that is a poor
approximation of the approximation?

189
00:10:44,180 --> 00:10:46,600
And that decomposition will
give us the bias-variance.

190
00:10:46,600 --> 00:10:49,930
And we'll be able to put them at the
end of the lecture, side by side, in

191
00:10:49,930 --> 00:10:53,020
order to compare: here is what the VC
analysis does, and here is what

192
00:10:53,020 --> 00:10:54,270
bias-variance does.

193
00:10:54,270 --> 00:10:59,080


194
00:10:59,080 --> 00:11:01,330
The analysis, from a mathematical
point of view, applies

195
00:11:01,330 --> 00:11:02,760
to real-valued targets.

196
00:11:02,760 --> 00:11:03,570
And that's good news.

197
00:11:03,570 --> 00:11:08,210
Because remember, in the VC analysis, we
were confined to binary functions

198
00:11:08,210 --> 00:11:10,020
in the particular analysis that I did.

199
00:11:10,020 --> 00:11:12,340
You can extend it, but
it's very technical.

200
00:11:12,340 --> 00:11:16,050
So it's a good idea to see the same
tradeoff, and the same generalization

201
00:11:16,050 --> 00:11:18,180
questions, apply to real-valued functions.

202
00:11:18,180 --> 00:11:21,710
Now we have regression, and we are
able to make a statement about

203
00:11:21,710 --> 00:11:25,820
generalization on regression, which we
will apply very specifically to linear

204
00:11:25,820 --> 00:11:32,060
regression, the model that we already
studied that has real-valued outputs.

205
00:11:32,060 --> 00:11:37,790
And we are going to confine the analysis
here to squared error.

206
00:11:37,790 --> 00:11:41,930
The reason we are doing this is that, for
the math to go through in such

207
00:11:41,930 --> 00:11:47,210
a way that these two guys decompose
cleanly-- there are no cross terms, we

208
00:11:47,210 --> 00:11:49,480
will need the squared error.

209
00:11:49,480 --> 00:11:51,770
So this is a restriction
of the analysis.

210
00:11:51,770 --> 00:11:53,240
There are ways to extend it.

211
00:11:53,240 --> 00:11:56,150
They are not as clean, so this
is the simplest form that we

212
00:11:56,150 --> 00:11:57,270
are going to use.

213
00:11:57,270 --> 00:11:57,560


214
00:11:57,560 --> 00:12:00,050
Let's start.

215
00:12:00,050 --> 00:12:01,670
Our starting point is E_out.

216
00:12:01,670 --> 00:12:02,920
So let me put it--

217
00:12:02,920 --> 00:12:05,360


218
00:12:05,360 --> 00:12:06,640
Don't worry about the gap.

219
00:12:06,640 --> 00:12:09,250
The gap here will be filled.

220
00:12:09,250 --> 00:12:10,095
What do we have?

221
00:12:10,095 --> 00:12:11,250
We have E_out.

222
00:12:11,250 --> 00:12:13,280
E_out depends on the hypothesis
you pick.

223
00:12:13,280 --> 00:12:15,710
E_out is E_out of your
final hypothesis.

224
00:12:15,710 --> 00:12:19,690
How does it perform on
the overall space?

225
00:12:19,690 --> 00:12:23,150
And in order to do that, since we are
talking about squared error, you are

226
00:12:23,150 --> 00:12:26,990
going to take the value of your
hypothesis, and compare it to the value

227
00:12:26,990 --> 00:12:28,700
of the target function,
and take that squared.

228
00:12:28,700 --> 00:12:30,480
And that will be your error.

229
00:12:30,480 --> 00:12:34,910
So this is the building block for
getting the out-of-sample performance.

230
00:12:34,910 --> 00:12:39,660
Now the gap here comes from the fact
that, if you look at the final

231
00:12:39,660 --> 00:12:43,670
hypothesis, the final hypothesis
depends on a number of things.

232
00:12:43,670 --> 00:12:47,080
Among other things, it does depend
on the data set that I'm

233
00:12:47,080 --> 00:12:49,550
going to give you, right?

234
00:12:49,550 --> 00:12:52,760
Because if I give you a different data
set, you'll find a different final

235
00:12:52,760 --> 00:12:54,220
hypothesis.

236
00:12:54,220 --> 00:12:58,190
That dependency is quite important
in the bias-variance analysis.

237
00:12:58,190 --> 00:13:00,980
Therefore, I am going to make
it explicit in the notation.

238
00:13:00,980 --> 00:13:05,100
It has always been there, but I didn't
need to carry ugly notation throughout,

239
00:13:05,100 --> 00:13:06,170
when I'm not using it.

240
00:13:06,170 --> 00:13:08,770
Here I'm using it, so we'll
have to live with it.

241
00:13:08,770 --> 00:13:12,180
So now I'll make that
dependency explicit.

242
00:13:12,180 --> 00:13:16,530
I'm having now a superscript, which
tells me that this g comes from that

243
00:13:16,530 --> 00:13:18,120
particular data set.

244
00:13:18,120 --> 00:13:20,940
If you give me another data set, this
will be a different g.

245
00:13:20,940 --> 00:13:26,280
And you take the same g, apply it to x,
compare it to f, and this is your error.

246
00:13:26,280 --> 00:13:29,740
And finally, in order for it to be
genuinely out-of-sample error, you

247
00:13:29,740 --> 00:13:33,730
need to get the expected value of that
error over the entire space.

248
00:13:33,730 --> 00:13:35,640
So this is what we have.

249
00:13:35,640 --> 00:13:39,720
Now what we would like to do, we would
like to see a decomposition of this

250
00:13:39,720 --> 00:13:44,780
quantity into the two conceptual
components, approximation and

251
00:13:44,780 --> 00:13:46,740
generalization, that we saw.

252
00:13:46,740 --> 00:13:50,270
So here's what we are going to do.

253
00:13:50,270 --> 00:13:55,170
We are going to take this quantity,
which equals this quantity, as I

254
00:13:55,170 --> 00:13:59,510
mentioned here, and then realize
that this depends on the

255
00:13:59,510 --> 00:14:02,420
particular data set.

256
00:14:02,420 --> 00:14:07,420
I would like to rid this from the
dependency on the specific data set

257
00:14:07,420 --> 00:14:08,710
that I give you.

258
00:14:08,710 --> 00:14:11,160
So I'm going to play
the following game.

259
00:14:11,160 --> 00:14:16,110
I am going to give you a budget of
N examples, training examples

260
00:14:16,110 --> 00:14:18,350
to learn from.

261
00:14:18,350 --> 00:14:22,360
If I give you that budget N, I could
generate one D and another D and

262
00:14:22,360 --> 00:14:25,620
another D, each of them
with N examples.

263
00:14:25,620 --> 00:14:30,220
Each of them will result in a different
hypothesis g, and each of

264
00:14:30,220 --> 00:14:33,180
them will result in a different
out-of-sample error.

265
00:14:33,180 --> 00:14:34,460
Correct?

266
00:14:34,460 --> 00:14:37,860
So if I want to get rid of the
dependency on the particular sample

267
00:14:37,860 --> 00:14:41,640
that I give you, and just know the
behavior-- if I give you N data points,

268
00:14:41,640 --> 00:14:45,720
what happens?-- then I would
like to integrate D out.

269
00:14:45,720 --> 00:14:51,606
So I am going to get the expected value
of that error, with respect to D.

270
00:14:51,606 --> 00:14:53,900
This is not a quantity that
you are going to encounter

271
00:14:53,900 --> 00:14:55,110
in any given situation.

272
00:14:55,110 --> 00:14:59,280
In any given situation, you have
a specific data set to work with.

273
00:14:59,280 --> 00:15:03,020
However, if I want to analyze the
general behavior--

274
00:15:03,020 --> 00:15:06,410
someone comes to my door, how many
examples do you have, and they tell me

275
00:15:06,410 --> 00:15:08,930
100. I haven't seen the examples yet.

276
00:15:08,930 --> 00:15:12,350
So it stands to logic that I say,
for 100 examples, the following

277
00:15:12,350 --> 00:15:13,370
behavior follows.

278
00:15:13,370 --> 00:15:16,550
So I must be taking an expected value
with respect to all possible

279
00:15:16,550 --> 00:15:18,770
realizations of 100 examples.

280
00:15:18,770 --> 00:15:20,910
And that is, indeed, what
I am going to do.

281
00:15:20,910 --> 00:15:22,670
I'm going to get the expected
value of that.

282
00:15:22,670 --> 00:15:25,350
And this is the quantity that
I am going to decompose.

283
00:15:25,350 --> 00:15:28,670
And this obviously happens to be the
expected value of the other guy, and

284
00:15:28,670 --> 00:15:29,920
we have that.

285
00:15:29,920 --> 00:15:32,200


286
00:15:32,200 --> 00:15:36,260
Now I am going to take this quantity,
the expression for the quantity that

287
00:15:36,260 --> 00:15:40,870
I'm interested in, and keep deriving
stuff until I get to the

288
00:15:40,870 --> 00:15:42,690
decomposition I want.

289
00:15:42,690 --> 00:15:46,020
The first order of business,
I have two expectations.

290
00:15:46,020 --> 00:15:48,600
The first thing I'm going to do, I am
going to reverse the order of the

291
00:15:48,600 --> 00:15:51,310
expectations.

292
00:15:51,310 --> 00:15:52,830
Why can I do that?

293
00:15:52,830 --> 00:15:54,160
I am integrating.

294
00:15:54,160 --> 00:15:55,750
So now I change the order
of integration.

295
00:15:55,750 --> 00:16:01,150
I am allowed to do that, because the
integrand is strictly non-negative.

296
00:16:01,150 --> 00:16:02,520
So I get this.

297
00:16:02,520 --> 00:16:05,345
And the reason for that is because
I am really interested in the

298
00:16:05,345 --> 00:16:09,020
expectation with respect to D, and I'd
rather not carry the expectation with

299
00:16:09,020 --> 00:16:10,710
respect to x throughout.

300
00:16:10,710 --> 00:16:14,170
So I am going to get rid of that
expectation for a while, until I get

301
00:16:14,170 --> 00:16:15,430
a clean decomposition.

302
00:16:15,430 --> 00:16:17,920
And when I get the clean decomposition,
I'll go back and get

303
00:16:17,920 --> 00:16:21,050
the expectation, just to
keep the focus clear.

304
00:16:21,050 --> 00:16:24,540


305
00:16:24,540 --> 00:16:27,530
You focus on the inside quantity.

306
00:16:27,530 --> 00:16:30,920
If I give you the expression for the
inside quantity for any x, then all

307
00:16:30,920 --> 00:16:33,310
you need to do in order to get the
quantity that you need, is get the

308
00:16:33,310 --> 00:16:36,270
expected value of what I
said, with respect to x.

309
00:16:36,270 --> 00:16:40,000
So this is the quantity that we are
going to carry to the next slide.

310
00:16:40,000 --> 00:16:42,350
Let's do that.

311
00:16:42,350 --> 00:16:46,690
And the main notion, in order to evaluate
this quantity, is the notion

312
00:16:46,690 --> 00:16:48,210
of an average hypothesis.

313
00:16:48,210 --> 00:16:50,880
It's a pretty interesting idea.

314
00:16:50,880 --> 00:16:53,530
Here is the idea.

315
00:16:53,530 --> 00:16:58,400
You have a hypothesis set, and you are
learning from a particular data set.

316
00:16:58,400 --> 00:17:02,120
And I am going to define
a particular hypothesis.

317
00:17:02,120 --> 00:17:04,618
I am going to call it the
average hypothesis.

318
00:17:04,618 --> 00:17:08,319
And because it's average, I am going
to give it a bar notation.

319
00:17:08,319 --> 00:17:10,790
So what is this fellow?

320
00:17:10,790 --> 00:17:15,410
Well, this fellow is
defined as follows.

321
00:17:15,410 --> 00:17:17,230
You learn from a data set.

322
00:17:17,230 --> 00:17:18,319
You get a hypothesis.

323
00:17:18,319 --> 00:17:20,150
Someone else learns from
another data set.

324
00:17:20,150 --> 00:17:22,480
They get another hypothesis, et cetera.

325
00:17:22,480 --> 00:17:26,848
So how about getting the expected
value of these hypotheses?

326
00:17:26,848 --> 00:17:28,900
What does that formally mean?

327
00:17:28,900 --> 00:17:30,220
We have x fixed.

328
00:17:30,220 --> 00:17:34,860
So we actually are in a good position,
because g of x is really just a random

329
00:17:34,860 --> 00:17:35,920
variable at this point.

330
00:17:35,920 --> 00:17:39,250
It's a random variable, determined
by the choice of your data.

331
00:17:39,250 --> 00:17:41,080
The data is the randomization source.

332
00:17:41,080 --> 00:17:44,480
x is fixed, so you think I have one
test point in the space, that I'm

333
00:17:44,480 --> 00:17:46,130
interested in.

334
00:17:46,130 --> 00:17:49,190
Maybe you are playing the stock market,
and now you are only interested

335
00:17:49,190 --> 00:17:50,600
in what's going to happen tomorrow.

336
00:17:50,600 --> 00:17:53,360
So you take the inputs, and these are
the only inputs you're interested in

337
00:17:53,360 --> 00:17:54,320
performing on.

338
00:17:54,320 --> 00:17:55,770
That's your x.

339
00:17:55,770 --> 00:17:57,610
And all of the questions
now pertain to this.

340
00:17:57,610 --> 00:17:58,650
You are learning from other data.

341
00:17:58,650 --> 00:18:00,920
And then you ask yourself, how am
I performing on this point?

342
00:18:00,920 --> 00:18:02,880
That is the point x.

343
00:18:02,880 --> 00:18:03,900
Now you are looking at this point.

344
00:18:03,900 --> 00:18:06,740
And you say, if you give me a data set
versus another, I am going to

345
00:18:06,740 --> 00:18:10,510
get different values for the
hypothesis on that point.

346
00:18:10,510 --> 00:18:14,280
It stands to logic that, if I take the
average with respect to all possible

347
00:18:14,280 --> 00:18:16,790
data sets, that would be awesome.

348
00:18:16,790 --> 00:18:20,940
Because now I am getting the benefit
of an infinite number of data sets.

349
00:18:20,940 --> 00:18:24,040
I am using them in the capacity
of one data set at a time.

350
00:18:24,040 --> 00:18:25,000
But I am getting value.

351
00:18:25,000 --> 00:18:27,030
Maybe the correct value
should be here.

352
00:18:27,030 --> 00:18:30,070
But since I am getting fluctuations
because of the data set, sometimes I'm

353
00:18:30,070 --> 00:18:31,670
here, sometimes I'm here, et cetera.

354
00:18:31,670 --> 00:18:34,740
If you get the expected value,
you will get it right.

355
00:18:34,740 --> 00:18:36,610
So this looks like a great
quantity to have.

356
00:18:36,610 --> 00:18:39,340
And in reality, we will
never have that.

357
00:18:39,340 --> 00:18:41,610
Because if you give me an infinite
number of examples, I'm not going to

358
00:18:41,610 --> 00:18:45,500
divide them neatly into N and N
and N, and learn from these, and

359
00:18:45,500 --> 00:18:46,230
then take the average.

360
00:18:46,230 --> 00:18:48,860
I'm just going to take all your examples,
and learn all through and get

361
00:18:48,860 --> 00:18:51,570
the target function almost perfectly.

362
00:18:51,570 --> 00:18:54,550
So this is just a conceptual tool
for us to do the analysis.

363
00:18:54,550 --> 00:18:55,850
But we understand what it is.

364
00:18:55,850 --> 00:18:56,440


365
00:18:56,440 --> 00:19:00,800
If you now vary x, your test 
point in general, then you take that

366
00:19:00,800 --> 00:19:04,050
random variable and the expected value
of it, and the function that is

367
00:19:04,050 --> 00:19:09,640
constituted by the expected values at
different points is your g bar.

368
00:19:09,640 --> 00:19:10,970
So this is understood.

369
00:19:10,970 --> 00:19:12,540
Why do I need this for the analysis?

370
00:19:12,540 --> 00:19:15,950
Because if you look at the top thing,
I have here squared, so I'm

371
00:19:15,950 --> 00:19:18,910
probably going to expand it.

372
00:19:18,910 --> 00:19:22,750
And in expanding it, I am just going
to get a linear term of this.

373
00:19:22,750 --> 00:19:24,300
And I have an expected value.

374
00:19:24,300 --> 00:19:27,220
So you can see that I'm going to
get something that requires me

375
00:19:27,220 --> 00:19:28,910
to define g bar.

376
00:19:28,910 --> 00:19:31,010
That's the technical utility here.

377
00:19:31,010 --> 00:19:33,210
But the conceptual utility
is very important.

378
00:19:33,210 --> 00:19:36,460
And if you want to tell someone
what g bar is, think that you

379
00:19:36,460 --> 00:19:38,220
have many, many data sets.

380
00:19:38,220 --> 00:19:41,630
And the game is such that you learn from
one data set at a time, and you

381
00:19:41,630 --> 00:19:43,580
want to make the most of
it after you learn.

382
00:19:43,580 --> 00:19:44,210
What do you do?

383
00:19:44,210 --> 00:19:45,040
You take votes.

384
00:19:45,040 --> 00:19:46,330
You take just the average.

385
00:19:46,330 --> 00:19:47,870
You have this.

386
00:19:47,870 --> 00:19:47,890


387
00:19:47,890 --> 00:19:52,550
There is 1 over K here,
the size of those.

388
00:19:52,550 --> 00:19:53,800
So this is the average.

389
00:19:53,800 --> 00:19:56,210


390
00:19:56,210 --> 00:19:59,480
Now let's see how we can use
g bar, in order to get the

391
00:19:59,480 --> 00:20:00,740
decomposition we want.

392
00:20:00,740 --> 00:20:04,730


393
00:20:04,730 --> 00:20:08,970
Here, this is again the quantity I'm
passing from one slide to another,

394
00:20:08,970 --> 00:20:10,390
in order not to forget.

395
00:20:10,390 --> 00:20:12,550
This is the quantity that I'd like
to decompose.

396
00:20:12,550 --> 00:20:17,200
The first thing I am going to do, I am
going to make it longer, by the simple

397
00:20:17,200 --> 00:20:21,260
trick of adding g bar
and subtracting it.

398
00:20:21,260 --> 00:20:23,010
I'm allowed to do that, right?

399
00:20:23,010 --> 00:20:23,800


400
00:20:23,800 --> 00:20:27,890
Doing that, I am going to
consolidate these two terms,

401
00:20:27,890 --> 00:20:31,990
and I'm going to consolidate
these two terms. And then

402
00:20:31,990 --> 00:20:33,280
expand with the squared.

403
00:20:33,280 --> 00:20:36,180
So let's do that.

404
00:20:36,180 --> 00:20:37,450
You get this.

405
00:20:37,450 --> 00:20:42,040
This is the first consolidated
guy with the squared.

406
00:20:42,040 --> 00:20:44,930
This is the second consolidated
guy with the squared.

407
00:20:44,930 --> 00:20:46,590
Am I missing something?

408
00:20:46,590 --> 00:20:48,760
Yes, I am missing the cross terms.

409
00:20:48,760 --> 00:20:51,170
So let's add the cross terms.

410
00:20:51,170 --> 00:20:53,760
And I get twice the product.

411
00:20:53,760 --> 00:20:57,360


412
00:20:57,360 --> 00:20:58,610
This equals that.

413
00:20:58,610 --> 00:21:01,180


414
00:21:01,180 --> 00:21:06,020
So the expected value here applies
to the whole thing.

415
00:21:06,020 --> 00:21:08,780
The first order of business is to look
at the cross terms, because they are

416
00:21:08,780 --> 00:21:12,090
annoying, and see if I
can get rid of them.

417
00:21:12,090 --> 00:21:18,070
That's where the benefit of
the squared error comes in.

418
00:21:18,070 --> 00:21:22,390
I am getting the expected value
with respect to D, right?

419
00:21:22,390 --> 00:21:28,620
So this fellow is a constant.

420
00:21:28,620 --> 00:21:32,470
Therefore, when I get the expected value
of this whole thing, all I need

421
00:21:32,470 --> 00:21:35,540
to do is get the expected value of
this part, because this one will

422
00:21:35,540 --> 00:21:37,730
factor out.

423
00:21:37,730 --> 00:21:40,660
Now, if I get the expected value of this,
the expected value of the sum is

424
00:21:40,660 --> 00:21:44,980
the sum of the expected values-- one of
the few universal rules that you can

425
00:21:44,980 --> 00:21:47,360
apply, without asking any
other questions.

426
00:21:47,360 --> 00:21:49,080
So I get the expected value of this.

427
00:21:49,080 --> 00:21:52,030
What is the expected value of g^D?

428
00:21:52,030 --> 00:21:52,850
Wait a minute.

429
00:21:52,850 --> 00:21:54,340
That was g bar, by definition.

430
00:21:54,340 --> 00:21:56,260
That's how we defined it, right?

431
00:21:56,260 --> 00:21:59,950
So I get g bar, minus the expected
value of a constant, which

432
00:21:59,950 --> 00:22:01,015
happens to be g bar.

433
00:22:01,015 --> 00:22:06,280
So this goes to 0, and happily
this guy goes away.

434
00:22:06,280 --> 00:22:07,740
Now I have only these two guys.

435
00:22:07,740 --> 00:22:11,880
So let's write them.

436
00:22:11,880 --> 00:22:15,450
I have the expected value of this whole
thing, which again is the sum

437
00:22:15,450 --> 00:22:17,720
of the expected values.

438
00:22:17,720 --> 00:22:21,150
The first guy is a genuine expected
value of these guys.

439
00:22:21,150 --> 00:22:24,840
When I apply expected value to this
guy, again this is just a constant, so

440
00:22:24,840 --> 00:22:27,660
the expected value of it is itself.

441
00:22:27,660 --> 00:22:30,950
The second guy I add without bothering
with the expected value, because it's

442
00:22:30,950 --> 00:22:32,410
just a constant.

443
00:22:32,410 --> 00:22:34,970
So this is what I have as
the expression for the

444
00:22:34,970 --> 00:22:37,350
quantity that I want.

445
00:22:37,350 --> 00:22:40,520
Now let's take this and look at it
closely, because this will be the bias

446
00:22:40,520 --> 00:22:41,770
and variance.

447
00:22:41,770 --> 00:22:44,930


448
00:22:44,930 --> 00:22:50,960
This is the quantity again,
and it equals this fellow.

449
00:22:50,960 --> 00:22:54,710
Now let's look beyond the math,
and understand what's going on.

450
00:22:54,710 --> 00:23:02,520
This measure-- this quantity-- tells you
how far your hypothesis, that you got

451
00:23:02,520 --> 00:23:06,540
from learning on a particular
data set, differs from the

452
00:23:06,540 --> 00:23:10,070
ultimate thing, the target.

453
00:23:10,070 --> 00:23:13,230
And we are decomposing
this into two steps.

454
00:23:13,230 --> 00:23:18,800
The first step is to ask you, how far
is your hypothesis that you got from

455
00:23:18,800 --> 00:23:24,950
that particular data set, from the best
possible you can get using your

456
00:23:24,950 --> 00:23:27,890
hypothesis set?

457
00:23:27,890 --> 00:23:33,240
Now there is a leap here, because
I don't know whether this is

458
00:23:33,240 --> 00:23:34,610
the best in the hypothesis set.

459
00:23:34,610 --> 00:23:35,150
I got it by the averaging.

460
00:23:35,150 --> 00:23:38,840
But since I'm averaging from several
data sets, it looks like a pretty good

461
00:23:38,840 --> 00:23:39,920
hypothesis.

462
00:23:39,920 --> 00:23:42,770
I am not even sure that it's actually
in the hypothesis set.

463
00:23:42,770 --> 00:23:45,790
It's the average of guys that came
from the hypothesis set.

464
00:23:45,790 --> 00:23:49,090
But I can definitely construct
hypothesis sets, where the average of

465
00:23:49,090 --> 00:23:51,240
hypotheses does not necessarily
belong there.

466
00:23:51,240 --> 00:23:52,860
So there are some funny stuff.

467
00:23:52,860 --> 00:23:56,150
But just think of it that, this
is an intermediate step.

468
00:23:56,150 --> 00:23:58,420
Instead of going all the way to the
target function, here is your

469
00:23:58,420 --> 00:23:59,460
hypothesis set.

470
00:23:59,460 --> 00:24:01,170
It restricts your resources.

471
00:24:01,170 --> 00:24:03,870
Now I am getting the best possible
out of it, based on some formula.

472
00:24:03,870 --> 00:24:06,940
I'm learning from infinite
number of data sets.

473
00:24:06,940 --> 00:24:08,300
This is a pretty good hypothesis.

474
00:24:08,300 --> 00:24:10,030
So how far are you from
that hypothesis?

475
00:24:10,030 --> 00:24:11,710
That's the first step.

476
00:24:11,710 --> 00:24:17,520
The second step is how far that
hypothesis, that great hypothesis, is

477
00:24:17,520 --> 00:24:19,980
from the ultimate target function.

478
00:24:19,980 --> 00:24:26,630
So hopping from your guy to the target,
goes into a small hop from your guy to

479
00:24:26,630 --> 00:24:29,800
the best hypothesis, and another
hop from the best hypothesis

480
00:24:29,800 --> 00:24:31,010
to the target function.

481
00:24:31,010 --> 00:24:33,230
And the neat thing is that they
decomposed cleanly.

482
00:24:33,230 --> 00:24:35,340
And we found that they decomposed
cleanly because the cross term

483
00:24:35,340 --> 00:24:36,190
disappeared.

484
00:24:36,190 --> 00:24:39,900
That's the advantage of the particular
measure that we have.

485
00:24:39,900 --> 00:24:43,460
Now we need to give names
to these guys.

486
00:24:43,460 --> 00:24:45,050
They will be the bias and variance.

487
00:24:45,050 --> 00:24:48,780
I'd like you to think for five seconds,
and you don't have to even

488
00:24:48,780 --> 00:24:49,410
answer the question.

489
00:24:49,410 --> 00:24:51,530
Which will be the bias, and which
will be the variance?

490
00:24:51,530 --> 00:24:54,220
Just look at which would be
a better description to them.

491
00:24:54,220 --> 00:24:56,810


492
00:24:56,810 --> 00:24:57,500
I'm not going to ask.

493
00:24:57,500 --> 00:25:00,330
This is not a quiz, like last time.

494
00:25:00,330 --> 00:25:01,010
This is the bias.

495
00:25:01,010 --> 00:25:01,940
Why is it the bias?

496
00:25:01,940 --> 00:25:06,410
Because what I'm saying is that,
learning or no learning, your

497
00:25:06,410 --> 00:25:10,110
hypothesis set is biased away
from the target function.

498
00:25:10,110 --> 00:25:13,120
Because this is the best I could
do, under fictitious scenario.

499
00:25:13,120 --> 00:25:15,820
You have infinite data sets, and you
are doing all of this, and you're

500
00:25:15,820 --> 00:25:18,210
taking the average, and that's the
best you could come up with.

501
00:25:18,210 --> 00:25:20,090
And you are still far away
from the target.

502
00:25:20,090 --> 00:25:23,300
So it must be a limitation
of your hypothesis set.

503
00:25:23,300 --> 00:25:27,090
I'm going to measure that limitation and
say that your hypothesis set, which

504
00:25:27,090 --> 00:25:30,880
is represented at its best by
this guy, is biased away

505
00:25:30,880 --> 00:25:33,180
from the target function.

506
00:25:33,180 --> 00:25:34,690
So this is the bias term.

507
00:25:34,690 --> 00:25:38,520
And again, bias applies to that
particular point x, the test point in

508
00:25:38,520 --> 00:25:41,230
the input space that
I'm interested in.

509
00:25:41,230 --> 00:25:43,810
The other guy must be the variance.

510
00:25:43,810 --> 00:25:44,970
Why is that?

511
00:25:44,970 --> 00:25:49,380
Because if I knew everything, if I could
zoom in perfectly, I would zoom

512
00:25:49,380 --> 00:25:52,020
in onto the best, assuming
this is there,

513
00:25:52,020 --> 00:25:54,260
so I have this guy. But you don't.

514
00:25:54,260 --> 00:25:56,330
You have one data set at a time.

515
00:25:56,330 --> 00:25:58,220
When you get one data set,
you get this guy.

516
00:25:58,220 --> 00:25:59,950
You get another data set,
you get another guy.

517
00:25:59,950 --> 00:26:01,280
These are different from that.

518
00:26:01,280 --> 00:26:04,950
So you are away from that, and I'm
measuring how far you are away.

519
00:26:04,950 --> 00:26:07,840
But because the expected value
of this fellow is g bar.

520
00:26:07,840 --> 00:26:10,960
And I am comparing the difference
squared with this.

521
00:26:10,960 --> 00:26:12,800
It is properly called variance.

522
00:26:12,800 --> 00:26:17,480
This is the variance of what I am
getting, due to the fact that I get

523
00:26:17,480 --> 00:26:18,780
a finite data set.

524
00:26:18,780 --> 00:26:23,110
Every time I get a data set, I get
a different one, and I am measuring the

525
00:26:23,110 --> 00:26:26,370
distance from the core
that I get here.

526
00:26:26,370 --> 00:26:30,100
So this we call the bias, and
this we call the variance.

527
00:26:30,100 --> 00:26:31,110
This is very clean.

528
00:26:31,110 --> 00:26:35,610
Now let's go back, and put it
into the original form.

529
00:26:35,610 --> 00:26:36,710
Remember this guy?

530
00:26:36,710 --> 00:26:38,640
This is where we started.

531
00:26:38,640 --> 00:26:42,310
We got the other expression, and then
we neglected to take the expected

532
00:26:42,310 --> 00:26:44,910
value with respect to x, in order
to simplify the analysis.

533
00:26:44,910 --> 00:26:48,460
We would like to get that back,
so we'll look at this.

534
00:26:48,460 --> 00:26:54,130
This was the expected value, with respect
to x, of the quantity we just

535
00:26:54,130 --> 00:26:56,220
decomposed.

536
00:26:56,220 --> 00:27:00,900
Now I take the decomposition and put it
back, in order to get the expected

537
00:27:00,900 --> 00:27:04,570
value of the out-of-sample error, in
terms of the bias and variance.

538
00:27:04,570 --> 00:27:05,720
So this will be what?

539
00:27:05,720 --> 00:27:09,540
This will be the expected value with
respect to x, of bias plus variance

540
00:27:09,540 --> 00:27:11,370
with x.

541
00:27:11,370 --> 00:27:14,610
And the expected value of the bias with
respect to x, I'm just going to

542
00:27:14,610 --> 00:27:16,120
call it bias.

543
00:27:16,120 --> 00:27:20,550
The expected here, I'm going to call it
variance, and that's what

544
00:27:20,550 --> 00:27:21,440
you get.

545
00:27:21,440 --> 00:27:24,990
And this is the bias-variance
decomposition.

546
00:27:24,990 --> 00:27:29,750
Now I have a single number that
describes the expected out-of-sample.

547
00:27:29,750 --> 00:27:31,780
So I give you a full learning situation.

548
00:27:31,780 --> 00:27:35,860
I give you a target function, and
an input distribution, and a hypothesis

549
00:27:35,860 --> 00:27:37,510
set, and a learning algorithm.

550
00:27:37,510 --> 00:27:39,220
And you have all the components.

551
00:27:39,220 --> 00:27:41,830
You go about, and learn
for every data set.

552
00:27:41,830 --> 00:27:44,400
And you get-- someone else learned
from another data set.

553
00:27:44,400 --> 00:27:46,470
And get the expected value of
the out-of-sample error.

554
00:27:46,470 --> 00:27:50,130
And I'm telling you if this
out-of-sample error is 0.3, well, 0.05

555
00:27:50,130 --> 00:27:55,500
of it is because of bias, and
0.25 is because of variance.

556
00:27:55,500 --> 00:27:59,180
So 0.05 means that your hypothesis set
is pretty good in approximation, but

557
00:27:59,180 --> 00:27:59,970
maybe it's too big.

558
00:27:59,970 --> 00:28:03,150
Therefore, you have a lot of
variance, which is 0.25.

559
00:28:03,150 --> 00:28:06,420
This is the decomposition.

560
00:28:06,420 --> 00:28:11,340
Now let's look at the tradeoff of
generalization versus approximation, in

561
00:28:11,340 --> 00:28:12,700
terms of this decomposition.

562
00:28:12,700 --> 00:28:13,950
That was the purpose.

563
00:28:13,950 --> 00:28:16,530


564
00:28:16,530 --> 00:28:20,530
Here is the bias, explicitly
written as a formula.

565
00:28:20,530 --> 00:28:24,590
And here is the variance.

566
00:28:24,590 --> 00:28:27,310
We would like to argue that there is
a tradeoff, that when you change your

567
00:28:27,310 --> 00:28:30,460
hypothesis set-- you make it bigger,
more complex, or smaller.

568
00:28:30,460 --> 00:28:33,790
One of these guys goes up, and
one of these guys goes down.

569
00:28:33,790 --> 00:28:35,910
So I'm going to argue
about it informally.

570
00:28:35,910 --> 00:28:38,930
And then we'll take a specific
example, where we are going

571
00:28:38,930 --> 00:28:40,530
to get exact numbers.

572
00:28:40,530 --> 00:28:43,930
But this is just to realize
that this decomposition actually

573
00:28:43,930 --> 00:28:47,190
captures the tradeoff of approximation
versus generalization.

574
00:28:47,190 --> 00:28:49,630
Why is that?

575
00:28:49,630 --> 00:28:51,750
Let's look at this picture.

576
00:28:51,750 --> 00:28:55,370
Here, I have a small hypothesis set.

577
00:28:55,370 --> 00:28:59,140
One function, if you want, but, in
general, let's call it small.

578
00:28:59,140 --> 00:29:02,520
This one, I have a huge
hypothesis set.

579
00:29:02,520 --> 00:29:06,030
So I have here the black points are
hypotheses, that are candidates.

580
00:29:06,030 --> 00:29:09,870
Someone gives me a data set, and
I learn, and choose something.

581
00:29:09,870 --> 00:29:12,390
Now the target function
is sitting here.

582
00:29:12,390 --> 00:29:16,930
If I use this guy, obviously I am far
away from the target function.

583
00:29:16,930 --> 00:29:19,885
And therefore, the bias is big.

584
00:29:19,885 --> 00:29:22,650
If I have a big hypothesis set-- this
is big enough that it actually

585
00:29:22,650 --> 00:29:24,220
includes the f.

586
00:29:24,220 --> 00:29:27,610
Then when I learn, on average,
I would be very close to f.

587
00:29:27,610 --> 00:29:30,960
Maybe I won't hit f exactly, because
of the nonlinearity of the regime.

588
00:29:30,960 --> 00:29:35,480
The regime, I get N examples, learn and
keep it, another N example, learn

589
00:29:35,480 --> 00:29:36,850
and keep it, and then
take the average.

590
00:29:36,850 --> 00:29:38,840
I might have lost some because
of the nonlinearity.

591
00:29:38,840 --> 00:29:41,540
I might not get f, but I'll
get pretty close.

592
00:29:41,540 --> 00:29:44,800
So the bias here is very,
very small, close to 0.

593
00:29:44,800 --> 00:29:46,940
In terms of the variance here,
there is no variance.

594
00:29:46,940 --> 00:29:49,600
If I have one target function, I don't
care what data set you give me.

595
00:29:49,600 --> 00:29:51,860
I will always give you that function.

596
00:29:51,860 --> 00:29:54,050
So there's nothing to lose here
in terms of variance.

597
00:29:54,050 --> 00:29:57,910
Here, I have so many varieties
that, depending on the examples you

598
00:29:57,910 --> 00:29:58,910
give me, I may pick this.

599
00:29:58,910 --> 00:30:02,350
And in another example-- because I'm
fitting your data, so I get

600
00:30:02,350 --> 00:30:04,030
a red cloud around this.

601
00:30:04,030 --> 00:30:08,580
And their centroid will be g bar,
the one that is good, but I may

602
00:30:08,580 --> 00:30:09,750
get one or the other.

603
00:30:09,750 --> 00:30:12,250
And the size of this guy
measures the variance.

604
00:30:12,250 --> 00:30:14,320
This is the price I pay.

605
00:30:14,320 --> 00:30:17,020
Now you can see that if I go from
a small hypothesis to a bigger

606
00:30:17,020 --> 00:30:22,120
hypothesis, the bias goes down,
and the variance goes up.

607
00:30:22,120 --> 00:30:28,950
The idea here, if I make the hypothesis
set bigger, I am making the

608
00:30:28,950 --> 00:30:30,910
bias smaller, because I
am making this bigger.

609
00:30:30,910 --> 00:30:33,950
I'm getting it closer to f, and being
able to approximate it better, so the

610
00:30:33,950 --> 00:30:36,330
bias is diminishing.

611
00:30:36,330 --> 00:30:40,020
But I am making this-- so
the bias goes down.

612
00:30:40,020 --> 00:30:41,470
And here the variance goes up.

613
00:30:41,470 --> 00:30:42,230
Why is the variance going up?

614
00:30:42,230 --> 00:30:43,950
Because the red cloud becomes
bigger and bigger.

615
00:30:43,950 --> 00:30:47,520
If I have this thing, then I have more
variety to choose from, and I am

616
00:30:47,520 --> 00:30:49,770
getting bigger variance to work with.

617
00:30:49,770 --> 00:30:52,950
So this is the nature of the tradeoff.

618
00:30:52,950 --> 00:30:55,730
You may not believe this, because
I just drew a picture and argued

619
00:30:55,730 --> 00:30:56,820
very informally.

620
00:30:56,820 --> 00:31:00,340
So now let's take a very concrete
example, and we will solve it

621
00:31:00,340 --> 00:31:01,700
beginning to end.

622
00:31:01,700 --> 00:31:05,000
And if you understand this example
fully, you will have understood bias

623
00:31:05,000 --> 00:31:07,490
and variance perfectly.

624
00:31:07,490 --> 00:31:08,510
So let's see.

625
00:31:08,510 --> 00:31:14,680
I took that simplest possible example
that I can get a solution of, fully.

626
00:31:14,680 --> 00:31:18,330
My target is a sinusoid.

627
00:31:18,330 --> 00:31:19,330
That's an easy function.

628
00:31:19,330 --> 00:31:22,690
And I just wanted to restrict
myself to -1, +1.

629
00:31:22,690 --> 00:31:24,690
So I'm going to get sine pi x.

630
00:31:24,690 --> 00:31:27,130
Just to scale it so that it's
from -1 to +1, gets

631
00:31:27,130 --> 00:31:28,680
me the whole action.

632
00:31:28,680 --> 00:31:32,960
Therefore, the target function
formally defined, is from

633
00:31:32,960 --> 00:31:35,050
-1, +1, to the real numbers.

634
00:31:35,050 --> 00:31:36,250
The co-domain is the real numbers.

635
00:31:36,250 --> 00:31:38,105
But obviously, the function would
be restricted from -1

636
00:31:38,105 --> 00:31:41,290
to +1, as a range.

637
00:31:41,290 --> 00:31:44,800
Now the target function is unknown.

638
00:31:44,800 --> 00:31:47,760
That's what we have been preaching
for several lectures now.

639
00:31:47,760 --> 00:31:50,660
And now I am just giving you
the target function.

640
00:31:50,660 --> 00:31:52,100
Again, this is an illustration.

641
00:31:52,100 --> 00:31:55,100
When we come to learning, we will try
to blank it out, so that it becomes

642
00:31:55,100 --> 00:31:56,750
unknown in our mind.

643
00:31:56,750 --> 00:31:59,910
But in order to understand the analysis
of the bias-variance, we

644
00:31:59,910 --> 00:32:02,250
would like to know what target
function we are working with.

645
00:32:02,250 --> 00:32:05,520
We're going to get things in terms of
it, and then you will understand why

646
00:32:05,520 --> 00:32:08,160
the tradeoff exists.

647
00:32:08,160 --> 00:32:13,000
So the function looks like this.
Surprise-- just like a sinusoid.

648
00:32:13,000 --> 00:32:14,350
Now the catch is the following.

649
00:32:14,350 --> 00:32:15,990
You are going to learn this function.

650
00:32:15,990 --> 00:32:17,460
I am going to give you a data set.

651
00:32:17,460 --> 00:32:18,710
How big is the data set?

652
00:32:18,710 --> 00:32:22,360


653
00:32:22,360 --> 00:32:25,490
I am not in a generous mood today,
so I am just going to

654
00:32:25,490 --> 00:32:26,930
give you two examples.

655
00:32:26,930 --> 00:32:31,230
And from the two examples, you need to
learn the whole target function.

656
00:32:31,230 --> 00:32:32,680
I'll try.

657
00:32:32,680 --> 00:32:34,750
N equals 2.

658
00:32:34,750 --> 00:32:37,670
The next item is to give
you the hypothesis set.

659
00:32:37,670 --> 00:32:40,680
I'm going to give you two hypothesis
sets to play with.

660
00:32:40,680 --> 00:32:43,740
So one of you gets one, and another gets
another, and you try to learn and

661
00:32:43,740 --> 00:32:46,410
then compare the results.

662
00:32:46,410 --> 00:32:47,590
Well, I have two examples.

663
00:32:47,590 --> 00:32:50,940
So I cannot give you
a 17th-order polynomial.

664
00:32:50,940 --> 00:32:53,530
So I am just going to give
you the following.

665
00:32:53,530 --> 00:32:57,480
The two models are H_0 and H_1.

666
00:32:57,480 --> 00:33:03,040
H_0 happens to be the constant model.

667
00:33:03,040 --> 00:33:04,280
Just give me a constant.

668
00:33:04,280 --> 00:33:07,760
I am going to approximate the sine
function with a constant.

669
00:33:07,760 --> 00:33:10,410
OK, this doesn't look good.

670
00:33:10,410 --> 00:33:12,570
But that's what we are working with.

671
00:33:12,570 --> 00:33:14,830
And the other one is far
more sophisticated.

672
00:33:14,830 --> 00:33:17,660
It's so elaborate, you will love it.

673
00:33:17,660 --> 00:33:19,550
It's linear.

674
00:33:19,550 --> 00:33:22,910
Looks good now, having seen the
constant already, right?

675
00:33:22,910 --> 00:33:24,340
These are your two hypothesis sets.

676
00:33:24,340 --> 00:33:28,240
And we would like to see
which one is better.

677
00:33:28,240 --> 00:33:31,130


678
00:33:31,130 --> 00:33:33,120
Better for what?

679
00:33:33,120 --> 00:33:35,910
That's the key issue.

680
00:33:35,910 --> 00:33:40,110
Let's start to answer the question of
approximation first, and then go to the

681
00:33:40,110 --> 00:33:42,080
question of learning.

682
00:33:42,080 --> 00:33:45,980
Here is the question of approximation,
H_0 versus H_1.

683
00:33:45,980 --> 00:33:48,840
When I talk about approximation, I
am not talking about learning.

684
00:33:48,840 --> 00:33:50,900
I am giving you the target
function, outright.

685
00:33:50,900 --> 00:33:52,390
It's a sinusoid.

686
00:33:52,390 --> 00:33:52,830


687
00:33:52,830 --> 00:33:55,500
If it's a sinusoid, why don't I
say it's just a sinusoid, and

688
00:33:55,500 --> 00:33:57,030
have E_out equal 0?

689
00:33:57,030 --> 00:34:00,020
Oh, because the rule of the game is that
you're using one of the models.

690
00:34:00,020 --> 00:34:02,940
You have use either the constant
or the linear.

691
00:34:02,940 --> 00:34:03,830
Do your best.

692
00:34:03,830 --> 00:34:05,300
Use all the information you have.

693
00:34:05,300 --> 00:34:08,320
But if you use the constant,
return a constant.

694
00:34:08,320 --> 00:34:10,340
If you use the linear,
return a line.

695
00:34:10,340 --> 00:34:13,320
You are not going to be able to return
a bigger hypothesis than those.

696
00:34:13,320 --> 00:34:14,380
That's the game.

697
00:34:14,380 --> 00:34:15,199
OK?

698
00:34:15,199 --> 00:34:16,880
Let's see what happens with H_1.

699
00:34:16,880 --> 00:34:19,560


700
00:34:19,560 --> 00:34:20,840
Here is the target.

701
00:34:20,840 --> 00:34:25,010
I am trying to fit it with
a line, an arbitrary line.

702
00:34:25,010 --> 00:34:27,440
Can you think of what it looks like?

703
00:34:27,440 --> 00:34:31,900
Line is not much, but at least I can
get something like this, right?

704
00:34:31,900 --> 00:34:34,070
Try to get part of the
slope, et cetera.

705
00:34:34,070 --> 00:34:35,449
I can solve this.

706
00:34:35,449 --> 00:34:38,900
I get a line in general, calculate
the mean squared error.

707
00:34:38,900 --> 00:34:40,190
It will be a function of 'a' and 'b'.

708
00:34:40,190 --> 00:34:42,980
Differentiate with respect to 'a'
and 'b', and get the optimal.

709
00:34:42,980 --> 00:34:45,440
It's not a big deal.

710
00:34:45,440 --> 00:34:47,070
So you end up with this.

711
00:34:47,070 --> 00:34:48,670
That's your best approximation.

712
00:34:48,670 --> 00:34:51,469
This is not a learning situation, but
this is the best you can do using the

713
00:34:51,469 --> 00:34:53,630
linear model.

714
00:34:53,630 --> 00:34:56,900
Under those conditions,
you made errors.

715
00:34:56,900 --> 00:34:59,570
And these are your errors.

716
00:34:59,570 --> 00:35:02,770
You didn't get it right, and these
regions tell you how far you are from

717
00:35:02,770 --> 00:35:05,180
the target.

718
00:35:05,180 --> 00:35:07,770
Let's do it with the other guy.

719
00:35:07,770 --> 00:35:09,690
Now I have a constant.

720
00:35:09,690 --> 00:35:12,850
I want to approximate this
guy with a constant.

721
00:35:12,850 --> 00:35:15,610
What is the constant?

722
00:35:15,610 --> 00:35:17,455
I guess I have to work with 0.

723
00:35:17,455 --> 00:35:18,490
That's the best I have.

724
00:35:18,490 --> 00:35:19,860
Remember, it's mean squared error.

725
00:35:19,860 --> 00:35:23,820
So if I move the 0, the big error will
contribute a lot, because it's squared.

726
00:35:23,820 --> 00:35:29,560
So I just put it in the middle,
and this is your hypothesis.

727
00:35:29,560 --> 00:35:31,660
And how much is your error?

728
00:35:31,660 --> 00:35:32,090
Big.

729
00:35:32,090 --> 00:35:34,730
The whole thing is your error.

730
00:35:34,730 --> 00:35:35,520
Let's quantify it.

731
00:35:35,520 --> 00:35:38,010
If you get the expected values of mean
squared error, you'll get a number,

732
00:35:38,010 --> 00:35:44,290
which here will be 0.5, and here
will be approximately 0.2.

733
00:35:44,290 --> 00:35:46,080
So the linear model wins.

734
00:35:46,080 --> 00:35:47,540
Yeah, I'm approximating.

735
00:35:47,540 --> 00:35:48,750
I have more parameters, sure.

736
00:35:48,750 --> 00:35:50,800
If you give me third order, I
will be able to do better.

737
00:35:50,800 --> 00:35:53,260
If you give me 17th order, I'll
be able to do better.

738
00:35:53,260 --> 00:35:55,110
But that's the game.

739
00:35:55,110 --> 00:35:57,210
In terms of approximation,
the more the merrier.

740
00:35:57,210 --> 00:36:01,670
Because you have all the information.
There's no question of zooming in.

741
00:36:01,670 --> 00:36:04,080
Now let's go for learning.

742
00:36:04,080 --> 00:36:07,080
This course is about machine learning, right?
Not about approximation.

743
00:36:07,080 --> 00:36:09,370
So this is the important part for us.

744
00:36:09,370 --> 00:36:12,220
Let's play the same game with
a view to learning.

745
00:36:12,220 --> 00:36:13,240
You have two examples.

746
00:36:13,240 --> 00:36:14,560
You are going to learn from them.

747
00:36:14,560 --> 00:36:18,510
You are restricted to one hypothesis
set or the other.

748
00:36:18,510 --> 00:36:22,690
So let's start with H_1, and
I'll go to H_0 again.

749
00:36:22,690 --> 00:36:24,610
This is your target function.

750
00:36:24,610 --> 00:36:25,920
Now you get two examples.

751
00:36:25,920 --> 00:36:29,790
I'm going to, let's say, uniformly pick
two examples independently, and I

752
00:36:29,790 --> 00:36:31,840
get these two examples.

753
00:36:31,840 --> 00:36:35,160
I'd like you to fit the examples, and
we'll see how well you approximate the

754
00:36:35,160 --> 00:36:36,680
target function.

755
00:36:36,680 --> 00:36:39,640
The first item of business is to
get rid of the target function.

756
00:36:39,640 --> 00:36:41,300
Because you don't know it.

757
00:36:41,300 --> 00:36:42,770
You only know the examples.

758
00:36:42,770 --> 00:36:48,150
So in a learning situation,
this is what you get.

759
00:36:48,150 --> 00:36:50,660
Now I ask you to fit a line.

760
00:36:50,660 --> 00:36:54,500
Line, two points. I can do that.

761
00:36:54,500 --> 00:36:56,150
This is what you do.

762
00:36:56,150 --> 00:36:58,980
Now that you settled on the final
hypothesis, I'm going to grade you.

763
00:36:58,980 --> 00:37:03,800
So I'm going to bring back the target
function, and compare this to that, and

764
00:37:03,800 --> 00:37:07,310
give you what is your
out-of-sample error.

765
00:37:07,310 --> 00:37:09,790
Let's do it for H_0.

766
00:37:09,790 --> 00:37:11,210
You have the same two points.

767
00:37:11,210 --> 00:37:13,770
You're fitting them with a constant.

768
00:37:13,770 --> 00:37:15,180
How would you do that?

769
00:37:15,180 --> 00:37:17,560
Probably the midpoint will give
you the least squared error

770
00:37:17,560 --> 00:37:19,430
on these two points, right?

771
00:37:19,430 --> 00:37:22,030
So this would be your
final hypothesis.

772
00:37:22,030 --> 00:37:24,640
And you get back your target function,
in order to evaluate your

773
00:37:24,640 --> 00:37:27,790
out-of-sample error, and
this is what you get.

774
00:37:27,790 --> 00:37:31,060
Now you can see what the problem is.

775
00:37:31,060 --> 00:37:32,980
I can compute the error here,
and I can have the error

776
00:37:32,980 --> 00:37:34,030
regions, and all of that.

777
00:37:34,030 --> 00:37:36,310
But this depends on which
two points I gave you.

778
00:37:36,310 --> 00:37:39,550
If I give you another two points, I
give you another two points, et

779
00:37:39,550 --> 00:37:42,940
cetera, I am not sure how to really
compare them, because it does depend

780
00:37:42,940 --> 00:37:44,330
on your data set.

781
00:37:44,330 --> 00:37:46,970
That's why we needed the
bias-variance analysis.

782
00:37:46,970 --> 00:37:50,060
That's why we got the expected value
of the error, with respect to the

783
00:37:50,060 --> 00:37:53,900
choice of the data set, so that we
actually are talking inherently about

784
00:37:53,900 --> 00:37:59,220
a linear model learning a target using
two points, regardless of which two

785
00:37:59,220 --> 00:38:02,020
points I'm talking about.

786
00:38:02,020 --> 00:38:10,270
So let's do the bias and variance
decomposition for the constant guy.

787
00:38:10,270 --> 00:38:11,570
Here is the figure.

788
00:38:11,570 --> 00:38:13,740
It's an interesting figure.

789
00:38:13,740 --> 00:38:20,400
Here I am generating data sets, each
of size 2 points, and then

790
00:38:20,400 --> 00:38:21,400
fitting a line.

791
00:38:21,400 --> 00:38:24,000
And the line would be the midpoint.

792
00:38:24,000 --> 00:38:27,900
I keep repeating this exercise,
and I am showing you the final

793
00:38:27,900 --> 00:38:29,530
hypothesis you get.

794
00:38:29,530 --> 00:38:31,170
I repeated it a very large
number of times.

795
00:38:31,170 --> 00:38:34,170
This is a real simulation, and these
are the hypotheses you get.

796
00:38:34,170 --> 00:38:37,100
You can see that when you get this line,
it means that the two points

797
00:38:37,100 --> 00:38:38,570
were equally distant from here.

798
00:38:38,570 --> 00:38:39,800
Sometimes I get the points here.

799
00:38:39,800 --> 00:38:41,840
Sometimes I get them equal to
that, so I get here.

800
00:38:41,840 --> 00:38:44,870
The middle point is
a little bit heavier.

801
00:38:44,870 --> 00:38:48,950
Because, obviously, the chance of
getting them on the two lobes is

802
00:38:48,950 --> 00:38:49,700
there, and so on.

803
00:38:49,700 --> 00:38:52,890
So this is basically the
distribution you get.

804
00:38:52,890 --> 00:38:55,220
Each of them will give you
an out-of-sample error.

805
00:38:55,220 --> 00:38:58,600
And the interesting thing for us is
the expected out-of-sample error.

806
00:38:58,600 --> 00:39:00,920
That's what will grade the model.

807
00:39:00,920 --> 00:39:03,530
Now what we are going to do, we are
going to get the bias and variance

808
00:39:03,530 --> 00:39:05,100
decomposition based on that.

809
00:39:05,100 --> 00:39:06,350
And that is our next figure.

810
00:39:06,350 --> 00:39:09,290


811
00:39:09,290 --> 00:39:11,920
Look at this carefully.

812
00:39:11,920 --> 00:39:20,040
The green guy, the very light
green guy, is g bar of x.

813
00:39:20,040 --> 00:39:22,380
This is the average hypothesis
you get.

814
00:39:22,380 --> 00:39:23,610
How did I get that?

815
00:39:23,610 --> 00:39:28,250
I simply added up all of these guys,
and divided by their number.

816
00:39:28,250 --> 00:39:31,000
And it is expected obviously, by the
symmetry, that on average, I will get

817
00:39:31,000 --> 00:39:33,310
something very close to 0.

818
00:39:33,310 --> 00:39:38,180
The interesting thing is that you can see
now that g bar, here, happens to be

819
00:39:38,180 --> 00:39:39,810
also the best approximation.

820
00:39:39,810 --> 00:39:43,100
If I keep repeating this, I will
actually get the 0 guy, which I was

821
00:39:43,100 --> 00:39:46,630
able to get when I had the full target
function I was approximating.

822
00:39:46,630 --> 00:39:48,400
Here I don't have the full
target function.

823
00:39:48,400 --> 00:39:49,520
I have one hypothesis at a time.

824
00:39:49,520 --> 00:39:51,880
I am getting the average,
but I am getting this.

825
00:39:51,880 --> 00:39:54,590
So there is a justification for saying
that g bar will be the best

826
00:39:54,590 --> 00:39:55,340
hypothesis.

827
00:39:55,340 --> 00:39:58,400
Because this game of getting one at
a time, and then getting the average,

828
00:39:58,400 --> 00:39:59,800
does get me somewhere.

829
00:39:59,800 --> 00:40:02,760
But do remember, this is not the output
of your learning process.

830
00:40:02,760 --> 00:40:03,840
I wish it were.

831
00:40:03,840 --> 00:40:04,930
It isn't.

832
00:40:04,930 --> 00:40:07,130
The output of your learning process
is one of those guys, and

833
00:40:07,130 --> 00:40:08,370
you don't know which.

834
00:40:08,370 --> 00:40:11,190
It just happens that, if you repeat
it, this will be your average.

835
00:40:11,190 --> 00:40:14,990
And because you are getting different
guys here, there will be a variance

836
00:40:14,990 --> 00:40:15,730
around this.

837
00:40:15,730 --> 00:40:19,010
And the variance, I'm describing it
basically by the standard deviation

838
00:40:19,010 --> 00:40:20,620
you are going to get.

839
00:40:20,620 --> 00:40:24,560
So the error between the green line
and the target function will

840
00:40:24,560 --> 00:40:26,120
give you the bias.

841
00:40:26,120 --> 00:40:30,350
And the width of the gray region
will give you the variance.

842
00:40:30,350 --> 00:40:32,680
Understood what the analysis is?

843
00:40:32,680 --> 00:40:35,610
So that takes care of H_0.

844
00:40:35,610 --> 00:40:36,980
Let's go to H_1.

845
00:40:36,980 --> 00:40:40,630
So to remember, the learning
situation for H_0 was this.

846
00:40:40,630 --> 00:40:42,790
This is when I had the constant model.

847
00:40:42,790 --> 00:40:46,730
What will happen if you are actually
fitting the two points, not with

848
00:40:46,730 --> 00:40:50,240
a constant, which you do at the midpoint,
but you are fitting them

849
00:40:50,240 --> 00:40:51,660
with a complete line?

850
00:40:51,660 --> 00:40:54,030
What will it look like?

851
00:40:54,030 --> 00:40:55,280
It will look like this.

852
00:40:55,280 --> 00:40:58,840


853
00:40:58,840 --> 00:40:59,070
Wow.

854
00:40:59,070 --> 00:41:00,580
You can see where the problem is.

855
00:41:00,580 --> 00:41:02,310
Talk about variance.

856
00:41:02,310 --> 00:41:02,930
Take two points.

857
00:41:02,930 --> 00:41:03,660
You connect them.

858
00:41:03,660 --> 00:41:06,560
Wherever the two points, you
get this jungle of lines.

859
00:41:06,560 --> 00:41:10,420
This is for exactly the same data sets
that gave me the horizontal lines in

860
00:41:10,420 --> 00:41:12,910
the previous slide.

861
00:41:12,910 --> 00:41:14,360
So this is what I get.

862
00:41:14,360 --> 00:41:17,510
Now I ask myself, what on
average will I get?

863
00:41:17,510 --> 00:41:21,743
You can immediately say, on average,
you'd better get a positive slope.

864
00:41:21,743 --> 00:41:23,970
There is a tendency to
get a positive slope.

865
00:41:23,970 --> 00:41:26,870
Because when you get the points
split, you will get this.

866
00:41:26,870 --> 00:41:29,310
Sometimes you get a negative
slope here, here.

867
00:41:29,310 --> 00:41:32,310
But that is balanced by getting
a positive slope here.

868
00:41:32,310 --> 00:41:34,710
You can argue this, but
you can do the math.

869
00:41:34,710 --> 00:41:38,190
And then you get the bias-variance
decomposition.

870
00:41:38,190 --> 00:41:40,980
This will be your average.

871
00:41:40,980 --> 00:41:42,940
This is g bar.

872
00:41:42,940 --> 00:41:44,760
And this will be the variance you get.

873
00:41:44,760 --> 00:41:46,020
The variance depends on x.

874
00:41:46,020 --> 00:41:47,070
This is the way we defined it.

875
00:41:47,070 --> 00:41:49,780
And when you want one variance to
describe it, you get the expected

876
00:41:49,780 --> 00:41:52,370
value of the width squared
of that gray region.

877
00:41:52,370 --> 00:41:55,320
This gray region has the
standard deviation.

878
00:41:55,320 --> 00:41:56,640
Now you can see exactly that

879
00:41:56,640 --> 00:41:59,940
I am getting better approximation
than the previous guy.

880
00:41:59,940 --> 00:42:05,360
But I sure am getting very bad variance,
which is expected here.

881
00:42:05,360 --> 00:42:08,150
Now you can see what the tradeoff is.

882
00:42:08,150 --> 00:42:13,410
And the question is, given these
two models, which one wins

883
00:42:13,410 --> 00:42:15,570
from a learning scenario?

884
00:42:15,570 --> 00:42:17,540
You need to ask the question,
to remember what it is.

885
00:42:17,540 --> 00:42:21,210
I am trying to approximate a sinusoid.

886
00:42:21,210 --> 00:42:26,210
Is it better to do it with
a constant or a general line?

887
00:42:26,210 --> 00:42:28,690
The answer to that question
is obvious.

888
00:42:28,690 --> 00:42:31,920
But that is not the question
I am asking in learning.

889
00:42:31,920 --> 00:42:37,600
The question I am asking in learning,
you have two points coming from

890
00:42:37,600 --> 00:42:39,630
something I don't know.

891
00:42:39,630 --> 00:42:43,310
Is it better to use
a constant or a line?

892
00:42:43,310 --> 00:42:44,560
You notice the difference.

893
00:42:44,560 --> 00:42:46,790


894
00:42:46,790 --> 00:42:51,960
I am going to put them side by side,
and then see which is the winner.

895
00:42:51,960 --> 00:42:54,990
So this guy has a big bias
and a small variance.

896
00:42:54,990 --> 00:42:57,960
This guy has a small bias
and a big variance.

897
00:42:57,960 --> 00:43:00,900
Let's get quantitative.

898
00:43:00,900 --> 00:43:03,230
What is the bias here?

899
00:43:03,230 --> 00:43:04,200
It's actually 0.5.

900
00:43:04,200 --> 00:43:06,800
Exactly the same we got when we
were approximating outright.

901
00:43:06,800 --> 00:43:07,360
It's the 0.

902
00:43:07,360 --> 00:43:08,310
That's the expected value.

903
00:43:08,310 --> 00:43:12,790
You get 0.5, the mean squared error.

904
00:43:12,790 --> 00:43:16,220
What is the bias here?

905
00:43:16,220 --> 00:43:17,150
It's 0.21.

906
00:43:17,150 --> 00:43:21,280
Interestingly enough, when we did the
approximation, it was about 0.2.

907
00:43:21,280 --> 00:43:24,500
And indeed, this is not
exactly the best fit.

908
00:43:24,500 --> 00:43:26,750
Remember when I told you there
is a nonlinearity aspect.

909
00:43:26,750 --> 00:43:29,610
You are taking two points at the time,
and then taking a fit, and then taking

910
00:43:29,610 --> 00:43:30,120
the average.

911
00:43:30,120 --> 00:43:33,180
And it's conceivable that this will give
you something different from if

912
00:43:33,180 --> 00:43:35,390
you have the full curve, and you
are fitting it outright.

913
00:43:35,390 --> 00:43:37,510
The difference is usually
very small, and it is.

914
00:43:37,510 --> 00:43:40,160
But here you get something which is not
exactly perfect, but is very close

915
00:43:40,160 --> 00:43:41,820
to perfect.

916
00:43:41,820 --> 00:43:44,250
So obviously, here the
bias is much smaller.

917
00:43:44,250 --> 00:43:45,180
Let's look at the variance.

918
00:43:45,180 --> 00:43:47,480
What is the variance here?

919
00:43:47,480 --> 00:43:50,360
The variance here is 0.25.

920
00:43:50,360 --> 00:43:52,830
It's not too bad.

921
00:43:52,830 --> 00:43:54,610
The variance here, we
expect it to bigger.

922
00:43:54,610 --> 00:43:56,160
But is it big enough to kill us?

923
00:43:56,160 --> 00:44:01,150


924
00:44:01,150 --> 00:44:05,080
It's a disaster, complete
and utter disaster.

925
00:44:05,080 --> 00:44:07,750
And now, when you see what is the
expected out-of-sample error, you add

926
00:44:07,750 --> 00:44:08,520
these two numbers.

927
00:44:08,520 --> 00:44:11,220
Here I'm going to get 0.75, and
here you are going to get

928
00:44:11,220 --> 00:44:13,170
something much bigger.

929
00:44:13,170 --> 00:44:14,420
And the winner is--

930
00:44:14,420 --> 00:44:16,630


931
00:44:16,630 --> 00:44:19,090
Now you go to your friends, and tell
them that I learned today that in

932
00:44:19,090 --> 00:44:22,240
order to approximate a sine, I am better
off approximating it with

933
00:44:22,240 --> 00:44:24,810
a constant than with a general line.

934
00:44:24,810 --> 00:44:26,110
And have a smile on your face.

935
00:44:26,110 --> 00:44:29,110
Of course you know what you're talking
about, but they might not really

936
00:44:29,110 --> 00:44:30,850
appreciate the humor here.

937
00:44:30,850 --> 00:44:32,170


938
00:44:32,170 --> 00:44:33,210
This is the game.

939
00:44:33,210 --> 00:44:34,735
I think we understand it well.

940
00:44:34,735 --> 00:44:37,510


941
00:44:37,510 --> 00:44:40,220
So the lesson learned, if I want to
articulate it, is that when you are in

942
00:44:40,220 --> 00:44:48,210
a learning situation always remember: you
are matching the model complexity

943
00:44:48,210 --> 00:44:55,520
to the data resources you have,
not to the target complexity.

944
00:44:55,520 --> 00:44:57,010
I don't know the target.

945
00:44:57,010 --> 00:45:01,210
And even if I knew the level of
complexity it has, I don't have the

946
00:45:01,210 --> 00:45:03,250
resources to match it.

947
00:45:03,250 --> 00:45:07,420
Because if I match it, I will have the
target in my hypothesis set, but I

948
00:45:07,420 --> 00:45:09,720
will never arrive at it.

949
00:45:09,720 --> 00:45:13,960
Pretty much like I'm sitting in my
office, and I want a document of some

950
00:45:13,960 --> 00:45:15,590
kind, an old letter.

951
00:45:15,590 --> 00:45:17,930
Someone has asked me for a letter of
recommendation, and I don't want to

952
00:45:17,930 --> 00:45:19,090
rewrite it for you.

953
00:45:19,090 --> 00:45:21,410
So I want to take the old guy and just
see what I wrote, and then add the

954
00:45:21,410 --> 00:45:23,640
update to that.

955
00:45:23,640 --> 00:45:27,160
Before everything was archived
in the computers, it used to

956
00:45:27,160 --> 00:45:27,870
be a piece of paper.

957
00:45:27,870 --> 00:45:30,750
So I know the letter of recommendation
is somewhere.

958
00:45:30,750 --> 00:45:33,410
Now I face the question, should I
write the letter of recommendation

959
00:45:33,410 --> 00:45:34,280
from scratch?

960
00:45:34,280 --> 00:45:37,740
Or should I look for the letter
of recommendation?

961
00:45:37,740 --> 00:45:39,100
The recommendation is there.

962
00:45:39,100 --> 00:45:40,650
It's much easier when I find it.

963
00:45:40,650 --> 00:45:43,980
However, finding it is a big deal.

964
00:45:43,980 --> 00:45:46,210
So the question is not that the
target function is there.

965
00:45:46,210 --> 00:45:48,940
The question is, can I find it?

966
00:45:48,940 --> 00:45:52,800
Therefore, when I give you 100 examples,
you choose the hypothesis

967
00:45:52,800 --> 00:45:54,740
set to match the 100 examples.

968
00:45:54,740 --> 00:45:58,480
If the 100 examples are terribly
noisy, that's even worse.

969
00:45:58,480 --> 00:46:01,200
Because their information
to guide you is worse.

970
00:46:01,200 --> 00:46:04,150
That's what I mean by the
data resources you have.

971
00:46:04,150 --> 00:46:07,650
The data resources you have is, what do
you have in order to navigate the

972
00:46:07,650 --> 00:46:09,090
hypothesis set?

973
00:46:09,090 --> 00:46:12,550
Let's pick a hypothesis set that
we can afford to navigate.

974
00:46:12,550 --> 00:46:13,800
That is the game in learning.

975
00:46:13,800 --> 00:46:16,810


976
00:46:16,810 --> 00:46:18,230
Done with the bias and variance.

977
00:46:18,230 --> 00:46:21,330
Now we are going to take just
an illustrative tool, called

978
00:46:21,330 --> 00:46:22,750
the learning curves.

979
00:46:22,750 --> 00:46:26,040
And then we are going to put the bias
and variance versus the VC analysis on

980
00:46:26,040 --> 00:46:27,130
those curves.

981
00:46:27,130 --> 00:46:28,950
So what are the learning curves?

982
00:46:28,950 --> 00:46:31,520
They are related to what we think of
intuitively as a learning curve.

983
00:46:31,520 --> 00:46:33,300
But they are a technical term here.

984
00:46:33,300 --> 00:46:37,480
They are basically plotting the expected
value of E_out and E_in.

985
00:46:37,480 --> 00:46:38,980
We have done E_out already.

986
00:46:38,980 --> 00:46:43,330
But here we also plot the expected value
of E_in, as a function of N.

987
00:46:43,330 --> 00:46:45,070
Let's go through the details.

988
00:46:45,070 --> 00:46:50,590
I give you a data set of size N. We know
what the expected value of the

989
00:46:50,590 --> 00:46:51,630
out-of-sample error is.

990
00:46:51,630 --> 00:46:54,110
We have seen that already in the
bias-variance decomposition.

991
00:46:54,110 --> 00:46:55,440
And this is the quantity.

992
00:46:55,440 --> 00:46:58,770
I know this is the quantity that I will
get in any learning situation.

993
00:46:58,770 --> 00:47:00,300
It depends on the data set.

994
00:47:00,300 --> 00:47:03,620
If I want a quantity that describes
just the size of the set, I will

995
00:47:03,620 --> 00:47:06,000
integrate this out, and get the expected
value with respect to D.

996
00:47:06,000 --> 00:47:07,500
That's the quantity I have.

997
00:47:07,500 --> 00:47:10,060
And the other one is exactly the
same, except it's in-sample.

998
00:47:10,060 --> 00:47:12,160
We didn't use it in the bias-variance
analysis.

999
00:47:12,160 --> 00:47:14,350
This one, I am going to get the expected
value of the in-sample.

1000
00:47:14,350 --> 00:47:18,090
So I want to get, given this situation,
if I give you N examples,

1001
00:47:18,090 --> 00:47:19,240
how well are you going to fit them?

1002
00:47:19,240 --> 00:47:20,300
Well, it depends on the examples.

1003
00:47:20,300 --> 00:47:23,510
But on average, this is how well
you are going to fit them.

1004
00:47:23,510 --> 00:47:26,650
And you ask yourself, how
do these vary with N?

1005
00:47:26,650 --> 00:47:27,960
And here comes the learning curve.

1006
00:47:27,960 --> 00:47:30,160
As you get more examples,
you learn better.

1007
00:47:30,160 --> 00:47:32,760
So hopefully, the learning curve--
and we'll see what the learning

1008
00:47:32,760 --> 00:47:35,440
curve looks like.

1009
00:47:35,440 --> 00:47:37,270
Let's take a simple model first.

1010
00:47:37,270 --> 00:47:39,950


1011
00:47:39,950 --> 00:47:41,540
So it's a simple model.

1012
00:47:41,540 --> 00:47:44,860
And because it's a simple model, it
does not approximate your target

1013
00:47:44,860 --> 00:47:45,900
function well.

1014
00:47:45,900 --> 00:47:49,350
The best out-of-sample error
you can do is pretty high.

1015
00:47:49,350 --> 00:47:52,460


1016
00:47:52,460 --> 00:47:56,770
When you learn, the in-sample will be
very close to the out-of-sample.

1017
00:47:56,770 --> 00:48:00,980
So let's look first at the behavior as
you increase N. As you increase N,

1018
00:48:00,980 --> 00:48:04,200
hopefully the out-of-sample
error is going down.

1019
00:48:04,200 --> 00:48:05,940
I have more examples to learn from.

1020
00:48:05,940 --> 00:48:08,010
I have a better chance of approximating
the target function.

1021
00:48:08,010 --> 00:48:09,280
And indeed, it goes.

1022
00:48:09,280 --> 00:48:12,330
And it can go down and down, until it
gets to the absolute limit of your

1023
00:48:12,330 --> 00:48:13,480
hypothesis set.

1024
00:48:13,480 --> 00:48:14,660
Your hypothesis set very simple.

1025
00:48:14,660 --> 00:48:16,710
It doesn't have a very good
approximation for your target.

1026
00:48:16,710 --> 00:48:17,830
This is the best it can do.

1027
00:48:17,830 --> 00:48:19,600
The best you can do is
the best you can do.

1028
00:48:19,600 --> 00:48:21,341
So that's what you get.

1029
00:48:21,341 --> 00:48:24,170
When you look at the in-sample, it
actually goes the other way around.

1030
00:48:24,170 --> 00:48:27,520
Because here my task is
simpler than here.

1031
00:48:27,520 --> 00:48:29,700
Here I am trying to fit 5 examples.

1032
00:48:29,700 --> 00:48:32,490
Here I am trying to fit 20 examples.

1033
00:48:32,490 --> 00:48:33,710
And I only have the examples to fit.

1034
00:48:33,710 --> 00:48:35,860
I'm not looking at target function,
or anything like that.

1035
00:48:35,860 --> 00:48:40,490
So obviously, I can use my degrees of
freedom in the hypothesis set, and fit

1036
00:48:40,490 --> 00:48:43,640
the 5 examples better, and get
a smaller in-sample error.

1037
00:48:43,640 --> 00:48:46,790
Whereas if I increase N, I will
get a worse in-sample error.

1038
00:48:46,790 --> 00:48:48,470
It doesn't bother me, because
the in-sample error is

1039
00:48:48,470 --> 00:48:49,250
not the bottom line.

1040
00:48:49,250 --> 00:48:50,310
The out-of-sample is.

1041
00:48:50,310 --> 00:48:52,940
And as you can see, although I am
getting worse in-sample, I am getting

1042
00:48:52,940 --> 00:48:53,980
better out-of-sample.

1043
00:48:53,980 --> 00:48:57,340
And indeed, the discrepancy between
them, which is the generalization error,

1044
00:48:57,340 --> 00:49:00,340
is getting tighter and tighter
as N increases.

1045
00:49:00,340 --> 00:49:01,700
Completely logical.

1046
00:49:01,700 --> 00:49:05,250
By the way, this is a real model, so when
we talk about overfitting, I will

1047
00:49:05,250 --> 00:49:09,230
tell you what that model is, as the
simple model and the complex model.

1048
00:49:09,230 --> 00:49:11,950
The complex model, exactly the same
behavior, except it's shifted.

1049
00:49:11,950 --> 00:49:15,750
It's a complex model, so it has
a better approximation

1050
00:49:15,750 --> 00:49:16,740
for your target function.

1051
00:49:16,740 --> 00:49:20,950
So it can achieve, in principle,
a better out-of-sample error.

1052
00:49:20,950 --> 00:49:24,530
You have so many degrees of freedom, that
you were able to fit the training

1053
00:49:24,530 --> 00:49:26,410
set perfectly up to here.

1054
00:49:26,410 --> 00:49:29,290
This corresponds, more or less,
to the VC dimension.

1055
00:49:29,290 --> 00:49:31,440
The VC dimension can
shatter everything.

1056
00:49:31,440 --> 00:49:32,440
So you can shatter these guys.

1057
00:49:32,440 --> 00:49:33,360
You can fit them perfectly.

1058
00:49:33,360 --> 00:49:34,970
So you get zero error.

1059
00:49:34,970 --> 00:49:37,550
You start compromising when you have
more guys and you cannot shatter, so

1060
00:49:37,550 --> 00:49:38,520
maybe you have to compromise.

1061
00:49:38,520 --> 00:49:41,150
And you end up starting
to have in-sample error.

1062
00:49:41,150 --> 00:49:44,740
And the in-sample error goes up, and the
out-of-sample error goes down.

1063
00:49:44,740 --> 00:49:47,330
The interesting thing is that in
here, when you have this, I

1064
00:49:47,330 --> 00:49:48,320
fit the examples perfectly.

1065
00:49:48,320 --> 00:49:49,030
I'm so happy.

1066
00:49:49,030 --> 00:49:50,760
What is out-of-sample?

1067
00:49:50,760 --> 00:49:53,360
An utter disaster.

1068
00:49:53,360 --> 00:49:54,390
Absolutely no information.

1069
00:49:54,390 --> 00:49:55,080
We didn't learn anything.

1070
00:49:55,080 --> 00:49:58,030
We just memorized the examples.

1071
00:49:58,030 --> 00:50:03,680
So here, again, the out-of-sample
error goes down.

1072
00:50:03,680 --> 00:50:05,120
The in-sample error goes up.

1073
00:50:05,120 --> 00:50:06,230
Same argument exactly.

1074
00:50:06,230 --> 00:50:07,170
They get closer together.

1075
00:50:07,170 --> 00:50:10,350
But obviously the discrepancy between
them is bigger, because I have a more

1076
00:50:10,350 --> 00:50:11,700
complex set.

1077
00:50:11,700 --> 00:50:14,760
Therefore, the generalization
error is bigger.

1078
00:50:14,760 --> 00:50:17,050
The bound on it is bigger
in the VC analysis.

1079
00:50:17,050 --> 00:50:19,330
And the actual value is bigger.

1080
00:50:19,330 --> 00:50:21,090
So this is the analysis.

1081
00:50:21,090 --> 00:50:22,440
It's a very simple tool.

1082
00:50:22,440 --> 00:50:26,420
And the reason I introduced it here is
that I want to illustrate the bias and

1083
00:50:26,420 --> 00:50:30,800
variance analysis versus the VC analysis,
using the learning curves.

1084
00:50:30,800 --> 00:50:34,240
It will be very illustrative to
understand how the two theories relate

1085
00:50:34,240 --> 00:50:35,490
to each other.

1086
00:50:35,490 --> 00:50:37,210


1087
00:50:37,210 --> 00:50:43,580
Let's start with the VC analysis
on learning curves.

1088
00:50:43,580 --> 00:50:45,340
These are learning curves.

1089
00:50:45,340 --> 00:50:46,765
The in-sample error goes
up, as promised.

1090
00:50:46,765 --> 00:50:48,760
The out-of-sample error goes down.

1091
00:50:48,760 --> 00:50:52,210
There is a best approximation that
corresponds to this level of

1092
00:50:52,210 --> 00:50:54,880
out-of-sample error, if we
actually knew the thing.

1093
00:50:54,880 --> 00:50:57,840
And what did we do in the VC analysis?

1094
00:50:57,840 --> 00:51:01,630
We had the in-sample error, which is
this region, the height of this

1095
00:51:01,630 --> 00:51:07,030
region, and then we had a bound on the
generalization error, which is Omega.

1096
00:51:07,030 --> 00:51:10,530
And we said that the bound behaves the
same way as the quantity itself.

1097
00:51:10,530 --> 00:51:12,230
So the bound actually will
not be this thing.

1098
00:51:12,230 --> 00:51:14,990
It will be way bigger.

1099
00:51:14,990 --> 00:51:19,140
But in proportionality, it will
give us the same proportion.

1100
00:51:19,140 --> 00:51:22,520
So as you increase N, the generalization
error goes down.

1101
00:51:22,520 --> 00:51:23,600
The bound on it goes down.

1102
00:51:23,600 --> 00:51:26,940
Omega goes down, which
we already realized.

1103
00:51:26,940 --> 00:51:29,330
And obviously, you can
take another model.

1104
00:51:29,330 --> 00:51:34,240
And if the model is very complex, the
discrepancy between them becomes

1105
00:51:34,240 --> 00:51:35,840
bigger, which agrees with that.

1106
00:51:35,840 --> 00:51:37,810
So this is the decomposition of it.

1107
00:51:37,810 --> 00:51:40,760
Now I took some liberties, in order
to be able to do that.

1108
00:51:40,760 --> 00:51:43,210
The VC analysis doesn't
have expected values.

1109
00:51:43,210 --> 00:51:45,510
So I took expected values
of everything there is.

1110
00:51:45,510 --> 00:51:49,510
So there is some liberty taken, in
order to put it to fit in that

1111
00:51:49,510 --> 00:51:51,295
diagram, but the principle holds.

1112
00:51:51,295 --> 00:51:54,585


1113
00:51:54,585 --> 00:51:57,610
The blue region is the in-sample
error, and the red region is

1114
00:51:57,610 --> 00:51:58,620
basically the Omega.

1115
00:51:58,620 --> 00:52:01,410
That is what happens in
the generation bound.

1116
00:52:01,410 --> 00:52:05,130
Think for a moment, which region will be
blue and which region will be red

1117
00:52:05,130 --> 00:52:06,300
in the bias-variance analysis?

1118
00:52:06,300 --> 00:52:09,700
I'll get exactly the same
curves, the same model.

1119
00:52:09,700 --> 00:52:12,310
So what will it be?

1120
00:52:12,310 --> 00:52:16,190
It will be this.

1121
00:52:16,190 --> 00:52:18,740
That's the difference.

1122
00:52:18,740 --> 00:52:23,250
In the bias-variance, I got the bias
based on the best approximation.

1123
00:52:23,250 --> 00:52:26,160
I didn't look at how you
performed in-sample.

1124
00:52:26,160 --> 00:52:29,770
I assumed hypothetically that you
could look for the best possible

1125
00:52:29,770 --> 00:52:30,950
approximation.

1126
00:52:30,950 --> 00:52:33,100
And I charged the bias for that.

1127
00:52:33,100 --> 00:52:35,210
And this is the bias you have.

1128
00:52:35,210 --> 00:52:36,530
So this is the best you can do.

1129
00:52:36,530 --> 00:52:38,090
And this is the error you are making.

1130
00:52:38,090 --> 00:52:39,490
Again, there is a liberty taken here.

1131
00:52:39,490 --> 00:52:43,800
Because this is genuinely the best
approximation in your hypothesis set.

1132
00:52:43,800 --> 00:52:46,330
The one I am using for the
bias-variance analysis is

1133
00:52:46,330 --> 00:52:48,960
the error on g bar.

1134
00:52:48,960 --> 00:52:51,735
And we said, g bar will be
close in error to this guy.

1135
00:52:51,735 --> 00:52:53,395
It may not even be in
the hypothesis set.

1136
00:52:53,395 --> 00:52:55,750
So there is some liberty, but
it's not a huge liberty.

1137
00:52:55,750 --> 00:52:59,140
This is very much close to what you
are getting in the bias-variance.

1138
00:52:59,140 --> 00:53:01,720
And the rest of it is the variance.

1139
00:53:01,720 --> 00:53:04,720
Because you get the bias plus that, and
you will get the expected value of

1140
00:53:04,720 --> 00:53:06,870
the out-of-sample error.

1141
00:53:06,870 --> 00:53:09,350
Now you can see why they are both
talking about the same thing.

1142
00:53:09,350 --> 00:53:11,930
Both of them are talking
about approximation.

1143
00:53:11,930 --> 00:53:13,260
That's the blue part.

1144
00:53:13,260 --> 00:53:15,950
Here it's approximation overall.

1145
00:53:15,950 --> 00:53:19,750
And here it's approximation in-sample.

1146
00:53:19,750 --> 00:53:22,970
And both of them take into consideration
what happens in terms of

1147
00:53:22,970 --> 00:53:24,230
generalization.

1148
00:53:24,230 --> 00:53:26,980
Well, the red region here
is maybe twice the size.

1149
00:53:26,980 --> 00:53:27,850
Not twice the size

1150
00:53:27,850 --> 00:53:29,440
in general. It will be twice
the size actually in the

1151
00:53:29,440 --> 00:53:31,010
linear regression example.

1152
00:53:31,010 --> 00:53:32,390
But basically, they have
the same behavior.

1153
00:53:32,390 --> 00:53:33,800
They have just different scale.

1154
00:53:33,800 --> 00:53:38,330
So they capture the same principle of
generalizing, or the uncertainty about

1155
00:53:38,330 --> 00:53:42,720
which hypothesis to pick, or how much
do I lose from going in-sample to

1156
00:53:42,720 --> 00:53:43,400
out-of-sample.

1157
00:53:43,400 --> 00:53:45,090
So they have the same behavior.

1158
00:53:45,090 --> 00:53:48,310
And the only difference here is that,
here the bias obviously is constant

1159
00:53:48,310 --> 00:53:51,800
with respect to N. The bias depends
on the hypothesis set.

1160
00:53:51,800 --> 00:53:53,450
Now this is also an assumption.

1161
00:53:53,450 --> 00:53:55,950
Because it says, I have 2 examples
and take the average.

1162
00:53:55,950 --> 00:53:56,950
I will get an error.

1163
00:53:56,950 --> 00:53:59,650
If I have 10 examples and take
the average, I get an error.

1164
00:53:59,650 --> 00:54:00,480
Is it the same?

1165
00:54:00,480 --> 00:54:03,850
Well, in both cases, you effectively used
an infinite number of examples.

1166
00:54:03,850 --> 00:54:06,610
Because the first one you used two
at a time, and you repeated it

1167
00:54:06,610 --> 00:54:09,430
an infinite number of times,
and you took an average.

1168
00:54:09,430 --> 00:54:12,380
This, you used 10 at a time,
and you took an average.

1169
00:54:12,380 --> 00:54:12,670


1170
00:54:12,670 --> 00:54:14,790
I grant you maybe the 10 will
give you a better situation.

1171
00:54:14,790 --> 00:54:18,990
But again, it's a little bit of
a license, in order to be able to

1172
00:54:18,990 --> 00:54:22,710
attribute the bias and variance to this
line, which happens to be the

1173
00:54:22,710 --> 00:54:26,210
best hypothesis proper within
your hypothesis set.

1174
00:54:26,210 --> 00:54:28,840
So this is the contrast between the
two theoretical approaches we have

1175
00:54:28,840 --> 00:54:31,590
covered, in this lecture and the
previous three lectures.

1176
00:54:31,590 --> 00:54:34,100


1177
00:54:34,100 --> 00:54:38,400
I am going to end up with the analysis
for the linear regression case.

1178
00:54:38,400 --> 00:54:41,830
So I'm going to basically go
through it fairly quickly.

1179
00:54:41,830 --> 00:54:45,610
This is a very good exercise to do.

1180
00:54:45,610 --> 00:54:49,110
And if you read the exercise and you
follow the steps, it will give you

1181
00:54:49,110 --> 00:54:51,190
very good insight into the
linear regression.

1182
00:54:51,190 --> 00:54:53,780
I'll try to explain the
highlights of it.

1183
00:54:53,780 --> 00:54:57,130
Let's start with a reminder
of linear regression.

1184
00:54:57,130 --> 00:54:59,250
So linear regression,
I'm using a target.

1185
00:54:59,250 --> 00:55:04,700
For the purpose of simplification, I am
going to use a noisy target, which

1186
00:55:04,700 --> 00:55:08,280
is linear plus noise.

1187
00:55:08,280 --> 00:55:11,450
So I'm using linear regression to learn
something linear plus noise.

1188
00:55:11,450 --> 00:55:13,700
If it weren't for the noise,
I would get it perfectly.

1189
00:55:13,700 --> 00:55:14,950
It's already linear.

1190
00:55:14,950 --> 00:55:17,960
But because of the noise, I will
be deviating a little bit.

1191
00:55:17,960 --> 00:55:23,360
This is just to make the mathematics
that results easier to handle.

1192
00:55:23,360 --> 00:55:25,710
Now you're given a data set.

1193
00:55:25,710 --> 00:55:27,350
And the data set is a noisy data set.

1194
00:55:27,350 --> 00:55:30,190
So each of these is picked
independently.

1195
00:55:30,190 --> 00:55:34,960
This y depends on x, and the only
unknown here is the noise.

1196
00:55:34,960 --> 00:55:37,720
So you get this value, it gives you
the average, and then you add

1197
00:55:37,720 --> 00:55:38,970
a noise to get the y.

1198
00:55:38,970 --> 00:55:41,310


1199
00:55:41,310 --> 00:55:43,440
Do you remember the linear
regression solution?

1200
00:55:43,440 --> 00:55:46,200
Regardless of what the target function
is, you look at the data, and this is

1201
00:55:46,200 --> 00:55:48,770
what you get for the solution.

1202
00:55:48,770 --> 00:55:52,950
You take the input data set,
and the output data set.

1203
00:55:52,950 --> 00:55:57,480
You do this algebraic combination, and
whatever comes out is your output of

1204
00:55:57,480 --> 00:55:58,420
the linear regression.

1205
00:55:58,420 --> 00:56:00,150
This is your final hypothesis.

1206
00:56:00,150 --> 00:56:00,570


1207
00:56:00,570 --> 00:56:02,590
We have done that.

1208
00:56:02,590 --> 00:56:07,050
And now we are going to think about
a notion of the in-sample error, not

1209
00:56:07,050 --> 00:56:10,920
in-sample error as a summary quantity,
but the in-sample error pattern.

1210
00:56:10,920 --> 00:56:12,600
How much error do I get
in the first example?

1211
00:56:12,600 --> 00:56:15,000
How much error do I get in the
second, third, et cetera?

1212
00:56:15,000 --> 00:56:16,150
Just for organization.

1213
00:56:16,150 --> 00:56:17,390
So what would that be?

1214
00:56:17,390 --> 00:56:22,060
Well, that would be what I got
in the final hypothesis.

1215
00:56:22,060 --> 00:56:24,900
I apply the final hypothesis
to the input points.

1216
00:56:24,900 --> 00:56:29,020
I am going to get a pattern of values
that my hypothesis is predicting.

1217
00:56:29,020 --> 00:56:32,870
I compare them to the actual targets,
which happen to be stored in the y.

1218
00:56:32,870 --> 00:56:34,980
And that would be an error pattern.

1219
00:56:34,980 --> 00:56:37,290
So it would be plus something
minus something, plus

1220
00:56:37,290 --> 00:56:38,230
something minus something.

1221
00:56:38,230 --> 00:56:42,110
And if I add the squared values here,
get the average of those, I will

1222
00:56:42,110 --> 00:56:45,160
get what we call the in-sample error.

1223
00:56:45,160 --> 00:56:48,490
For the out-of-sample error, I am going
to play a simplifying trick here,

1224
00:56:48,490 --> 00:56:52,650
in order to get the learning
curve in the finite case.

1225
00:56:52,650 --> 00:56:57,270
Here I am going to consider that, in
order to get the out-of-sample error,

1226
00:56:57,270 --> 00:57:02,030
what I'm going to do I am going to just
generate the same inputs, which

1227
00:57:02,030 --> 00:57:04,060
is a complete no-no in out-of-sample.

1228
00:57:04,060 --> 00:57:07,250
Supposedly in out-of-sample, you get
points that you haven't seen before.

1229
00:57:07,250 --> 00:57:09,270
You have seen these x's before.

1230
00:57:09,270 --> 00:57:13,850
But the redeeming value is that I'm
now going to give you fresh noise.

1231
00:57:13,850 --> 00:57:18,210
So that's the unknown, and that is what
allows me to say that it plays

1232
00:57:18,210 --> 00:57:19,710
the role of an out-of-sample.

1233
00:57:19,710 --> 00:57:24,190
I'm going to generate another set of
points with different noises, but on

1234
00:57:24,190 --> 00:57:26,770
the same inputs in order to
simplify the analysis.

1235
00:57:26,770 --> 00:57:28,860
You see that the x's
here are involved.

1236
00:57:28,860 --> 00:57:31,620
And if I use the same inputs,
things will simplify.

1237
00:57:31,620 --> 00:57:34,890
And in that case, if you ask yourself
what is the out-of-sample error of

1238
00:57:34,890 --> 00:57:36,010
those, it's exactly the same.

1239
00:57:36,010 --> 00:57:37,920
I evaluated on the points.

1240
00:57:37,920 --> 00:57:40,210
They happen to be the points
for the out-of-sample.

1241
00:57:40,210 --> 00:57:41,490
And I'm comparing it with y.

1242
00:57:41,490 --> 00:57:44,610
I'm calling it y dash, which is exactly
the same thing, except with

1243
00:57:44,610 --> 00:57:47,590
noise dash, another realization
of the noise.

1244
00:57:47,590 --> 00:57:52,950
This is the outline of the setup to
get us the learning curves we want.

1245
00:57:52,950 --> 00:57:57,210
When you do the analysis, not that
difficult at all, you will get this

1246
00:57:57,210 --> 00:57:59,530
very interesting curve.

1247
00:57:59,530 --> 00:58:02,770
This is the learning curve, and
it has very specific values.

1248
00:58:02,770 --> 00:58:05,310
sigma squared, that's the
variance of the noise.

1249
00:58:05,310 --> 00:58:06,620
This is the best you can do.

1250
00:58:06,620 --> 00:58:09,460
I expect that, because you told
me the target is linear.

1251
00:58:09,460 --> 00:58:10,970
So I can get that perfectly.

1252
00:58:10,970 --> 00:58:12,320
But then, there is this added noise.

1253
00:58:12,320 --> 00:58:13,790
I cannot capture the noise.

1254
00:58:13,790 --> 00:58:15,490
What is the variance of the
noise? sigma squared.

1255
00:58:15,490 --> 00:58:18,580
So this is the error
that is inevitable.

1256
00:58:18,580 --> 00:58:20,000
Look at the in-sample error.

1257
00:58:20,000 --> 00:58:21,570
Up to d plus 1, you were perfect.

1258
00:58:21,570 --> 00:58:22,640
Yeah, of course I am perfect.

1259
00:58:22,640 --> 00:58:26,720
Because I have d plus 1 parameters in
linear, and I am fitting less than

1260
00:58:26,720 --> 00:58:28,630
those, so I can fit them perfectly.

1261
00:58:28,630 --> 00:58:31,350
It doesn't mean much for the
out-of-sample error, but

1262
00:58:31,350 --> 00:58:32,310
that's what I get.

1263
00:58:32,310 --> 00:58:34,610
I start compromising when
I get more points.

1264
00:58:34,610 --> 00:58:39,680
And as I go with more points,
here I'm fitting the noise.

1265
00:58:39,680 --> 00:58:40,450
I am fitting the noise less.

1266
00:58:40,450 --> 00:58:41,540
The noise is averaging out.

1267
00:58:41,540 --> 00:58:44,110
Now I'm getting very, very close,
to as if there was no noise.

1268
00:58:44,110 --> 00:58:47,300
Because the pattern persists,
which is the linear guy.

1269
00:58:47,300 --> 00:58:50,800
And the noise, if I get more
examples, more or less cancels out in

1270
00:58:50,800 --> 00:58:51,440
the fitting.

1271
00:58:51,440 --> 00:58:53,760
I don't have enough degrees of
freedom to fit them all.

1272
00:58:53,760 --> 00:58:56,760
So I get to average, until
eventually I get to as if

1273
00:58:56,760 --> 00:58:58,030
I am doing it perfectly.

1274
00:58:58,030 --> 00:59:00,170
And out-of-sample goes down.

1275
00:59:00,170 --> 00:59:02,960
There is a very specific formula that
you can get, which is interesting.

1276
00:59:02,960 --> 00:59:04,760
So let me finish with this.

1277
00:59:04,760 --> 00:59:07,270
The best approximation error
is sigma squared.

1278
00:59:07,270 --> 00:59:10,320
That's the line, right?

1279
00:59:10,320 --> 00:59:12,275
What is the expected in-sample error?

1280
00:59:12,275 --> 00:59:18,250
It has a very simple formula, which is--
everything is scaled by sigma squared.

1281
00:59:18,250 --> 00:59:21,700
So What you have here is,
it's almost perfect.

1282
00:59:21,700 --> 00:59:27,210
And you are doing better than perfect, by
this amount, the ratio of d plus 1.

1283
00:59:27,210 --> 00:59:28,500
Remember what d plus 1 was?

1284
00:59:28,500 --> 00:59:31,000
For the perceptron, it
was the VC dimension.

1285
00:59:31,000 --> 00:59:33,930
Here it's also a VC dimension of sorts,
the degrees of freedom that

1286
00:59:33,930 --> 00:59:35,410
linear regression has.

1287
00:59:35,410 --> 00:59:38,280
So we divide the degrees of freedom
by the number of examples.

1288
00:59:38,280 --> 00:59:40,720
That is the factor that you get.

1289
00:59:40,720 --> 00:59:45,220
And you realize here that this
is the best you can do.

1290
00:59:45,220 --> 00:59:47,440
And here you are doing
better than the best.

1291
00:59:47,440 --> 00:59:48,320
Why is it better than the best?

1292
00:59:48,320 --> 00:59:49,780
Because I'm not trying to
fit the whole function.

1293
00:59:49,780 --> 00:59:53,020
I am only fitting the finite sample.

1294
00:59:53,020 --> 00:59:56,260
So I'm doing very well, and I'm very
happy about it, little that I know

1295
00:59:56,260 --> 00:59:58,280
that I'm actually harming myself.

1296
00:59:58,280 --> 01:00:00,400
Because what I'm doing here,
I am fitting the noise.

1297
01:00:00,400 --> 01:00:02,870
And as a result of that, I am deviating
from the optimal guy.

1298
01:00:02,870 --> 01:00:06,570
And I am paying the price
in out-of-sample error.

1299
01:00:06,570 --> 01:00:08,870
What is the price I am paying
in out-of-sample error?

1300
01:00:08,870 --> 01:00:11,500
It is the mirror image.

1301
01:00:11,500 --> 01:00:16,030
I lose exactly in out-of-sample
what I gained in-sample.

1302
01:00:16,030 --> 01:00:18,230
And the most interesting quantity
is the summary quantity.

1303
01:00:18,230 --> 01:00:20,100
What is the expected generalization
error?

1304
01:00:20,100 --> 01:00:22,160
Well, the generalization error is the
difference between this and that.

1305
01:00:22,160 --> 01:00:23,330
I have the formula for them.

1306
01:00:23,330 --> 01:00:26,220
So all I need to do is write this.

1307
01:00:26,220 --> 01:00:29,510
Let me magnify this.

1308
01:00:29,510 --> 01:00:31,072
This is the generalization error.

1309
01:00:31,072 --> 01:00:37,510
It has the form of the VC dimension
divided by the number of examples.

1310
01:00:37,510 --> 01:00:40,180
In this case, it's exact.

1311
01:00:40,180 --> 01:00:42,220
And this is what I promised last time.

1312
01:00:42,220 --> 01:00:45,260
I told you that this rule of
proportionality between a VC dimension

1313
01:00:45,260 --> 01:00:49,440
and a number of examples persists to
the level where sometimes, you just

1314
01:00:49,440 --> 01:00:52,200
divide the VC dimension by the number of
examples, and that will give you

1315
01:00:52,200 --> 01:00:54,000
a generalization error.

1316
01:00:54,000 --> 01:00:57,780
This is the concrete version of it, in
spite of the fact that here is not

1317
01:00:57,780 --> 01:00:58,560
a VC dimension.

1318
01:00:58,560 --> 01:00:59,670
This is real-valued.

1319
01:00:59,670 --> 01:01:01,970
But it's degrees of freedom,
so it plays the role.

1320
01:01:01,970 --> 01:01:05,230
We could actually solve for it and
realize that this is indeed the

1321
01:01:05,230 --> 01:01:08,350
compromise between the degrees of
freedom I have, in the case of linear

1322
01:01:08,350 --> 01:01:12,510
regression, and the number
of examples I am using.

1323
01:01:12,510 --> 01:01:13,690
So we will stop here.

1324
01:01:13,690 --> 01:01:17,518
And we will go into questions and
answers after a short break.

1325
01:01:17,518 --> 01:01:21,820


1326
01:01:21,820 --> 01:01:22,660


1327
01:01:22,660 --> 01:01:25,410
Let's go into the questions.

1328
01:01:25,410 --> 01:01:25,770
MODERATOR: Right.

1329
01:01:25,770 --> 01:01:28,754
The first question is if you
can go back to slide 19.

1330
01:01:28,754 --> 01:01:30,004
PROFESSOR: 19.

1331
01:01:30,004 --> 01:01:34,130


1332
01:01:34,130 --> 01:01:39,670
MODERATOR: The question is if you can
explain how complex models are better

1333
01:01:39,670 --> 01:01:41,210
than simple models.

1334
01:01:41,210 --> 01:01:43,260
PROFESSOR: OK.

1335
01:01:43,260 --> 01:01:44,670
Better in something.

1336
01:01:44,670 --> 01:01:49,530
I think the key issue in the theory
is, there is a tradeoff.

1337
01:01:49,530 --> 01:01:53,790
Nothing is better on all fronts, and
nothing is worse on all fronts.

1338
01:01:53,790 --> 01:01:57,930
So let's compare the simple model
and the complex model.

1339
01:01:57,930 --> 01:02:01,730
In terms of the ability to approximate,
whether that ability to

1340
01:02:01,730 --> 01:02:04,870
approximate is in-sample, or whether
the ability to approximate is

1341
01:02:04,870 --> 01:02:09,000
absolute, what is the ability to
approximate in the absolute?

1342
01:02:09,000 --> 01:02:11,640
Here is my hypothesis set, and
I have a target function.

1343
01:02:11,640 --> 01:02:19,110
The horizontal line, that height gives
you the error of approximation.

1344
01:02:19,110 --> 01:02:23,160
So if you go from a simple model to
a complex model, you will be able to

1345
01:02:23,160 --> 01:02:24,550
approximate better.

1346
01:02:24,550 --> 01:02:25,900
That is obvious.

1347
01:02:25,900 --> 01:02:29,960
And that also is inherited if your
approximation is focused only on the

1348
01:02:29,960 --> 01:02:30,730
training examples.

1349
01:02:30,730 --> 01:02:33,530
In this case, you are comparing
not the horizontal lines,

1350
01:02:33,530 --> 01:02:34,710
but the blue curves.

1351
01:02:34,710 --> 01:02:40,050
This is the error you make in
approximating the sample you get.

1352
01:02:40,050 --> 01:02:43,790
And again, the approximation for the
simple model is worse than the

1353
01:02:43,790 --> 01:02:45,890
approximation for the complex model.

1354
01:02:45,890 --> 01:02:49,820
So if your game is approximation, and
that's your purpose, then obviously

1355
01:02:49,820 --> 01:02:51,852
the complex model is better.

1356
01:02:51,852 --> 01:02:55,620
In this particular case, you
can also ask yourself about the

1357
01:02:55,620 --> 01:02:57,090
generalization ability.

1358
01:02:57,090 --> 01:03:02,800
The generalization ability will be the
discrepancy between, either the blue

1359
01:03:02,800 --> 01:03:03,750
and red curve.

1360
01:03:03,750 --> 01:03:05,105
That would be the VC analysis.

1361
01:03:05,105 --> 01:03:09,810
This would be how much I lose from going
from in-sample to out-of-sample.

1362
01:03:09,810 --> 01:03:14,920
Or how much I lose from a perfect
approximation, in the case of the

1363
01:03:14,920 --> 01:03:21,850
bias-variance analysis, to getting E_out,
because of my inability to zoom in on

1364
01:03:21,850 --> 01:03:22,910
the right hypothesis.

1365
01:03:22,910 --> 01:03:25,860
This would be that area here.

1366
01:03:25,860 --> 01:03:28,550
So whether you are taking the difference
between the blue and red

1367
01:03:28,550 --> 01:03:32,600
curve, or the difference between the
red curve and the black line, that

1368
01:03:32,600 --> 01:03:35,470
area is smaller here than here.

1369
01:03:35,470 --> 01:03:39,310
Therefore, the simple model
is better, as far as the

1370
01:03:39,310 --> 01:03:41,730
generalization is concerned.

1371
01:03:41,730 --> 01:03:44,880
Now because it's a tradeoff, and I have
one of them better and one of

1372
01:03:44,880 --> 01:03:48,840
them worse, then the question is, when
I put them together, which is better?

1373
01:03:48,840 --> 01:03:52,980
Because the bottom line in learning
is the red curve.

1374
01:03:52,980 --> 01:03:54,360
That's what I care about.

1375
01:03:54,360 --> 01:03:57,070
This is the performance of the system
that I'm going to deliver to my

1376
01:03:57,070 --> 01:03:59,760
customer, and they're going
to test it out-of-sample.

1377
01:03:59,760 --> 01:04:03,010
And if they get it right,
they will be happy.

1378
01:04:03,010 --> 01:04:07,140
So now because I have two quantities
that I'm adding, and one of them is

1379
01:04:07,140 --> 01:04:12,020
going down, and one of them is going
up, then it is obvious that the sum

1380
01:04:12,020 --> 01:04:13,390
could go either way.

1381
01:04:13,390 --> 01:04:15,860
And in this case, you can see
that it is going either way.

1382
01:04:15,860 --> 01:04:20,750
For example, if you have few examples,
then E_out here is OK.

1383
01:04:20,750 --> 01:04:22,240
It's not great, but it's decent.

1384
01:04:22,240 --> 01:04:26,830
If you have the same number of examples
here, E_out is a disaster.

1385
01:04:26,830 --> 01:04:28,640
So if you have few examples,
you simply cannot

1386
01:04:28,640 --> 01:04:30,280
afford the complex model.

1387
01:04:30,280 --> 01:04:33,210
You are better off working with a simple
model, and you will get better

1388
01:04:33,210 --> 01:04:34,810
out-of-sample error.

1389
01:04:34,810 --> 01:04:38,260
If I give you much bigger resource
of the examples-- if you are here, now

1390
01:04:38,260 --> 01:04:40,460
this one is limited by the
fact that it's simple.

1391
01:04:40,460 --> 01:04:41,600
It cannot get any better.

1392
01:04:41,600 --> 01:04:42,465
It has all the information.

1393
01:04:42,465 --> 01:04:45,040
It zooms in perfectly, but
it cannot get any better.

1394
01:04:45,040 --> 01:04:48,930
This guy now gets to use its degrees of
freedom properly, and gets you to

1395
01:04:48,930 --> 01:04:49,860
a smaller value.

1396
01:04:49,860 --> 01:04:53,800
So for larger number of points, you
get a better performance here.

1397
01:04:53,800 --> 01:04:57,070
That's why we are saying that you should
match the complexity of the

1398
01:04:57,070 --> 01:05:02,530
model to the data resources you have,
which in this case are represented by

1399
01:05:02,530 --> 01:05:06,790
N. We're talking about different target
functions and different things.

1400
01:05:06,790 --> 01:05:10,920
But in choosing this model or another,
what really dictates the performance

1401
01:05:10,920 --> 01:05:14,246
is the number of examples versus
the complexity of the model.

1402
01:05:14,246 --> 01:05:17,660


1403
01:05:17,660 --> 01:05:18,910
MODERATOR: OK.

1404
01:05:18,910 --> 01:05:21,340


1405
01:05:21,340 --> 01:05:25,290
When you did the analysis for linear
regression, if you did it using the

1406
01:05:25,290 --> 01:05:29,510
perception model, would you get
the same generalization error?

1407
01:05:29,510 --> 01:05:31,085
PROFESSOR: Let's go for that.

1408
01:05:31,085 --> 01:05:35,590


1409
01:05:35,590 --> 01:05:41,440
The analysis of the bias-variance, and
it's also inherited in the learning

1410
01:05:41,440 --> 01:05:46,400
curves-- the analysis is very clean
when you use mean squared error.

1411
01:05:46,400 --> 01:05:50,050
Obviously, you can use mean squared
error in the perceptron.

1412
01:05:50,050 --> 01:05:52,460
There will be a correspondence here.

1413
01:05:52,460 --> 01:05:57,350
But the ability to get such a clean
formula here really depends on the

1414
01:05:57,350 --> 01:06:01,610
very particulars of linear regression.

1415
01:06:01,610 --> 01:06:05,480
If you go back to the previous slide,
where the assumption is, it was very

1416
01:06:05,480 --> 01:06:09,830
critical to make the assumption that the
out-of-sample is this way, and to

1417
01:06:09,830 --> 01:06:13,180
make the target very specifically linear
plus noise, in order to be able

1418
01:06:13,180 --> 01:06:14,160
to simplify.

1419
01:06:14,160 --> 01:06:17,240
The result, by the way, holds
in general, asymptotically.

1420
01:06:17,240 --> 01:06:20,460
So if you take genuine out-of-sample,
which means that you pick different

1421
01:06:20,460 --> 01:06:22,880
points, you will get
a different matrix X.

1422
01:06:22,880 --> 01:06:27,690
So you will apply w that you got from
in-sample, you'll apply it to X dash in

1423
01:06:27,690 --> 01:06:30,230
this case, which is this,
and then y dash.

1424
01:06:30,230 --> 01:06:33,510
And the problem is that when you plug
it in here, and try to get a formula

1425
01:06:33,510 --> 01:06:38,670
for that, the formula will depend on
how the X dash relates to the X.

1426
01:06:38,670 --> 01:06:41,200
When it's the same, they cancel
out neatly, and you get the

1427
01:06:41,200 --> 01:06:42,280
formula that I had.

1428
01:06:42,280 --> 01:06:45,720
But asymptotically, if you make certain
assumptions about how X is

1429
01:06:45,720 --> 01:06:49,560
generated and you take the asymptotic
result, you will get the same thing.

1430
01:06:49,560 --> 01:06:51,350
So the short answer is the following.

1431
01:06:51,350 --> 01:06:55,190
The analysis in the exact form that I
gave, which gives me these very neat

1432
01:06:55,190 --> 01:06:59,030
results, is very specific to linear
regression, very specific to the

1433
01:06:59,030 --> 01:07:00,680
choice of out-of-sample as I did it,

1434
01:07:00,680 --> 01:07:03,810
if you want to give the answer
exactly in a finite case.

1435
01:07:03,810 --> 01:07:06,990
If you use a perceptron, you will be
able to find a parallel, but it may

1436
01:07:06,990 --> 01:07:10,140
not be as neat.

1437
01:07:10,140 --> 01:07:11,190
MODERATOR: Quick clarification.

1438
01:07:11,190 --> 01:07:14,220
sigma squared is the variance
of the noise in the--

1439
01:07:14,220 --> 01:07:15,700
PROFESSOR: Yeah.

1440
01:07:15,700 --> 01:07:15,990


1441
01:07:15,990 --> 01:07:17,810
I just realized that.

1442
01:07:17,810 --> 01:07:19,820
I have been using bias-variance.

1443
01:07:19,820 --> 01:07:22,830
The lecture is called bias-variance, and
now we have variance of the noise.

1444
01:07:22,830 --> 01:07:26,000
So obviously, I am so used to these
things that I didn't notice.

1445
01:07:26,000 --> 01:07:29,700
When I say the variance here, this has
absolutely nothing to do with the

1446
01:07:29,700 --> 01:07:31,610
bias-variance analysis
that I talked about.

1447
01:07:31,610 --> 01:07:32,700
It's a noise.

1448
01:07:32,700 --> 01:07:35,380
I am trying to measure
the energy of it.

1449
01:07:35,380 --> 01:07:41,360
It's a zero-mean noise, so the energy of
it is proportional to the variance.

1450
01:07:41,360 --> 01:07:45,080
So I should have called it-- the energy of
the noise is sigma squared, in order

1451
01:07:45,080 --> 01:07:46,000
not to confuse people.

1452
01:07:46,000 --> 01:07:49,830
But I hope that I did not
confuse too many people.

1453
01:07:49,830 --> 01:07:51,820
MODERATOR: Can the bias-variance
analysis be

1454
01:07:51,820 --> 01:07:55,580
used for model selection?

1455
01:07:55,580 --> 01:07:58,040
PROFESSOR: Bias-variance
analysis, just because it is so

1456
01:07:58,040 --> 01:08:02,230
specific, it actually assumes that you
know the target function, if you want

1457
01:08:02,230 --> 01:08:03,900
to get the quantities explicitly.

1458
01:08:03,900 --> 01:08:07,700
So for example linear regression,
I assume the form is linear plus noise.

1459
01:08:07,700 --> 01:08:11,100
For the sinusoidal case, we got the
answers, and we were able to choose.

1460
01:08:11,100 --> 01:08:13,410
But you actually knew that
it was a sinusoid.

1461
01:08:13,410 --> 01:08:17,010
So the bias-variance analysis
is taken as a guide.

1462
01:08:17,010 --> 01:08:18,689
But it's a very important guide.

1463
01:08:18,689 --> 01:08:22,920
Because I can ask myself how do I
affect-- I want to

1464
01:08:22,920 --> 01:08:24,120
get E_out to be down.

1465
01:08:24,120 --> 01:08:24,609


1466
01:08:24,609 --> 01:08:26,100
Now I know that there
are two contributing

1467
01:08:26,100 --> 01:08:28,260
factors, bias and variance.

1468
01:08:28,260 --> 01:08:33,140
Can I get the variance down, without
getting the bias up?

1469
01:08:33,140 --> 01:08:34,140
That's a bunch of techniques.

1470
01:08:34,140 --> 01:08:36,479
Regularization will belong
to that category.

1471
01:08:36,479 --> 01:08:38,880
Can I get both of them down?

1472
01:08:38,880 --> 01:08:40,130
That will be learning from hints.

1473
01:08:40,130 --> 01:08:43,060
There will be something that affects
both of them, and so on.

1474
01:08:43,060 --> 01:08:46,450
So you can map different techniques
to how they are affecting

1475
01:08:46,450 --> 01:08:48,620
the bias and variance.

1476
01:08:48,620 --> 01:08:51,689
I would say that, in terms of any
application to learning situation,

1477
01:08:51,689 --> 01:08:55,120
it's a guideline rather than something
that I'm going to plug in, and tell you

1478
01:08:55,120 --> 01:08:58,090
what the model is.

1479
01:08:58,090 --> 01:09:02,819
The answer for the model selection is
mostly through validation, which we're

1480
01:09:02,819 --> 01:09:05,170
going to talk about in a few lectures.

1481
01:09:05,170 --> 01:09:09,410
And this is the gold standard for the
choices you make in a learning

1482
01:09:09,410 --> 01:09:13,425
situation, including
choosing the model.

1483
01:09:13,425 --> 01:09:15,560
MODERATOR: I have a question
getting a little bit ahead.

1484
01:09:15,560 --> 01:09:18,520


1485
01:09:18,520 --> 01:09:23,000
In ensemble methods, like boosting or
something, is there a reason under

1486
01:09:23,000 --> 01:09:26,140
these analyses why those methods work?

1487
01:09:26,140 --> 01:09:29,130
PROFESSOR: I almost included
this in the lecture, but I thought it

1488
01:09:29,130 --> 01:09:31,120
was one too many.

1489
01:09:31,120 --> 01:09:37,706
If you look at the idea of g bar.

1490
01:09:37,706 --> 01:09:41,300
Let me try to get to its definition.

1491
01:09:41,300 --> 01:09:43,899
This was just a theoretical
tool of analysis.

1492
01:09:43,899 --> 01:09:46,350
I have g bar equals the expected
value of that.

1493
01:09:46,350 --> 01:09:53,510
And if I want to do it with a finite
number of sets, I sum up this, and

1494
01:09:53,510 --> 01:09:55,810
normalize by 1 over K.

1495
01:09:55,810 --> 01:09:59,080
Although this was just
a theoretical way of getting the

1496
01:09:59,080 --> 01:10:01,750
bias-variance decomposition, and this is
a conceptual way of understanding

1497
01:10:01,750 --> 01:10:07,070
what it is, there is an ensemble
learning method that builds exactly on

1498
01:10:07,070 --> 01:10:10,950
this, which is called Bagging--
bootstrap aggregation.

1499
01:10:10,950 --> 01:10:14,130
And the idea is, what do I need
in order to get g bar?

1500
01:10:14,130 --> 01:10:15,700
We said g bar is great,
if I can get it.

1501
01:10:15,700 --> 01:10:19,340
But it requires an infinite number
of data sets, and I have

1502
01:10:19,340 --> 01:10:20,530
only one data set.

1503
01:10:20,530 --> 01:10:25,790
So the idea of Bagging is that, I am
going to use my data set to generate

1504
01:10:25,790 --> 01:10:28,690
a large number of different data sets.

1505
01:10:28,690 --> 01:10:29,710
How am I going to do that?

1506
01:10:29,710 --> 01:10:31,080
Well, that's bootstrapping.

1507
01:10:31,080 --> 01:10:34,350
Bootstrapping always looks like magic.

1508
01:10:34,350 --> 01:10:36,280
You know where the expression comes from?

1509
01:10:36,280 --> 01:10:42,360
Bootstrapping, you try to lift
yourself by pulling on your

1510
01:10:42,360 --> 01:10:43,030
bootstraps.

1511
01:10:43,030 --> 01:10:45,320
Which obviously, you cannot do,
because you are pulling on it.

1512
01:10:45,320 --> 01:10:46,830
But that's what you do.

1513
01:10:46,830 --> 01:10:49,820
Here we are trying to create something,
where it isn't there.

1514
01:10:49,820 --> 01:10:56,460
And in this particular case, what you
do is you sample randomly from your

1515
01:10:56,460 --> 01:10:59,580
data set, in order to get different
data sets, and then average.

1516
01:10:59,580 --> 01:11:01,640
And believe it or not, that gives
you actually a dividend.

1517
01:11:01,640 --> 01:11:04,920
It gives you something about
the ensemble learning.

1518
01:11:04,920 --> 01:11:06,520
There are other, obviously
more sophisticated,

1519
01:11:06,520 --> 01:11:07,800
methods of ensemble learning.

1520
01:11:07,800 --> 01:11:11,160
And one way or the other, they appeal to
the fact that you are reducing the

1521
01:11:11,160 --> 01:11:14,410
variance by averaging
a bunch of stuff.

1522
01:11:14,410 --> 01:11:18,470
So you can say that it's either taken
outright, like Bagging, or inspired

1523
01:11:18,470 --> 01:11:21,380
in some sense, that it's a good idea
to average because you cancel out

1524
01:11:21,380 --> 01:11:22,630
fluctuations.

1525
01:11:22,630 --> 01:11:26,420


1526
01:11:26,420 --> 01:11:31,180
MODERATOR: If we use the Bayesian
approach, does this bias-variance

1527
01:11:31,180 --> 01:11:34,550
dilemma still appear?

1528
01:11:34,550 --> 01:11:35,300
PROFESSOR: Repeat
the question, please.

1529
01:11:35,300 --> 01:11:39,070
MODERATOR: If you use a Bayesian
approach, does this bias-variance

1530
01:11:39,070 --> 01:11:39,740
still appear?

1531
01:11:39,740 --> 01:11:40,990
PROFESSOR: OK.

1532
01:11:40,990 --> 01:11:43,325


1533
01:11:43,325 --> 01:11:45,800
The bias-variance is there to stay.

1534
01:11:45,800 --> 01:11:47,050
It's a fact.

1535
01:11:47,050 --> 01:11:54,030
And we can take a particular approach,
and then we are going to perhaps find

1536
01:11:54,030 --> 01:11:56,290
an explicit expression for
the bias, and an explicit

1537
01:11:56,290 --> 01:11:57,060
expression for the variance.

1538
01:11:57,060 --> 01:11:59,310
But nothing will change about the
nature of things because of the

1539
01:11:59,310 --> 01:12:00,830
approach I have.

1540
01:12:00,830 --> 01:12:04,250
Now the Bayesian approach is very
particular, because the Bayesian

1541
01:12:04,250 --> 01:12:06,810
approach makes certain assumptions.

1542
01:12:06,810 --> 01:12:09,090
And after you make these assumptions,
you can answer

1543
01:12:09,090 --> 01:12:11,100
all questions perfectly.

1544
01:12:11,100 --> 01:12:13,980
You can answer questions like that,
and other questions as well.

1545
01:12:13,980 --> 01:12:18,270
And I will talk about the Bayesian
approach in the very last lecture of

1546
01:12:18,270 --> 01:12:19,030
the course.

1547
01:12:19,030 --> 01:12:23,530
So I will defer answers, that are specific
to that, until that point.

1548
01:12:23,530 --> 01:12:28,610
But basically, the answer to this very
specific question, it's like if you

1549
01:12:28,610 --> 01:12:31,390
ask, does the VC dimension change
if you apply the Bayesian?

1550
01:12:31,390 --> 01:12:34,800
Well, you apply the Bayesian, this
is just a bunch of assumptions.

1551
01:12:34,800 --> 01:12:36,440
The VC dimension is there.

1552
01:12:36,440 --> 01:12:39,840
Maybe by using the Bayesian you'll be
able to find more direct quantities to

1553
01:12:39,840 --> 01:12:40,920
predict what you want.

1554
01:12:40,920 --> 01:12:46,960
But the VC dimension is there, because
it's defined in a general setup.

1555
01:12:46,960 --> 01:12:52,510
MODERATOR: A question about relation
with numerical function approximation.

1556
01:12:52,510 --> 01:12:55,950
In that field, there's interpolation
and extrapolation.

1557
01:12:55,950 --> 01:13:00,440
When is there extrapolation
in machine learning?

1558
01:13:00,440 --> 01:13:01,970
PROFESSOR: Function
approximation is one of the fields

1559
01:13:01,970 --> 01:13:03,110
that is very much related.

1560
01:13:03,110 --> 01:13:06,430
Because we are given a finite sample,
and they're coming from a function, and

1561
01:13:06,430 --> 01:13:07,350
you're trying to approximate it.

1562
01:13:07,350 --> 01:13:09,690
And this is one of the applications.

1563
01:13:09,690 --> 01:13:12,480
In general, interpolation is
easier than extrapolation,

1564
01:13:12,480 --> 01:13:14,570
because you have a handle.

1565
01:13:14,570 --> 01:13:18,600
And if you want to articulate that in
terms of the stuff we have, the

1566
01:13:18,600 --> 01:13:21,020
variance in interpolation is smaller
than the variance in

1567
01:13:21,020 --> 01:13:22,280
extrapolation in general.

1568
01:13:22,280 --> 01:13:25,440
Remember, the lines in the sinusoid?

1569
01:13:25,440 --> 01:13:27,330
They were all over the place.

1570
01:13:27,330 --> 01:13:30,040
If you take between the two points-- so
I have the sinusoid, and I have the

1571
01:13:30,040 --> 01:13:31,620
two points, I'm connecting
them with a line.

1572
01:13:31,620 --> 01:13:36,010
Between the two points, I am very much
in good shape, because the sine is

1573
01:13:36,010 --> 01:13:38,050
this way, and I am this way.

1574
01:13:38,050 --> 01:13:39,840
So it's not that big a deal.

1575
01:13:39,840 --> 01:13:43,300
The further out you go, then there
is a lot of fluctuation.

1576
01:13:43,300 --> 01:13:47,830
And that is reflected in
the extrapolation.

1577
01:13:47,830 --> 01:13:49,080
MODERATOR: OK.

1578
01:13:49,080 --> 01:13:51,400


1579
01:13:51,400 --> 01:13:53,820
When the variance is big, we
know we're extrapolating.

1580
01:13:53,820 --> 01:13:54,925
Is that the answer?

1581
01:13:54,925 --> 01:13:56,330
PROFESSOR: No.

1582
01:13:56,330 --> 01:13:58,980
I will say there is an association
between them.

1583
01:13:58,980 --> 01:14:01,720
To answer this specifically, you need
to understand the particular case.

1584
01:14:01,720 --> 01:14:04,650
There may be cases, where the
extrapolation doesn't have a lot of

1585
01:14:04,650 --> 01:14:05,430
variance and whatnot.

1586
01:14:05,430 --> 01:14:09,510
I'm just trying to map in general,
what the quantity here

1587
01:14:09,510 --> 01:14:11,740
corresponds to, in that.

1588
01:14:11,740 --> 01:14:17,070
The problem with extrapolation can be
posed, in this case, in terms of more

1589
01:14:17,070 --> 01:14:19,670
variance than interpolation.

1590
01:14:19,670 --> 01:14:22,200
But I'm not making a mathematical
statement that this is guaranteed to

1591
01:14:22,200 --> 01:14:23,450
be the case.

1592
01:14:23,450 --> 01:14:25,950


1593
01:14:25,950 --> 01:14:30,530
MODERATOR: Could you explain what the
literature means by the bias-variance

1594
01:14:30,530 --> 01:14:33,050
covariance dilemma?

1595
01:14:33,050 --> 01:14:34,300
PROFESSOR: OK.

1596
01:14:34,300 --> 01:14:37,610


1597
01:14:37,610 --> 01:14:42,730
You can pursue this analysis a little
bit further to the cases where you

1598
01:14:42,730 --> 01:14:44,160
have cross terms.

1599
01:14:44,160 --> 01:14:47,500
Particularly for boosting,
this is the case.

1600
01:14:47,500 --> 01:14:52,870
And then there is a question of, I
am trying to get these guys that I'm

1601
01:14:52,870 --> 01:14:55,090
going to average in order to
get the final hypothesis.

1602
01:14:55,090 --> 01:14:56,570
That's my game.

1603
01:14:56,570 --> 01:15:00,190
Now it would be nice if I can
get them to be independent.

1604
01:15:00,190 --> 01:15:03,800
Because when I get them to be
independent, then adding them up

1605
01:15:03,800 --> 01:15:07,890
reduces the variance
in a very good way.

1606
01:15:07,890 --> 01:15:10,250
But then, in general, when you
actually apply some of these

1607
01:15:10,250 --> 01:15:11,830
algorithms, there is a correlation
between one and another.

1608
01:15:11,830 --> 01:15:13,120
So there's a covariance.

1609
01:15:13,120 --> 01:15:16,850
So there's a question of the
balance between the two.

1610
01:15:16,850 --> 01:15:21,470
But it really is, in terms of
application, related more to ensemble

1611
01:15:21,470 --> 01:15:25,360
learning than to just the general
bias-variance analysis as I did it.

1612
01:15:25,360 --> 01:15:30,580
Because in the bias-variance analysis,
I had the luxury of picking

1613
01:15:30,580 --> 01:15:34,140
independently generated data sets,
generating independent guys, and then

1614
01:15:34,140 --> 01:15:36,960
averaging them, because it's
a conceptual aspect.

1615
01:15:36,960 --> 01:15:39,930
But when you actually are using
a technique, where you are constructing

1616
01:15:39,930 --> 01:15:44,960
these guys based on variations of the
data set, then the covariance starts

1617
01:15:44,960 --> 01:15:46,210
playing a role.

1618
01:15:46,210 --> 01:15:49,675


1619
01:15:49,675 --> 01:15:52,810
MODERATOR: A question about,
I guess, naming the things.

1620
01:15:52,810 --> 01:15:54,820
Is linear regression
actually learning?

1621
01:15:54,820 --> 01:15:59,980
Or is it just fitting along the lines
of function approximation?

1622
01:15:59,980 --> 01:16:02,610
PROFESSOR: Linear regression,
is a learning technique.

1623
01:16:02,610 --> 01:16:05,980
And fitting is the first
part of learning.

1624
01:16:05,980 --> 01:16:08,130
So you always fit, in order to learn.

1625
01:16:08,130 --> 01:16:11,260
The only added thing is that you want
to make sure that, as you fit, you

1626
01:16:11,260 --> 01:16:12,660
always perform well out-of-sample.

1627
01:16:12,660 --> 01:16:14,610
That's what the theory was about.

1628
01:16:14,610 --> 01:16:17,250
I've been spending four lectures trying
to make sure, that when you do

1629
01:16:17,250 --> 01:16:19,740
the intuitive thing, I give
you data, you fit them.

1630
01:16:19,740 --> 01:16:22,050
You could do that without taking
a machine learning course.

1631
01:16:22,050 --> 01:16:24,640
Now I'm telling you that you have to
have the checks in place, such that

1632
01:16:24,640 --> 01:16:28,410
when you fit in-sample, something good
happens in what you care about, which

1633
01:16:28,410 --> 01:16:29,320
is out-of-sample.

1634
01:16:29,320 --> 01:16:31,990
So that's the--

1635
01:16:31,990 --> 01:16:32,488
MODERATOR: All right.

1636
01:16:32,488 --> 01:16:33,700
I think that's it.

1637
01:16:33,700 --> 01:16:34,240
PROFESSOR: Very good.

1638
01:16:34,240 --> 01:16:34,500


1639
01:16:34,500 --> 01:16:35,750
We'll see you next week.

1640
01:16:35,750 --> 01:16:50,310

